<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
   <!-- 피드경로 명시 -->
  <channel>
    <title> Welcome to my blog  </title>
    <description>Personal Tech Blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000https://6unoyunr.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 05 Mar 2025 22:57:03 +0900</pubDate>
    <lastBuildDate>Wed, 05 Mar 2025 22:57:03 +0900</lastBuildDate>
    <generator>Jekyll v4.3.1</generator>
    
      <item>
        <title>딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</title>
        <description>&lt;h1 id=&quot;a-little-bit-personal&quot;&gt;A Little Bit Personal&lt;/h1&gt;

&lt;h3 id=&quot;들어가기전-극히-개인적인-주저리&quot;&gt;들어가기전 극히 개인적인 주저리&lt;/h3&gt;

&lt;p&gt;나는 아직도 공부를 한다. 대부분의 개발자는 커리어를 쌓으며 끊임없는 공부가 필요한데, 왜냐하면 지금까지 익혀온 기술 스택이 더이상 트렌디하지 않아지는 경우가 많기 때문이다. 특히나 AI의 경우에는 빠른 변화를 보이는데, 그래서 그런지 실제로 논문을 쓰다가 미친 일반화 성능을 보이는 파운데이션 모델이 나와버리면 해당 task가 아예 날아가버리는 경우도 발생하기 시작했다. 이렇듯 AI 연구자의 숙명은 어쩔 수 없이 기술의 최전선에서 그 누구보다 빠르게 현황을 파악해야만 살아남는다는 사실이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img width=&quot;372&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/b369e902-73c8-4dee-bd34-fc8cc57b2c8e&quot; /&gt;
    &lt;img width=&quot;278&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/6c89dc94-ce72-407e-a6d0-90ea34ac33bc&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;본인도 학부 그리고 대학원 시절을 거치며 개인적으로 새로운 인공지능 논문들이 나올 때마다 거의 즉각 읽는 습관을 들였다. 처음에는 일주일에 한 편 읽는 것도 어려웠는데 이제는 완전 디테일하게 읽지는 못해도 하루에 논문 두세편 정도는 읽어낼 수 있게 되었다. 인공지능을 연구하는 사람이기도 하고 나에게 있어 논문은 일종의 뉴스, 신문같은 존재이며, 급변하는 기술 환경에 보다 빠르게 적응할 수 있도록 도움을 주는 방법이다. 딥시크라는 모델도 테크니컬 레포트로 처음 접하여 알고는 있었지만 R1 모델의 등장이 이렇게나 파격적인 행보를 보일 줄은 몰랐다.&lt;/p&gt;

&lt;p&gt;나름 챗지피티 이후로도 인공지능 모델은 끊임없이 학계나 산업계에서 연구되었으나, 그 모든 연구들이 딱히 크게 이슈화되는 경우는 거의 없었다. 여기서의 이슈화는 사람마다 기준이 다르겠지만 적어도 본인은 지금의 scaling law를 깨부술 무언가를 원했던 것일수도 있다. 물론 다양한 연구들이 진행되었기 때문에 현존하는 수많은 closed/opened source AI의 성능 향상에 큰 영향을 끼친 것은 사실이다.  그러나 실제 서비스에 활용되는 모델을 기준으로 대부분의 연구는 덩치 키우기에 집중했다. 거대한 자본이 수많은 리소스와 컴퓨팅 파워, 리소스 그리고 Human Resource(AI Engineer)을 끌어모았고,  매우 빠른 연구 및 개발이 시작되었다. Explainability와 scalability의 줄다리기가 어느새 scalability의 승리로 마무리되는 양상이 되어버렸다. 그로 인해 인공지능 연구 방향이 바뀌기도 했으며, 기존에는 생각하지도 못했던 task가 새롭게 제안되는 일도 생겼다.&lt;/p&gt;

&lt;p&gt;사실 테크니컬 레포트를 읽어본 후기로는 딥시크는 &lt;u&gt;기술적으로 그렇게까지 독보적이거나 유니크한 모델이라고 볼 수 있을까?&lt;/u&gt;였다. 그럼에도 대단하다고 여긴 것은 현재의 기술 시장을 흔들 정도의 파급력을 가져왔다는 사실이고, 기존에 소스코드를 공개하지 않았던 OpenAI의 기술력을 따라잡기 위해 정말 많은 노력을 했다는 사실이다. 적어도 적당히 타협해서 오픈 소스 튜닝하던 대부분의 방법들보단 훨씬 유의미한 결과를 낸 것은 사실이다.&lt;/p&gt;

&lt;p&gt;아무튼 이제 글을 시작해보고자 한다. 이번 글은 테크니컬 라이팅이라기 보다는 개인적인 고찰, 혹은 일기장 정도 될 것 같다.&lt;/p&gt;

&lt;h1 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h1&gt;

&lt;h3 id=&quot;강화학습에-대한-고찰&quot;&gt;강화학습에 대한 고찰&lt;/h3&gt;

&lt;p&gt;우리는 로봇에게 어떤 일을 수행하게 시키고 싶다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/2c50c2a0-e5f6-4edc-a64e-28bc8d274fc8&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;지금은 딥시크, 즉 LLM 모델에 대해 얘기하고 있으니 LLM 모델을 기준으로 말하면 &lt;u&gt;‘대답 잘 하게 만들기’&lt;/u&gt; 쯤 되겠다. 옛말에 미운놈 떡 하나 준다는 말이 있는데 인공지능의 세상은 그리 호락호락하지 않다.&lt;/p&gt;

&lt;p&gt;앞으로 우리는 &lt;u&gt;예쁜놈한테 떡 하나를 더 줄거다&lt;/u&gt;. 이러한 개념을 “Reward(보상)”이라고 한다. 적절히 잘 학습된 모델을 가지고 이런저런 질의응답 Environment(환경)에서 대답을 잘하면 칭찬해주는 프로세스를 반복해서 &lt;u&gt;더욱 대답을 잘하는 모델을 만들고자 하는 것&lt;/u&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;policy-based-method의-발전&quot;&gt;Policy based method의 발전&lt;/h3&gt;

&lt;p&gt;기존의 강화학습 방식이었던 Value based method는 인공지능 모델이 취할 모든 행동에 대해 가치평가하는 모델을 학습시킨다. 그래서 인공지능 모델이 현재 상태에서 어떤 Action(동작)을 취할지에 따른 Value(가치)를 판단하고, 가치에 따라 greedy 알고리즘으로 다음 동작을 결정하는 방식을 학습하게 된다. 이때 각 상황에서 어떤 동작을 취할지에 대한 기준이 바로 Policy(정책)이다. 이때 &lt;u&gt;가치를 판단해주는 모델을 딥러닝으로 학습&lt;/u&gt;시키자는 관점이 바로 &lt;u&gt;DQN(Deep Q-Network)&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;이때 Value function에 해당되는 Q-Network의 개념이 나온 이유는 여러 동작에 대한 Reward가 가지는 불안정성을 어느 정도 &lt;u&gt;action-value function Q&lt;/u&gt;가 해소해줄 수 있다는 관점이었다 (아래는 알고리즘 코드).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/6af4d508-2276-4820-ae4e-150e8a994b9c&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그러나 사실 생각해보면 결국 가치 판단을 위한 모델을 학습하는 것의 최종 목적은 &lt;u&gt;‘각 동작의 가치를 잘 판단하자’&lt;/u&gt;가 아니라 &lt;u&gt;‘가장 높은 가치를 지니는 동작을 취하게 하자’&lt;/u&gt; 라는 것이다. 그렇다면, 그냥 모델이 취할 모든 행동에 대한 가치 판단 없이 바로 모델의 현재 상태에서 바람직한 다음 동작을 예측하는 Policy, 그 자체를 학습하는 것이 낫지 않겠는가에 대한 근본적 의문이 생긴다. 심지어 동작이 속한 공간을 Discrete하게 만들 필요도 없다. 정책 모델이 내뱉은 확률 분포에 근거해서 다음 동작을 예측하고, 만약 이러한 흐름이 좋은 결과를 가져왔을때 보상을 주어 Policy 모델을 업데이트한다. 이러한 학습법에 대한 이론이 바로 &lt;u&gt;‘Policy Gradient’&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;강화학습에서는 다양한 시뮬레이션  환경을 가정한다. 데이터셋 전체를 objective function에 empirical하게 근사시키는 deep learning의 개념과 유사하게, 강화학습에서는 각 시뮬레이션 단계를 ‘Episode’라고 부르고 하나의 에피소드 내에서 시간의 흐름에 따라 강화학습을 구성하는 각 모델의 입출력이 나오게 된다. 결국 강화학습의 가장 큰 목적은 다양한 에피소드로부터 모델의 다양한 의사결정방식을 받고, 해당 모델이 &lt;u&gt;가장 이상적인 의사결정방식을 취했을 때&lt;/u&gt; 모든 에피소드로부터의 리워드는 가장 큰 기댓값을 가지게 된다. 흔히 정책은 $\theta$라는 매개변수를 가지는 함수 $\pi_\theta(\cdot)$로 주로 표현하게 된다. 이 정책 함수는 각 state $s$에서 수행할 수 있는 모든 액션 $a$에 대한 확률 분포를 추출할 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 우리는 특정 매개변수를 가지는 정책(Policy)모델이 특정 액션을 수행했을때 관측 가능한 입/출력 결과에 대해 최대의 reward를 얻을 수 있는 방향으로 모델을 학습하고자 하며, 이 방향이 곧 &lt;u&gt;policy gradient&lt;/u&gt;에 해당된다.&lt;/p&gt;

&lt;p&gt;사실 policy의 gradient를 계산하는 과정은 순탄치 않다. 그 이유는 reward function에 대한 gradient는 정책 함수($\pi_\theta$)가 어떤 액션을 수행하였는가 뿐만 아니라, 실제로 마르코프 프로세스에서 정책 함수에 의하여 불가피하게 조정된 현재 state $s$에 대한 확률 분포 또한 고려되어야하기 때문이다. 즉, 정책 함수에 추가로, 정책 함수로 하여금 지속적으로 변해온 state $s$의 확률 분포에도 $\theta$가 기여한 바가 있기 때문에 gradient를 직접 구하기 어렵다는 문제가 발생한다. 그러나 이를 단순히 무시할 수 있다는 이론이 바로 &lt;u&gt;policy gradient theorem&lt;/u&gt;이고 한 문장으로 다음과 같이 정리할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\text{보상 함수의 gradient는 정책 함수의 gradient에 비례한다.}
]
엄밀한 증명은 아래에 간단하게 요약해보겠다.&lt;/p&gt;

&lt;p&gt;[
\nabla_{\theta} J(\theta) = \nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \pi_{\theta}(a|s) &lt;br /&gt;
\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \pi_{\theta}(a|s)
]&lt;/p&gt;

&lt;p&gt;보상함수는 각 state에 머물 확률과, 그 확률에서의 모든 액션 수행에 대한 Value의 총합(혹은 기댓값)을 의미한다. 위의 식에서 &lt;u&gt;뒷쪽에 있는 state value function&lt;/u&gt;에 대한 gradient를 구하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \nabla_{\theta} V^{\pi}(s) &amp;amp;= \nabla_{\theta} \left( \sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s, a) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} Q^{\pi}(s, a) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} \sum_{s’, r} P(s’, r | s, a) (r + V^{\pi}(s’)) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s’, r} P(s’, r | s, a) \nabla_{\theta} V^{\pi}(s’) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s’} P(s’ | s, a) \nabla_{\theta} V^{\pi}(s’) \right)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;우선, 전체 식에 대한 gradient는 product rule에 따라 분배된다. 이때 뒤쪽에 있는 $Q^\pi$는 &lt;u&gt;현재 가치와 미래 가치의 합&lt;/u&gt;으로 표현 가능하다.  이때 뒤에 발생하는 $P$는 Markov decision process(MDP)에 따른 확률 분포로 생각하면 되고, 이전 state가 $s$일때 모든 다음 state에 대한 확률을 정의할 수 있다. 이는 어떠한 정책 함수에 따른 결과가 아니기 때문에 $\theta$라는 변수와 독립이다. 따라서 뒤쪽 term에 있는 gradient는 시그마의 안쪽으로 들어갈 수 있게 되며, 최종적으로는 $P(s^\prime, r \vert s, a)$를 $r$에 대해 marginalize하면서 수식이 완성되는 구조다.이 수식에서 주목할 점은, &lt;u&gt;다음 state value function의 gradient가 현재 state value function에 recursive하게 들어간다는 사실&lt;/u&gt;이다. 그렇다면 뒤쪽에 들어있는 $\nabla_\theta V^\pi (s^\prime)$ 또한 다음 state인 $\nabla_\theta V^\pi (s^{\prime\prime})$의 recursive한 수식으로 표현된다.  이렇게 계속 unrolling(recursive하게 $s^\infty$까지 전개)한다고 생각해보자.&lt;/p&gt;

&lt;p&gt;아래의 식에서 $\rho$는 policy function $\pi$에 의해 $k$번의 step 이후 특정 state로 바뀔 확률을 간소화하여 표현한 식이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
    \nabla_{\theta} V^{\pi}(s) &amp;amp;= \phi(s) + \sum_{a} \pi_{\theta}(a|s) \sum_{s’} P(s’|s,a) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \sum_{a} \pi_{\theta}(a|s) P(s’|s,a) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s’) Q^{\pi}(s’, a) + \pi_{\theta}(a|s’) \sum_{s’’} P(s’‘|s’, a) \nabla_{\theta} V^{\pi}(s’’) \right) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \left( \phi(s’) + \sum_{s’’} \rho^{\pi}(s’ \to s’’, 1) \nabla_{\theta} V^{\pi}(s’’) \right) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^{\pi}(s \to s’’, 2) \nabla_{\theta} V^{\pi}(s’’) \quad \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^{\pi}(s \to s’’, 2) \phi(s’’) + \sum_{s’’’} \rho^{\pi}(s \to s’’’, 3) \nabla_{\theta} V^{\pi}(s’’’) \newline
    &amp;amp;= \dots \newline
    &amp;amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \to x, k) \phi(x)
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;우리는 현재 state에서 미래 state $x \in \mathcal{S}$로 가는 모든 MDP를 전개했다. 이제 전개된 식에 state의 초기 상태를 대입하고 여러 전개 과정을 거치면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \nabla_{\theta} J(\theta) &amp;amp;= \nabla_{\theta} V^{\pi}(s_0) \newline    &amp;amp;= \sum_s \sum_{k=0}^{\infty} \rho^{\pi}(s_0 \to s, k) \phi(s) \newline    &amp;amp;= \sum_s \eta(s) \phi(s) \newline    &amp;amp;= \left( \sum_s \eta(s) \right) \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &amp;amp;\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &amp;amp;= \sum_s d^{\pi}(s) \sum_a \nabla_{\theta} \pi_{\theta} (a | s) Q^{\pi}(s, a)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이를 통해 &lt;u&gt;정책 함수의 gradient의 방향&lt;/u&gt;이 곧 &lt;u&gt;보상 함수의 gradient의 방향&lt;/u&gt;과 일치한다는 점을 알아낸 것이다.&lt;/p&gt;

&lt;h3 id=&quot;trpo-to-ppo&quot;&gt;TRPO to PPO&lt;/h3&gt;

&lt;p&gt;앞서 본 내용은 policy gradient에 대한 기본적인 내용이었다. 그러나 이 이후 사실 LLM에 적용되기까지 강화학습 분야에서 policy gradient에 대한 다양한 접근법이 있었으며, 지금부터 살펴볼 내용이 현존하는 딥시크-R1의 아이디어 근간이라고 할 수 있다. TRPO와 PPO 중 &lt;a href=&quot;https://arxiv.org/pdf/1707.06347&quot;&gt;PPO&lt;/a&gt;가 조금 더 중요한데(&lt;strong&gt;OpenAI에서 쓴 논문&lt;/strong&gt;), TRPO를 꼭 짚고 넘어가야하는 이유는 &lt;u&gt;TRPO&lt;/u&gt;(&lt;strong&gt;피터 아벨 연구소 논문&lt;/strong&gt;)가 가장 기본 틀이 되기 때문이라고 생각했기 때문이다. TRPO에는 수학적으로 어려운 내용이 포함된다. 가장 간단하게 설명하자면 TRPO는 정책 모델을 학습할 때 “신뢰 가능한 구간 내에서 업데이트한다”라는 개념이다. 그렇다면 대체 왜 신뢰 가능한 구간이 중요한 것일까?&lt;/p&gt;

&lt;p&gt;Policy gradient은 다음과 같이 진행된다. 초기 상태는 확률 분포 속에서 샘플링되었다고 가정하자. 예를 들어 우리가 걷기 시작하거나 뛰기 시작할때 항상 같은 자세에서 시작하지는 않는 것과 같다.  우리는 시시각각 지금 행동에 기반하여 다음 행동을 결정하고 이를 수행한다. 이러한 과정을 우리는 자연스럽게 처리하지만 로봇에서 시키는 경우를 생각해봐야 한다.&lt;/p&gt;

&lt;p&gt;로봇이 지금 현재 어떤 행동을 수행했을때, 그 행동이 가져오는 가치 그리고 리워드를 수치화하고 이를 누적해서 더해간다. 그리고 그 행동에 기인하는 Policy가 지속적으로 좋은 방향으로 학습된다면, 이론상 업데이트되는 정책에 따른 누적된 리워드는 계속 우상향할 것이다.&lt;/p&gt;

&lt;p&gt;Optimal solution에 수렴할 때까지 지속 학습을 진행한다고 가정해보자. 모든 state에서 평가된 가치가 0보다 크거나 같다면 결국 모든 미래가치가 0이 되어 더이상 발전할 수 없는 상황이 올 때의 Policy가  최적의 해가 될 것이다. 하지만 업데이트된 policy($\tilde{\pi}$)는 모든 다음 state, action($s, a$)에 따른 advantage($Q^\pi$)를 0보다 같거나 크게 만든다는 보장이 없다. 이를 다소 단순화하기 위한 방법으로 학습하고자 하는 Objective는 다음과 같이 표현된다.&lt;/p&gt;

&lt;p&gt;[
L(\tilde{\pi}) = \eta (\pi) + \sum_s d^\pi (s) \sum_a \tilde{\pi}(a \vert s) Q^\pi(s, a).
]&lt;/p&gt;

&lt;p&gt;이제는 state에 대한 density 변화($d^{\tilde{\pi}}$)는 무시할 수 있다. 좌측 함수와 우측 함수의 초기값(파라미터 : $\theta_0$)이 동일하기 때문에 Locally 아주 작은 변화량 $\Delta \pi$에 대해서 gradient를 같은 방향으로 유지할 수 있지만, step size를 키울 수 없다는 문제가 생긴다. 만약 gradient가 이상적으로 생기지 않았다면, step size를 잘못 지정해주면 학습이 불안정해질 수 있다. 실제로 많은 강화학습에서 가장 큰 문제가 이러한 학습 불안정성에서 기인하며, 이를 해결하기 위해 수많은 방법론이 제안되었다.&lt;/p&gt;

&lt;p&gt;이를 해결하는 방법은 새로운 policy를 기존의 policy와 $\alpha$ mixing하게 되면 lower bound를 가진다. 근데 이 $\alpha$값도 매 task마다 새롭게 정의하기 힘드니, $\pi$와 $\tilde{\pi}$ 간의 거리 메트릭으로 정하자는 것 ($\alpha =D_{\text{metric}}(\pi \vert \tilde{\pi})$ )이 솔루션이 된다. 해당 논문에서는 KL divergence를 확률 분포상의 거리 메트릭 기준으로 진행하였다. 이때의 가장 큰 문제점은 $\alpha$에 대한 constraints를 가지는 $L$의 최대화 수식(Objective)에서, 최적의 $\theta$값을 찾기 위해 Approximation을 진행하고, 이를 위해 &lt;u&gt;Hessian(2차 미분)을 수행해야한다는 점&lt;/u&gt;이다. 이를 우회해서 풀 수 있는 방법으로 &lt;u&gt;Fisher Information Matrix&lt;/u&gt;가 있는데, Gradient의 공분산을 Empirical하게 평균내면 Hessian에 근사할 수 있는 방법이다. TRPO에서는 $k=10$ 정도에서 타협을 했고, 이를 통해 Policy gradient의 안정적인 강화학습을 제안했던 논문이다.&lt;/p&gt;

&lt;p&gt;PPO에서는 TRPO의 이런 타협점을 개선할 수 있는 방향을 제시한다. TRPO에서 constraint인 KL divergence에 적용되던 2차 미분(이를 surrogate objective로 표현한다)대신 1차 미분에 근사할 수 있는 방법을 찾고자 했다. PPO에서 적용한 방법은 기존의 surrogate objective에서 old/new policy가 크게 벗어나는 지점을 비율로 조절하게 된다($1-\epsilon$, $1+\epsilon$). 이러한 개념은 TRPO에서의 constraints의 $2^{nd}$ order differentiation에서 $\pi \simeq \tilde{\pi}$ 인 지점을 찾고자 하는 주목적을 Hessian에서 단순 clipping으로 간소화시켰다고 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;사실 PPO/TRPO에 쓰이는 Q-function은 Advantage function $A$로 사용하는데, 이에 대한 보다 자세한 내용은 다음 섹터인 GRPO에서 언급하며 시작하도록 하겠다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/df18835d-b0f2-4ab2-afed-49e58b4b0dd0&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;grpo&quot;&gt;GRPO&lt;/h3&gt;

&lt;p&gt;이전에 올렸던 글 중 챗지피티는 어떤 식으로 학습되었을까에 대해 간단하게 소개했던 내용 중, RLHF와 PPO에 대해 간단하게 넘어갔던 부분이 있다. 딥시크는 PPO가 아닌 GRPO라는 자체적으로 연구한 방법을 통해 강화학습을 수행하였고, 강화학습을 통해 학습된 가장 고성능의 모델을 여러 작은 모델에 distillation하는 과정을 수행하였다. GRPO는 &lt;a href=&quot;https://arxiv.org/pdf/2402.03300&quot;&gt;“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”&lt;/a&gt;라는 논문에서 제안된 방법인 듯하다. 강화학습 논문이라기보다 &lt;u&gt;Deepseek에서 Language model의 수학적 추론 능력을 강화하기 위한 방법론&lt;/u&gt;으로 제안하였다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{J}_{PPO}(\theta) = \mathbb{E} \left( q \sim P(Q), o \sim \pi_{\theta_{\text{old}}} (O | q) \right) \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left( \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\theta_{\text{old}}} (o_t | q, o_{&amp;lt;t})} A_t, \text{clip} \left( \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\theta_{\text{old}}} (o_t | q, o_{&amp;lt;t})}, 1 - \epsilon, 1 + \epsilon \right) A_t \right).
]&lt;/p&gt;

&lt;p&gt;LLM에 위와 같은 PPO가 적용되며 다음과 같은 문제점이 생긴다. 우선 갑자기 등장한 수식에 맛있게 먹었던 점심이 다시 올라올 것 같기 때문에 천천히 보고 넘어가면 다음과 같다. 앞서 주구장창 언급했기 때문에 policy model에 대한 notation은 어느 정도 유추가 된다. 업데이트되기 전 파라미터가 $\theta_\text{old}$이고 업데이트된 파라미터는 $\theta$이다. $q, o$는 question dataset으로부터 샘플링된 질문, old policy로부터의 output을 의미한다(성능 비교의 베이스라인이 되는 LLM이 내뱉는 답변). 그리고 PPO에서의 clipping을 통해 학습 안정화하는 과정 또한 동일하다. 앞서 단순하게 넘어갔던 Advantage가 여기서 등장하는데, &lt;u&gt;기존의 Advantage function estimator들이 가지고 있는 문제점&lt;/u&gt;(특히 state의 변화 갯수 $k$에 따른 bias/variance trade-off, $k$가  크면 variance가 커지고, $k$를 줄이게 되면 bias가 커지는 문제)를 보다 일반화하는 방법으로 소개된 Generalized Advantage Estimator(GAE)을 짚고자 한다. 우선 시작하기 전, estimator에 대한 정의부터 알아야한다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \hat{A}_t^{(1)} &amp;amp;= r_t + \gamma V(s_{t+1}) - V(s_t) \newline    \hat{A}_t^{(2)} &amp;amp;= r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t) \newline    &amp;amp;\quad \vdots = \vdots \newline    \hat{A}_t^{(\infty)} &amp;amp;= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots - V(s_t)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;현재의 policy가 앞으로의 state 변화에 끼칠 영향력을 생각한다. Value를 판단할 function $V$ 또한 학습 가능한 하나의 뉴럴 네트워크이다. 이렇듯 policy 모델과 함께 value 평가 모델이 함께 학습되는 구조를 &lt;u&gt;actor-critic model&lt;/u&gt;이라 부른다.그런데 이때, time step+1인 시점과 time step+$k$인 시점의 가치는 서로 다르다. 당연하게도 만약 discounting value $\gamma$가 적용되지 않는다면 estimator가 보는 시점이 길어지면 길어질수록 그만큼 보다 가까운 다음 state에 대한 미래 가치를 상실하게 되고, 결국 &lt;u&gt;policy 및 value에 대한 공정 평가가 어렵기 때문&lt;/u&gt;이다. 하지만 이러한 Advantage Function에 문제가 있다.&lt;/p&gt;

&lt;p&gt;작은 $k$값을 갖는 추정량 $A_t^{(k)}$는 분산이 낮지만 편향이 크고, 큰 $k$값을 갖는 경우 편향이 낮지만 분산이 크다는 점이다. 이는 &lt;u&gt;항의 개수를 보면 직관화가 가능&lt;/u&gt;하다. 합산해야 할 항이 적을 경우(state의 변화가 많지 않기 때문에) 분산이 낮아지지만, 상대적으로 $r_k$에 대한 정확한 정보를 활용하지 않기 때문에 편향이 상대적으로 커진다. 합산해야 할 양이 큰 경우는 반대로 생각하면 된다. 또한, $V(s_t)$ 가 추정 클래스 내에서는 상수로 생각될 수 있기 때문에, 추정량 간 차이는 오직  $k-step$ return에서만 발생하게 된다(길게 표현했지만 trade-off가 존재한다는 사실).&lt;/p&gt;

&lt;p&gt;그렇기 때문에 GAE는 &lt;u&gt;추정량 집합 전체에 대해 싸그리 어셈블하는 전략을 사용&lt;/u&gt;하였다. 각 estimator에 존재하는 trade-off를 &lt;u&gt;$\lambda$로 조절하겠다는 생각&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \hat{A}_t^{GAE(\gamma, \lambda)}    &amp;amp;= (1 - \lambda) \left( \hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V + \lambda (\delta_t^V + \gamma \delta_{t+1}^V) + \lambda^2 (\delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V) + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V (1 + \lambda + \lambda^2 + \cdots) + \gamma \delta_{t+1}^V (\lambda + \lambda^2 + \cdots) + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V \frac{1}{1 - \lambda} + \gamma \delta_{t+1}^V \frac{\lambda}{1 - \lambda} + \cdots \right) \newline    &amp;amp;= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V\end{aligned}
]&lt;/p&gt;

&lt;p&gt;바로 이 개념이 GAE이고, PPO 또한 이를 적용한 모델이라고 볼 수 있다. 여기서의 문제점을 드러내면 다음과 같다. Value model은 Policy model이 학습됨에 따라 내뱉는 &lt;u&gt;output에 대한 성능 평가를 진행&lt;/u&gt;하는데, 이때 Policy model과 별개로 학습이 진행되어있어야한다는 점이다.  PPO에서는 reward model에 과적합되는 문제를 직접적으로 피하기 위해 &lt;u&gt;KL penalty&lt;/u&gt;를 도입하게 된다. Reference model은 주로 초기 언어 모델이나 SFT(Supervised Fine-tuned) 모델로 사용하게 되는데, 학습되는 정책 모델이 내뱉는 output이 SFT 모델과 지나치게 달라지지 않도록 각 토큰 단위에서 제약을 가하게 되어 생성되는 텍스트의 &lt;u&gt;각 토큰이 기준 모델에서 기대되는 분포에서 크게 벗어나지 않도록&lt;/u&gt; 한다.&lt;/p&gt;

&lt;p&gt;[
r_t = r_{\varphi} (q, o_{\leq t}) - \beta \log \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\text{ref}} (o_t | q, o_{&amp;lt;t})}
]&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/561b5870-65d3-4abe-8dbd-9513819b1c77&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그런데 실제 GRPO 논문의 그림을 보면 살짝 이해하기 어려운 부분이 있을텐데, 바로 KL divergence의 방향성이다. 그런데 실질적으로 KL penalty가 적용되는 구조는 동일하다. PPO 기반으로 동작하는 것은 거의 유사하지만, 바뀐 점은 다음과 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4dc7d92e-01ad-41bb-a512-a03062eb56f9&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO와 달리 GRPO는 더이상 Value Model을 사용하지 않는다. 그렇기 때문에 메모리를 절약할 수 있다.&lt;/li&gt;
  &lt;li&gt;그러나 Value Model을 사용할 수 없다는 것은 GAE를 사용할 수 없다는 뜻이고, 결국 기존에 연산되던 advantage function $A$를 새롭게 정의해야한다.&lt;/li&gt;
  &lt;li&gt;두 정책 모델의 TR 내에서 강화학습을 진행하는 구조는 동일하다. 그러나 이번에는 동일 질문($q$)에 대해 여러 output $o_{1 \ldots G}$를 답변으로 추출한다.&lt;/li&gt;
  &lt;li&gt;그리고 이 각각의 답변에 대해 예측된 이득의 평균을 정책 강화 학습의 Objective로 사용할 것이다.&lt;/li&gt;
  &lt;li&gt;학습된 reward 모델로부터 각각의 output을 평가한다.&lt;/li&gt;
  &lt;li&gt;Reward model의 결과 ${r_1, r_2, \cdots, r_G}$를 그룹 전체의 reward로 normalize해서 사용한다. 이는 특정 문맥 $q/o$에 대한 bias/variance(GAE에서 문제가 된 부분)를 줄이기 위한 노력이다.&lt;/li&gt;
  &lt;li&gt;KL divergence는 기존의 식이 아닌 Unbiased Estimator를 사용하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;요약하자면, Value function을 없앴기 때문에 &lt;u&gt;불가능한 GAE 대신 Reward의 grouped output을 정규화&lt;/u&gt;하여 trade-off 효과를 상쇄하였으며, KL divergence 식이 바뀐 정도가 되겠다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left( q \sim P(Q), {o_i}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}} (O | q) \right) &lt;br /&gt;
\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta} (o_i | q)}{\pi_{\theta_{\text{old}}} (o_i | q)} A_i,
\text{clip} \left( \frac{\pi_{\theta} (o_i | q)}{\pi_{\theta_{\text{old}}} (o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi{\theta} | \pi_{\text{ref}}) \right)
]&lt;/p&gt;

&lt;p&gt;여기서 쓰인 KL divergence 식은:&lt;/p&gt;

&lt;p&gt;[
\mathbb{D}_{KL} (\pi_{\theta} | \pi_{\text{ref}}) = 
    \frac{\pi_{\text{ref}} (o_i | q)}{\pi_{\theta} (o_i | q)} - \log \frac{\pi_{\text{ref}} (o_i | q)}{\pi_{\theta} (o_i | q)} - 1.
]&lt;/p&gt;

&lt;p&gt;이며 각각의 리워드는 다음과 같이 정규화하여 연산된다:&lt;/p&gt;

&lt;p&gt;[
A_i = \frac{r_i - \operatorname{mean} ({r_1, r_2, \dots, r_G})}{\operatorname{std} ({r_1, r_2, \dots, r_G})}.
]&lt;/p&gt;

&lt;h1 id=&quot;methods--approaches&quot;&gt;Methods / Approaches&lt;/h1&gt;

&lt;h3 id=&quot;deepseek-r1-zero의-학습법&quot;&gt;Deepseek-R1 Zero의 학습법&lt;/h3&gt;

&lt;p&gt;딥시크는 &lt;u&gt;DeepSeek-V3-Base를 베이스 모델&lt;/u&gt;로 활용, &lt;u&gt;GRPO를 가장 메인인 강화 학습법&lt;/u&gt;으로 적용한 연구이다. 그 중 가장 메인이 되는 학습법에는 크게 리워드 모델링과 학습 템플릿이 있다. 이외의 방법론은 직접 학습을 통해 demonstration 과정을 거쳤다. 딥시크의 가장 큰 주장은, LLM 베이스 모델을 강화학습 만으로도 o1 만큼의 코딩/수학적 능력까지 키울 수 있다는 사실로, 강화학습이 LLM 학습에 큰 도움이 된다는 사실을 입증한다. Deepseek-R1은 Ollama에서 경량화 버전을 바로 사용해볼 수 있는데, 이때의 output을 확인해보면 실제 답변을 하기 전 ‘생각하는 부분’이 추가된 것을 알 수 있다. 개인적으로 맥북 프로 (M3)를 사용중인데, 속도 면에서 deepseek-r1:14b 모델 정도의 distillation 버전 정도면 편하게 돌릴 수 있는 것 같다. Deepseek-R1:14B 버전에게 LLM system에 대해 질문한 예시는 다음과 같다. 물론 이렇게 사용한 모델은 R1-Zero는 아니다. 그냥 예시를 보여주기 위해 사용하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/55ed6a24-a3a4-44b3-b66b-3bfa9adf744e&quot; width=&quot;962&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;딥시크는 보는 바와 같이 추론 과정을 &amp;lt;think&amp;gt; ~ &amp;lt;/think&amp;gt; 태그로 넣고, 그 이후에 실제 답변을 내놓는 것을 볼 수 있다(근데 distill 버전을 쓰다보면 가끔 생각을 멈추기도 함). 이러한 답변 특성은 &lt;u&gt;학습 템플릿 구성&lt;/u&gt;에 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e27bbdec-90c2-4f8d-8a37-484ba4a9475a&quot; width=&quot;962&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 o1이고 o3고 어떤 방식으로 추론해서 정답을 내놓는지는 모르겠지만, 이 &lt;u&gt;일련의 과정을 강화학습으로 자동 학습하게 하는 것이 주된 아이디어인 듯&lt;/u&gt;하다. 이에 따라 R1-Zero의 Reward modeling 또한 두 분류로 나뉘게 되었다. 리워드 모델을 따로 학습하여 사용할 수도 있지만, Deepseek-R1-Zero 경우 Rule-based method (blackbox가 아닌 평가가 예측 가능한 경우 사용)를 적용하였다. 주요 보상 유형은 두 가지로 구성된다. 개발하는 과정에서 결과 기반 또는 과정 기반 신경망 보상 모델은 적용하지 않았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;정확성 보상:&lt;/strong&gt; 정확성 보상 모델은 응답이 올바른지 평가한다. 예를 들어, 결정적인 결과를 가진 수학 문제의 경우, 모델이 특정한 형식으로 정답을 제시해야 하며, 이를 통해 신뢰할 수 있는 규칙 기반 검증이 가능하다. 마찬가지로, LeetCode (코딩) 문제의 경우, 사전에 정의된 테스트 케이스를 기반으로 컴파일러를 활용하여 피드백을 생성할 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;형식 보상:&lt;/strong&gt; 정확성 보상 모델 외에도, 형식 보상 모델을 적용하여 모델이 사고 과정을 &lt;think&gt; 및 &lt;/think&gt; 태그 사이에 넣도록 강제한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;흥미로운-점&quot;&gt;흥미로운 점&lt;/h3&gt;

&lt;p&gt;학습 도중 발견한 흥미로운 내용으로 &lt;u&gt;‘아하 모먼트’&lt;/u&gt;를 언급한다. 처음엔 잘못 본 줄 알았는데 정말 아하 모먼트였다. 학습 중간의 정책 모델에서 아하 모멘트가 등장하고 난 후, 사고하는 시간에 보다 시간을 할애하고(스스로 생성한 think가 보다 풍부해진다는 뜻 같음) 본인의 초반 접근법(아하 포인트 이전의 의식의 흐름)을 자체 평가하고 개선하는 과정이 추가되었다는 것이다 (아래 예시).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/8072eb65-cea5-4f23-b29f-705319ba7e44&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/90323441-3efb-4410-9066-d47f60f1a13a&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;답변을 내놓는 LLM은 하나의 의사 결정을 하는 정책 모델이다. 이 정책 모델을 학습하기 위해 기준선인 reward를 세우고 템플릿에 맞춰 자체적으로 발전할 수 있게 했더니 o1의 성능을 넘었다는 연구가 되었다.&lt;/p&gt;

&lt;h3 id=&quot;deepseek-r1의-학습법&quot;&gt;Deepseek-R1의 학습법&lt;/h3&gt;

&lt;p&gt;흥미롭지만 일단 여기까지는 &lt;u&gt;수학문제나 코딩에 대한 부분&lt;/u&gt;이다. LLM은 보다 다양한 task에 적용되어야하는데, 다음 step으로 어떤 방법을 썼을까?
Deepseek-R1-Zero로부터 알아낸 결과는 템플릿과 강화 학습의 조합으로 충분히 좋은 추론 능력을 키울 수 있다는 점이었다. 그렇다면 다음과 같은 의문이 들게 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;강화학습 과정은 초반 학습이 불안정하다는 단점이 있다. 이때 만약 적은 양의 high-quality data(사전 학습 데이터)를 통해 수렴 속도를 줄일 수 있는가? (R1-Zero는 오로지 강화학습만으로 학습함)&lt;/li&gt;
  &lt;li&gt;수학이랑 코딩 능력 말고 일반화 능력이 강화됨과 동시에, 앞서 했던 것과 같이 CoT 과정이 보다 사용자 친화적이면서 명확해질 수 있는 방법이 있는가? (R1-Zero는 수학 문제랑 코딩만 함)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 해소하기 위해 Cold Start 방식을 사용하였다. RL을 통해 DeepSeek-V3-Base 모델을 처음부터 학습하는 것이 아닌, 어느 정도 길이의 CoT를 포함한 question/answer 쌍을 답변 데이터로 구성하고 이를 사용하여 fine-tuning한 모델을 시작 포인트로 삼는다. 해당 데이터는 기존 모델의 성능을 해치지 않고 사후의 RL 학습에 도움이 되어야하기 때문에 &lt;u&gt;최대한 깔끔하게 curating하는 과정&lt;/u&gt;을 거치며, 모집한 샘플은 대략 $8\times 10^5$개다.&lt;/p&gt;

&lt;p&gt;그러나 Cold Start로 미세 조정한 모델을 RL로 학습했을때 language mixing 문제가 발생하였는데, 이는 추론 과정이나 답변 과정에 하나의 언어만 포함되지 않고 여러 언어가 포함되는 경우(한글, 중국어, 영어 혼용 등등) 가 발생한다는 사실이다.&lt;/p&gt;

&lt;p&gt;실제로 DeepSeek 사용자들의 리뷰를 봤을 때 &lt;u&gt;한국어를 사용할 경우 종종 경량화 모델에서 언어가 섞여서 출몰한다는 경험담&lt;/u&gt;이 있었는데, 아마 이 문제를 완벽하게 해결하지는 못했나보다. 아무튼 이를 그나마 좀 해소하기 위해 선택한 방법은 RL training 과정에 추가 reward로 language consistency를 준다는 것이다. 해당 리워드 때문에 성능이 좀 저하는 되지만 그래도 일단 LLM이라면 알아듣게는 말하는게 선호도가 더 높다는 것이 DeepSeek의 판단.&lt;/p&gt;

&lt;h3 id=&quot;distilled-model&quot;&gt;Distilled Model&lt;/h3&gt;

&lt;p&gt;Distillation 과정을 통해 원본 모델에 비해 상대적으로 경량화된 버전을 내놓았다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/77e15c84-fec2-4d0e-ae44-daf3739da40d&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;앞서 Cold Start 때문에 모아놨던 샘플을 똑같이 Llama, Qwen 미세 조정에 적용하고, 이후에는 RL 학습을 하지 않고 SFT 방식을 적용했다. 이유는 &lt;u&gt;아래와 같이 RL로 학습하는 것보다, R1 모델로부터 증여받는 것이 훨씬 이득이었기 때문&lt;/u&gt;이다. &lt;strong&gt;이래서 금수저 집안이 좋은가보다ㅠㅠ&lt;/strong&gt;. 그렇다해서 RL 성능이 너무 낮은 것은 또 아니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b7d5ee81-2b05-4a48-b802-c5dc0ed455af&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;근데 사실 LLM 써보면 느끼는 점인데 추론 문제에 대한 정량평가는 수치일 뿐 실제 사용했을때 유저 평가 지표가 좀 더 중요할 듯 하다.&lt;/p&gt;

&lt;h1 id=&quot;내-생각&quot;&gt;내 생각&lt;/h1&gt;
&lt;p&gt;DeepSeek는 오픈소스이다. OpenAI는 클로즈소스이다. 이게 가장 큰 장점이라고 할 순 있겠다. 그런데 메모리 사양이 딸려서 원본 모델을 직접 올려볼 수는 없지만 정말 필요하다면 서버를 확보하고 사용할 수는 있을 것 같다.&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Feb 2025 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/deepseekr1</link>
        <guid isPermaLink="true">http://localhost:4000/blog/deepseekr1</guid>
        
        <category>Deepseek</category>
        
        <category>Reinforcement Learning</category>
        
        <category>LLM</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</title>
        <description>&lt;h3 id=&quot;시작하기-전에-&quot;&gt;시작하기 전에 …&lt;/h3&gt;

&lt;p&gt;이전 글들을 올린 후 꽤나 많은 시간이 지났다. 처음으로 올렸던 글인 &lt;a href=&quot;https://junia3.github.io/blog/lssl&quot;&gt;LSSL(Linear State-Space Layer)&lt;/a&gt;에서는 연속 시퀀스 데이터셋에 대해 딥러닝 모델이 효과적으로 latent space를 정의할 수 있는 구조의 발달 양상을 살펴보았다. 그 중 가장 주요한 키포인트가 되는 HiPPO 논문의 경우 임의의 길이를 가지는 시퀀스의 hidden state를 모델링할 수 있는 근거로 자리잡았고, 이후 LSSL은 레이어 개념으로 확장시켜 HiPPO 행렬의 학습을 통해 성능을 높일 수 있다는 가능성을 보여주게 된다. 그리고 학습이 진행됨에 따라 떨어질 수 있는 학습 안정성 및 수치 엄밀성을 확보하기 위해, 이에 추가로 이후 scalable(데이터 및 모델 확장 가능성)을 높이기 위해 제안된 &lt;a href=&quot;https://junia3.github.io/blog/s4&quot;&gt;S4모델&lt;/a&gt;을 두번째 글로 다루었다. 이제 이러한 기존 SSM based modeling을 근거로 하여, 트랜스포머 모델의 불가피한 연산량 증대를 개선하고자 한 Mamba 모델을 소개한다.&lt;/p&gt;

&lt;h3 id=&quot;트랜스포머-모델의-큰-문제&quot;&gt;트랜스포머 모델의 큰 문제&lt;/h3&gt;

&lt;p&gt;최근 가장 많이 다루게 되는 딥러닝 모델, 혹은 foundation model은 아마 대부분 알고 있겠지만 Transformer에 해당된다. 기계 번역 분야에서 등장한 transformer 구조는 이후 Computer vision, NLP, Audio 등등 모달리티(데이터 형태)의 종류에 무관하게 활발하게 활용되었으며, 가장 중요한 특성인 model scalability(모델 크기와 데이터 크기가 증가함에 따라 성능도 같이 향상됨) 특성이 현존하는 모델링 중 가장 확연하게 드러난 모델이라고 할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/932808ce-badc-4c79-bbc1-0b89df911309&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그러나 트랜스포머 모델에는 가장 큰 문제점이 있다. 그것은 바로 한정된 길이의 시퀀스를 (보통은 트랜스포머 모델은 토크나이저를 통해 문장을 토큰 단위로 분해하고, 이를 임베딩화하여 사용한다) 받아들이며 이를 병렬 연산(Attention)하기 위해 그만큼의 메모리를 소모하게 되고, 이는 결국 동시에 처리 가능한 데이터의 길이가 어느 정도 한정될 수 밖에 없는 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4e615ed8-b192-48ca-b520-7f5c24f39d4b&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;만약 짧은 문장 하나를 트랜스포머에 넣게 되면, 적당히 토크나이징하여 연산을 수행하면 되지만 논문과 같이 긴 줄글의 경우 한정된 연산량 때문에 이를 분리해서 넣게 된다. 가장 큰 문제는 트랜스포머 모델은 논문 제목에서도 볼 수 있듯이(Attention is All you Need) RNN모델과 같이 hidden state를 따로 학습하지 않다 보니, 모델에 들어간 시퀀스 내에서 모든 것을 수행하게끔 되어있다. 즉, 논문 전체를 쿼리로 하여 특정 질문에 대한 주요 부분들을 추출하거나 하는 과정에서는 충분한 연산량이 보장되지 않으면 좋은 성능을 보이지 않게 된다. 이러한 한계점은 곧 트랜스포머 모델의 어텐션 연산을 위한 효율화 작업으로 이어지게 된다.&lt;/p&gt;

&lt;h3 id=&quot;그럼-왜-트랜스포머-모델을-쓰는-건데&quot;&gt;그럼 왜 트랜스포머 모델을 쓰는 건데?&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b7b135a1-b5e0-4ff5-b774-06278574a59f&quot; width=&quot;550&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그럼 대체 왜 꼭 “트랜스포머”여야만 하는가??라고 한다면, 그간 연산 효율화를 위해 단순 어텐션을 벗어난 모델링인 &lt;strong&gt;linear attention,&lt;/strong&gt; &lt;strong&gt;gated convolution, RNN,&lt;/strong&gt;  &lt;strong&gt;structured state space models (SSMs)&lt;/strong&gt; 모두 연구되었지만 Language와 같은 데이터에 대해 트랜스포머의 어텐션보다 높은 성능을 보이지 못했다는 단순한 사실이다. 트랜스포머가 보여준 가능성에 대해 수많은 연구가 진행되었기 때문에 하드웨어 친화적인 알고리즘이나 다양한 학습법 등등 많은 연구가 진행되어 이미 상당히 높은 발전을 이루어낸 트랜스포머 시장에서 적당한 연산량으로 애매한 성능을 보이는 다른 모델이 주목받기가 힘든 상황이다. 특히나 최근 long range dependency/reasoning 에 집중했던 SSM의 경우에는 텍스트와 같이 오히려 정보 집약적인 task에 대해서 높은 성능을 보이지 못했다.&lt;/p&gt;

&lt;h3 id=&quot;mamba-너로-정했다&quot;&gt;Mamba, 너로 정했다&lt;/h3&gt;

&lt;p&gt;맘바는 SSM에 집중한다. SSM은 기본적으로 특정 시스템을 모델링하기 위한 구조로, 이 방법에 attention, RNN에서 사용되는 “gate” 개념을 “selection”으로 가져간 것이다. Attention은 사실 대단한 알고리즘은 아니고, 현재 토큰에 대해 참고가 가능한 시퀀스 내에서 집중할 부분과 무시할 부분을 구분하고, 이를 예측에 활용하는 것이다. 결국 SSM에서도 특정 정보만 선택적으로 활용하는 방법을 사용해볼 수 있는 것이다. 그러나 selective SSM은 일반적인 SSM에 비해 효율적 콘볼루션 연산 등 연산 효율화를 위한 장치를 전혀 사용하지 못하게 된다. 고로 가장 기본적인 연산 방식인 recurrent를 기본적으로 사용하게 되는데, 이때 하드웨어 친화적인 알고리즘을 고안하여 연산 비효율성을 보완하게 되는 것이다. 즉 어텐션과 MLP 없이, selective SSM과 이를 효율적으로 연산할 수 있는 방법을 더하여 Mamba를 만들어낸 것이다. 그래서 사실 맘바 논문의 가장 주요한 키포인트는 SSM에 있다기 보다는 FlashAttention과 비슷한 맥락인 GPU 활용력에 있는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;selection-메커니즘&quot;&gt;Selection 메커니즘&lt;/h3&gt;

&lt;p&gt;이전의 SSM에서 다뤄지지 않은 내용은 “input”에 대해 선택적 알고리즘이 없다는 사실이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/f1a00e64-e44a-4647-ab36-955b5ca4b587&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSM 모델링을 보게 되면 길이에 무관한 모델링을 할 수 있다는 장점이 있지만, 이를 다르게 해석하면 임의의 시점에 들어오는 입력은 모두 동일하게 모델링된 state를 보기 때문에 관찰 시점 이전의 input 혹은 이후의 input 중 일부 무시해야할 내용을 구분할 수 없다. SSM의 가장 큰 장점은 아무리 긴 길이의 입력이 들어오더라도, 모든 정보를 함축적으로 모델링할 수 있다는 사실이다. 그러나 위와 같은 예시를 보면 어텐션과의 차이점이 크게 드러난다. 예컨데 어떤 텍스트의 초반에 “고양이를 5년 전부터 키워왔다.”라는 정보가 있고, 중간에 그와 무관한 이야기인 “시골집에서 살던 내용”이 포함되어있고, 텍스트 후반부에 고양이 이름이 나와있는 경우를 생각해보자.&lt;/p&gt;

&lt;p&gt;해당 쿼리에 대해 “OOO는 몇살 정도로 예상되는가?”라는 질문을 받는다고 가정하면 어텐션 모델은 우선적으로 해당 질문과 가장 큰 연관성을 지니는 “고양이 이름은 OOO이다.” 라는 내용과 “대략 5년 전부터 고양이를 키웠다.”라는 내용을 참고하겠지만, SSM은 중간에 있는 시골집에 살던 내용까지 전부 참고하여 정답을 내놓게될 것이다. 물론 SSM이 잘못된 대답을 내놓지 않고 정답을 내놓을 수 있지만, 결국 말하고자 하는 것은 이처럼 정보가 집약적인 데이터(특정 부분에 집중해야 제대로 된 QnA가 가능한 데이터 구조)에 대해서는 어텐션 만큼 효율적이고 정확한 방법이 없다는 것이다. 따라서 맘바에서는 기존 SSM 구조에 추가로 입력 신호에 대해 SSM 파라미터(이전의 정보들)를 다변화하는 구조를 통해 Selection 매커니즘을 추가하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;하드웨어-친화적-알고리즘&quot;&gt;하드웨어 친화적 알고리즘&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/5cdd0409-44f4-4a39-a19d-dccc25209325&quot; width=&quot;750&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이전의 모든 SSM을 위한 효율화 알고리즘은 selective SSM에는 적용되지 않는다. 이는 LTI(Invariant)와 같은 모델링 구조에서 기본적으로 입력 및 시간에 무관하게 시스템은 동일하다는 가정을 가지고 있기 때문이다. 따라서 기존 논문에서 제안된 콘볼루션 기반 방법들을 모두 사용할 수 없게 되었고, 오로지 recurrent 연산을 사용할 수 밖에 없으므로, 이를 하드웨어 친화적으로 연산하는 방법을 고안하게 된다. 실제로 구현된 하드웨어 친화적인 알고리즘은 시퀀스 길이에 대해 Linear한 복잡도를 가지게 되어, 이전 콘볼루션 기반 알고리즘이 pseudo-linearity를 가졌던 것에 비해 recurrent 연산을 더욱 효과적으로 수행할 수 있게 되었다.&lt;/p&gt;

&lt;h3 id=&quot;모델링&quot;&gt;모델링&lt;/h3&gt;

&lt;p&gt;모델 구조는 간단하다. Selective SSM 구조를 하나의 모듈처럼 취급하여, 기존 트랜스포머 모델을 구성하는 MLP 파트를(Attention 및 Projection 등등) SSM 모듈로 갈아끼워서 사용한다. Selective SSM이 아닌 일반적인 SSM에 대한 내용은 이전에 다루었던 글들을 통해 간단하게 이해하고 오면 좋다. 간단하게 소개하자면 대개 Structured SSM은 4개의 파라미터$(\Delta, A, B, C)$를 기본으로 적용되며, 이를 discretize한 ($\overline{A}, \overline{B}, C$)을 사용한다. SSM은 LTI 시스템을 기반으로 하여 시간에 따른 시스템 함수 불변성을 가정하였으나, 이러한 불변성이 필연적으로 가지는 한계 때문에 맘바에서는 기존에 적용될 수 있었던 연산 효율성을 포기하고 Selective SSM을 채택하게 된다. 이 부분에서는 어떠한 파트가 구체화되어 Selective SSM이 설계되었는지 단계별로 정리하고자 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Compression(축약)을 위한 Selection&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;시퀀스를 다루는 모든 모델링은 임의의 길이를 가지는 ‘문맥’을 어떻게 하면 작은 크기의 ‘hidden state’ 혹은 ‘latent’로 함축하는가?를 다루게 된다. 모든 시퀀스 모델링은 해당 관점에서의 trade-off를 고려할 수 밖에 없는데, 대표적인 시퀀스 모델링에 해당되는 ‘트랜스포머(Transformer)’는 문맥을 전혀 압축/함축하지 않는다는 특징을 가지고 있다. 이러한 특징은 autoregressive한 추론 단계에서 Key-Value 문맥 전체를 참조하기 위해 길이에 따라 연산 속도 및 메모리가 증가하게 되며, 이는 트랜스포머의 quadratic-time consuming의 주된 원인으로 작용한다. 반대로 RNN과 같은 recurrent model은 한정된 크기의 state를 가진다는 점에서 학습 효율성을 가지나, 과연 한정된 크기의 state에 얼만큼 context(문맥)이 잘 요약될 수 있는가가 문제점으로 작용된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/50f92a2c-e803-4282-9a65-6ed99513892b&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 그림에 나타난 생성 task를 보게 되면 이러한 trade-off system을 SSM(LTI system)의 레벨 단에서 이해하기 쉽다.&lt;/p&gt;

&lt;p&gt;좌측에 나타난 task는 입력으로 들어온 연속 신호 중 일부분(연속되어 색칠된 부분)을 복사하여 생성하는 과정이다. LTI system이 처리할 수 있는 가장 기본적인 형태의 delay라고 볼 수 있다. 즉 LTI system으로 매핑 가능한 일반적인 모델로 간단하게 수행할 수 있는 과제이다.&lt;/p&gt;

&lt;p&gt;우측에 나타난 두 가지의 task 중 위쪽은 입력으로 들어온 연속 신호 중 관련 신호(색칠된 부분)과 무관한 신호(흰색 부분)을 구분하고, 관련 신호를 입력 순서대로 복사하여 생성하는 과제이다. 앞선 복사 task처럼 LTI system으로 수행될 수 없기에 time-varying system 및 non-linear system이 활용되어야하는 것을 볼 수 있다. 아래쪽은 Induction heads라는 과제로, 흔히 요즘 LLM에서의 In-context learning에서 대두되는 task라고 볼 수 있다. 입력으로 넣어준 일련의 신호에 대해 맥락을 파악하고, 이후에 특정 신호(검정색 토큰)를 입력으로 넣어줬을때 문맥에 맞는 정답을 내놓게되는 것이다(파란색 토큰). 이 역시 입력 신호에 대해 어떤 특정 신호가 뒤따를지 모르기 때문에 시스템이 문맥에 대한 추론이 필수적이고, 이를 위한 모델링을 추가로 수행해야한다.&lt;/p&gt;

&lt;p&gt;결국 위의 그림으로 이해하고자 한 것은 여러 복잡한 생성 이론을 효과적으로 수행하기 위해 “선택적으로” 문맥을 이해하는 과정을 모델링에 추가해야 한다는 사실은 시퀀스를 처리하는 모든 모델링이 다루는 문제라는 사실이다. 해당 문제를 수행하기 위해 기존 방법론들을 총정리했을때, trade-off로서 context 용량과 효율성 간의 합의점이 필요하고, 현재 다루는 모델에서 이를 어떻게 적용해낼지(Attention으로 일부 특징들을 걸러낼 것인지, Recurrent module로 문맥을 요약한 state를 구축할 것인지) 고민하게 된다. 그렇기 때문에 SSM에서도 비슷한 맥락으로의 구조가 필요하고, 기존 시퀀스 모델에서 개별적으로 적용되던 context compression의 수단으로 selection mechanism을 넣은 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SSM에 selection을 넣기&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/3ef341c3-98a9-41a5-9fde-a0458e508df2&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 Mamba는 SSM에 어떻게 selection mechanism을 심느냐는 것인데, 저자는 RNN의 recurrent dynamics나 CNN의 파라미터와 같이 직접적으로 입력에 영향을 주는 “파라미터”를 입력 신호에 따라 바꾸는 방식을 생각해냈다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1bd20d5b-4055-401d-8275-6030b5737bef&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;좌측과 우측을 비교하게되면 그 차이가 나타난다. 주된 차이는 $B, C, \Delta$ 파라미터가 &lt;strong&gt;더이상 입력(배치) 및 각 입력의 타이밍(길이)에 무관하지 않고&lt;/strong&gt;, 입력 및 출력 신호와 동일한 크기를 가지는 것을 볼 수 있고, 이는 더이상 Time-invariant system이 아닌 Time-variant system이 되었다는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;또한 $B, C. \Delta$가 어떻게 정해지는지 우측을 잘 보게되면 $s_B(x), s_C(x), s_\Delta(x)$와 같은 방식으로 입력 신호에 대한 함수로 표현이 되어있는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
s_B(x) = \text{Linear}_{N}(x),~s_C(x) = \text{Linear}_{N}(x)
]&lt;/p&gt;

&lt;p&gt;이는 가장 간단한 형태로, dimension $D$인 입력을 받아 $N$인 출력을 해주는 Linear module로 parameterize하여 함수를 구성하고,&lt;/p&gt;

&lt;p&gt;[
s_\Delta(x) = \text{Broadcast}_D(\text{Linear}_1(x))
]&lt;/p&gt;

&lt;p&gt;이산 신호의 간격은 위와 같이 스칼라 값을 dimension에 브로드캐스팅하는 형태로 함수를 구성하였다. 이렇게 SSM을 시간 변화에 따른 함수로 파라미터화 하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;하드웨어 친화적 알고리즘&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Convolution 모델이라던지, Attention 모델들은 하드웨어 친화적으로 설계가 되었다. 콘볼루션 커널은 입력 크기와 무관하게 항상 일정한 receptive field 크기를 가져 메모리를 최적화할 수 있으며, 어텐션은 길이에 따라 메모리 및 시간이 증가하기는 하지만 HBM 대신 &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;SRAM에서 잘 동작할 수 있는 알고리즘&lt;/a&gt;이 등장했으니까 (실제로 FlashAttention 저자인 Tri Dao가 맘바 저자로 참여했음). Selective SSM도 비록 LTI system을 사용할 수 없게 되어버렸지만 분명 학습 효율화할 수 있는 부분은 있을 것이다. 기존 방법들의 한계점은 다음과 같다.&lt;/p&gt;

&lt;p&gt;(1) SSM과 같은 recurrent model은 표현력(state size)과 속도 사이의 합의점이 필요하다. 높은 표현력을 가지면서도 속도 저하가 심하지 않은 방법을 찾는 것이 목적.&lt;/p&gt;

&lt;p&gt;(2) Recurrent가 Convolution보다 더 유연하다. 후자가 전자의 확장판이기 때문에 latent state 구축을 위한 연산량이 (B, L, D, N) 만큼 필요한데, 이러한 문제를 해결하려는 방법이 나옴 (&lt;a href=&quot;https://arxiv.org/pdf/2111.00396&quot;&gt;S4 모델&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;(3) 기존의 LTI state model은 표현력 확보를 위한 state dimension $N$의 넉넉한 확보를 위해 dual recurrent-convolutional form을 고안함.&lt;/p&gt;

&lt;p&gt;우리는 이제 selection mechanism을 적용하기 때문에 LTI system을 사용할 수 없다. LTI system이 가지는 한계점을 해결하기 위해 Selective SSM을 고안하였으나 연산 비효율성 문제를 해결해야한다는 점에 직면하게 된다. 저자는 문제를 해결하기 전 두가지 중요한 특징을 활용한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent를 단순하게 적용할 경우 FLOPs는 $O(BLDN)$, Convolution은 $O(BLD\log(L))$으로 적용된다. 즉 시퀀스 길이가 길어질수록 적당한 크기의 hidden state dimension $N$에 대해 오히려 Recurrent 연산이 적은 연산량을 가진다.&lt;/li&gt;
  &lt;li&gt;두가지 주된 문제는 recurrent 연산의 순차성과(병렬적 연산이 안됨)과 큰 메모리 사용 문제에 직면한다. 후자의 경우에는 convolution과 같이 굳이 전체 state $h$를 구성하지 않아도 된다는 개선점이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결국 주된 아이디어는 엄청 특별한 내용은 아니고, hidden state $h$를 GPU에서 효율적으로 연산할 수 있는 방법들(kernel fusion, parallel scan, recomputation)로 빠르게 구해보자는 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/80d323ef-1213-43a0-988b-e4c56892d6b9&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSM의 시스템 주축이 되는 $\overline{A}, \overline{B}$를 직접 HBM에서 계산하지 않고, SSM parameter $A, B, C, \Delta$를 SRAM단으로 로드, 이산화 작업을 거져 다시 HBM에 쓰는 방식을 취한다. 또한 순차성 부분은 스캔할 타이밍에서 parallel scan algorithm을 적용하게 된다. 이로써 적은 메모리 bandwidth를 가지는 SRAM과의 데이터 송수신 관련 코스트를 최소화하여 사용한다. 이외의 backpropagation 시의 recomputation 방식은 FlashAttention과 하드웨어적으로 동일하게 적용된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Neural Network에 Mamba 섞기&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Structured SSM(S4)와 마찬가지로 Selective SSM(Mamba) 또한 시퀀스에 대한 변환 모듈에 해당되기 때문에 neural network에 적용할 수 있다. 맘바의 구조를 종합적으로 이해하기 위해서는 H3와 Gated Unit을 이해하는 과정이 필요하다. 속칭 ‘배고픈 하마 (&lt;a href=&quot;https://arxiv.org/pdf/2212.14052&quot;&gt;Hungry Hungry Hippos&lt;/a&gt;)’라 불리는 H3의 경우 트랜스포머의 Attention Algorithm의 효과를 따라갈 수 있는 SSM 구조 모델링을 위해 Shifting SSM과 Recalling SSM을 구별하고, 이를 multiplication으로 엮는 시도를 하게 된다. 이렇게 모델링하게 되면 Q, K, V로 추출되는 입력에 대한 정보가 Shifting SSM에서 이전 입력을 참조하기 위해 옮겨주는 역할을 수행하고, 만약 현재 입력 정보가 기억이 된다면(Shifting $\odot$ SSM), 그 이후 입력에 대한 출력값(Value $\odot$ SSM)을 응답으로 내놓는 구조가 된다. Gated MLP의 경우에도 결국 트랜스포머의 Attention 구조를 MLP 구조에 통합하고자 한 구조에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e9830657-b21e-459e-90ad-75883d2b34c9&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;즉, 맘바의 경우에도 기본적으로 SSM 구조를 사용하기 때문에 트랜스포머의 Attention 효과를 활용하고자 했던 H3와 근본적으로 문제시하는 부분이 동일하다. 그렇기 때문에 내부적으로 연산되는 SSM 부분은 H3와 동일하다. 그러나 차이가 있는 점은 H3는 Linear Attention의 Q, K, V 구조를 활용하였지만, Mamba에서는 이러한 어텐션 구조를 전혀 사용하지 않고 Gated MLP를 2개의 SSM 시스템을 Wrapping하는 방식으로 구조화하였다.&lt;/p&gt;

&lt;h3 id=&quot;각-요소별-효과&quot;&gt;각 요소별 효과&lt;/h3&gt;

&lt;p&gt;이와 같이 모델링했다. 이때의 각 요소별 효과를 간략하게 서술하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable Spacing : 언어와 같이 Discrete data에 대해서 문맥 해석에 무관한 신호를 무시할 수 있게 된다. 문맥을 생성하는 상황에서 문맥에 무관한 신호를 제외하여($g_t = 0$) 보다 풍부한 문맥을 생성해낼 수 있다.&lt;/li&gt;
  &lt;li&gt;Filtering Context : 각 상황에서 문맥의 중요도를 결정한다. 경우에 따라 일부 문맥을 무시해야할 경우가 생기는데, 시간 불변성을 지니는 LTI system에서는 이를 적용할 수 없으나, selective SSM인 맘바에서는 상황에 따라 문맥을 필터링할 수 있게 된다.&lt;/li&gt;
  &lt;li&gt;Transformer는 구조상 문맥상에서 독립적인 문구를 어텐션하여 사용할 수 있지만(필요한 부분을 불연속적으로 추출 가능) LTI system에서는 문맥을 하나의 연속적인 형태로 보기 때문에 이러한 특성이 발견되지 않는다. 그러나 Selective SSM에서는 연속된 시간 단위에서의 $\Delta$, 혹은 이전 hidden state를 무시할 수 있는 gate $g_t$의 값이 유동적으로 학습될 수 있기 때문에 이러한 특성을 찾을 수 있다는 가능성이 있다.&lt;/li&gt;
  &lt;li&gt;일반적으로 문맥의 간격에 해당되는 $\Delta$는 현재 입력에 얼마나 집중할 지 결정해주게 된다. 바로 위의 꼭지에서 언급했던 것과 같이 $\Delta \rightarrow \infty$가 되는(커지는) 상황이 되면 이전의 state를 무시하고 현재의 입력에 집중하는 형태가 될 것이고, 반대로 작아지는 경우에는 이전의 state를 현재 입력보다 중요시하는 형태가 될 것이다.&lt;/li&gt;
  &lt;li&gt;$A$ 파라미터는 기존의 시스템에서는 hidden state를 구축하는 역할을 수행했었다. 마찬가지로 Selective SSM에서도 같은 역할을 수행하지만, 차이점은 유동적인 $\Delta$와 discretization되어 구축되는 문맥 시스템에 selective 속성을 부여할 수 있다는 것이다. (아래 수식 참고)&lt;/li&gt;
  &lt;li&gt;$B, C$ 파라미터는 gated system에서 현재 입력 $x_t$에 대한 정보를 문맥에 추가할 것인지, output $y_t$를 내보내는 과정에서 state 정보를 얼마나 활용할 것인지 결정하는 역할을 수행한다. (아래 수식 참고)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{h_t}^\prime = A\mathbf{h_{t-1}} + B\mathbf{x_t} \newline
\mathbf{y_t} = C\mathbf{h_t}+D\mathbf{x_t}
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/22d57a8c-f16a-4d3a-8d37-8836c00e2d2c&quot; width=&quot;350&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/57b3fb58-76b5-49ac-b0b8-8a5a771d5bfe&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Selective copying(좌측) 그리고 Induction head(우측) 각각의 성능이 기존 SSM baseline에 비해 월등히 좋아지는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/d20fc202-06ad-467d-a433-95ee96d9f829&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 확연히 좋아지는 부분은 Perplexity인데, 연산량이 늘어날수록(모델의 파라미터 수가 증가할수록) 문맥에 대한 생성 능력이 확연히 올라간 모습을 보여준다. 이전까지는 H3까지도 어텐션에 필적하지 못했던 부분이었는데, 맘바를 통해 꽤나 많이 따라잡은 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9f5559cf-358e-4b2c-b417-a5c827146bc9&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;여러 downstream task에 대해 zero-shot 성능을 확인하였다. 파라미터 수가 증가할수록 perplexity는 감소하고 average는 증가하였고, 다소 적은 파라미터 수를 가지고도 좋은 성능을 보인다. 이외에 DNA, Audio modeling등 다른 시계열 모달리티에 대해서도 좋은 성능을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/270b68a9-05ce-4ac2-96a6-98885e0baccd&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Mamba의 장점 중 가장 주요한 포인트는 context 길이가 길어질 경우에 연산량 및 추론 시간을 줄일 수 있다는 점인데, 실제로 Attention을 효율화한 FlashAttention과 비교했을 때에도 Mamba의 inference time 및 throughput이 좋아지는 것을 볼 수 있다.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Sep 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/mamba</link>
        <guid isPermaLink="true">http://localhost:4000/blog/mamba</guid>
        
        <category>Mamba</category>
        
        <category>Selective SSM</category>
        
        <category>H3</category>
        
        <category>Gated MLP</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;첫번째 글을 올린 이후 상당히 많은 시간이 지났다. 중간에 ECCV 논문 리부탈과 졸업 준비, 논문 등등 바쁜 일이 참 많았다. 결국 졸업도 해서 학위도 받고 ECCV 논문도 억셉되고 여러모로 좋은 일이 대부분 끝났지만 SSM 공부를 한 지 너무 오랜 시간이 지나버려서 논문을 처음부터 다시 읽어야한다는 큰 문제가 생겨 늦어지게 되었다. 아이고야 …&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;시작하기-전에&quot;&gt;시작하기 전에,&lt;/h3&gt;

&lt;p&gt;HiPPO는 SSM 구조에서 Long-term range를 구축하기 위한 matrix $A$ 구조의 중요성을 언급하였고, LSSL에서는 SSM을 연속적으로 존재하는 $A$ 전부에 대해 이를 일반화하였다. 이전 글에서 Mamba modeling의 기초가 되는 LSSL에 대해서 설명했었고, 해당 글에 간단한 수식과 관련된 증명을 첨부했었다. 솔직한 심정을 담아 말해보자면, 아직 본인은 이러한 기본 내용들을 전부 이해하지 못했다고 생각하고 있고 이 글을 통해서 맘바를 이해하고자 하는 것은 너무 돌아가는 과정이라고 생각되기도 한다. 자신감 없이 말한 감이 없지 않아 있지만 결국  맘바 모델링 자체를 이해하는데 있어서 state-space modeling을 완전히 뼛속부터 이해할 필요는 없다고 생각한다 (Bottom-up 보다는 Top-down이 맞는 방향이라는 개인적인 의견).&lt;/p&gt;

&lt;p&gt;그리고 여러 블로그에 보면 시각화와 함께 맘바 모델링을 한큐에 이해할 수 있게 쉽게 정리해둔 글도 많이 보인다. 그럼에도 불구하고 굳이 글을 길게 써서 리뷰했던 이유는 앞선 글에서 말했던 것처럼 맘바의 근본적인 내용에 대해 이해해보려 노력하는 과정이 의미가 있다고 생각했기 때문이다. 맘바를 간단하게만 이해한다면 맘바가 도대체 왜 transformer 구조가 가지는 문제들을 해결할 수 있었는지, 그리고 단순히 새로운 아키텍쳐로서 등장했다고 해서 무조건 좋다고 생각해야하는 것은 아니기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;무엇이든 쉽게 얻으면 그만큼 쉽게 잃는 법이니까.&lt;/em&gt; 분명이 맘바가 가지는 특징을 제대로 이해할 수 있다면 맘바가 가지는 근본적인 장단점을 발견할 수 있을 것이고, 이를 통해 향후 연구 및 모델링 개발의 기반이 될 것이다. 이번 글에서는 &lt;strong&gt;LSSL가 가지는 연산량과 연산 불안정성을 해결하고자 한&lt;/strong&gt; &lt;strong&gt;S4 모델링&lt;/strong&gt;에 대해서 정리해보도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;다시-시퀀스-모델링으로-돌아가서&quot;&gt;다시 시퀀스 모델링으로 돌아가서,&lt;/h3&gt;

&lt;p&gt;시퀀스 모델링에서 가장 중요한 것은 적당한 길이의 시퀀스를 얼마나 효율적으로 처리하는가와 시퀀스의 길이가 길어질수록 참조할 수 있는 문맥의 길이도 그에 따라 길어져야한다는 것이다. 트랜스포머는 연산량을 희생하여 단일 연산으로 전체 시퀀스에 대한 어텐션 정보를 획득할 수 있고, 토큰 임베딩의 길이를 늘임으로써 이를 해결할 수 있었으나 결국 연산량의 한계가 있다는 문제가 있다. RNN 및 CNN 각각이 가지는 특징들도 있지만 모든 모델링은 각자가 가지는 장단점 때문에 만능일 수는 없었고, 이에 대한 대응으로 SSM(state-space modeling)을 제안한 것이 바로 LMU, HiPPO를 비롯한 논문들이었던 것이다.&lt;/p&gt;

&lt;h3 id=&quot;그래서-ssm을-정리하자면&quot;&gt;그래서 SSM을 정리하자면,&lt;/h3&gt;

&lt;p&gt;SSM은 한마디로 Linear Time-Invariant System으로 latent space를 구축하고자 한 것이다. LTI system의 미분 방정식을 구성하는 matrix가 시간 불변성을 가진다는 특징은 결국 연속 신호를 이산화한 관측 단위에서 미분 방정식은 동일한 식으로 구축되며, 따라서 시퀀스 길이에 무관하게 동일한 latent space $x(t)$를 만들어낼 수 있다는 것이다. gate에 의존하는 RNN과는 다르게 &lt;strong&gt;특정 구조를 가지는&lt;/strong&gt; Matrix $A$가 있다면 실제 딥러닝 모델의 예측에 가장 중요한 특징 벡터인 latent를 long range dependent하게 뽑아낼 수 있게 된다. 트랜스포머로 각 토큰 단위로 어텐션을 구하든, RNN 구조로 연속으로 들어오는 데이터로 implicit latent를 만들든 그런 방법들이 아니라 실제로 시간 불변성이 성립하는 latent space를 예측해낼 수 있다면 아무리 오랜 시간이 흘러도 (관측 범위가 처음과 크게 벗어나도) hidden space는 동일한 함수로 구현될 것이기 때문에 Long Range Dependency를 보장할 수 있다는 것이다. 따라서 어텐션 연산에 대해 고정된 문맥 길이를 가지는 Transformer 구조에 비해 상대적으로 더 긴 길이의 시퀀스 데이터를 처리할 수 있고 이론상으로는 무한한 길이의 인풋을 감당 가능하다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/c69d9e00-7f30-4602-bd17-473a67899bae&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;논문에 나와있는 그림을 통해 이해해보도록 하자. 우선 좌측의 “Continuous State Space” 부분을 확인해보면, SSM 구조에서 인풋에 해당하는 $u(t)$가 행렬 $A, B, C, D$로 구성된 Linear system의 상태(이를 hidden state, 혹은 latent state $x(t)$라 부른다)를 통해 모델링된 출력값 $y(t)$를 내게 된다. 이때 $x(t)$가 지속적으로 변화하는 input to output $\mathbf{u} \rightarrow \mathbf{y}$를 저장하는 메모리 역할을 수행하게 된다면, 이론상 불변하는(고정된 요소를 가지는) 행렬 $A, B, C, D$에 대해 꾸준이 이전 정보를 저장할 수 있게 된다. 바로 이것이 중앙에 보이는 “Long-Range Dependencies”에 해당되고, 실제로 이에 대한 구조화된 행렬의 효과성을 입증한 것이 “&lt;strong&gt;HiPPO: Recurrent Memory with Optimal Polynomial Projections”&lt;/strong&gt; 논문에 해당된다.&lt;/p&gt;

&lt;p&gt;이때 기본적으로 SSM은 “Recurrent system”을 가지게 되는데, 이는 시스템의 구조가 입력에 대한 출력 구조로 이어져있으며, 이전 입력 대비 출력 결과에 따른 시스템 변화가 이후 입출력에 영향을 미치게 된다는 것이다. 이를 연산하는 방식으로는 귀납적으로(Recurrent) 연산 후 연산을 하는 방식으로도 구현이 가능하지만 단순화하여 콘볼루션 연산으로 수행하는 것도 가능하다. 그러나 여전히 고차원의 데이터에 대해 필연적으로 증가하는 행렬 연산($e.g.$ $A, B, C, D$ 행렬이나 $\bar{K}$) 때문에 &lt;strong&gt;연산량이 높다는 문제&lt;/strong&gt;가 생긴다.&lt;/p&gt;

&lt;h3 id=&quot;잘-구조화된-ssm을-사용하면-되지-않을까-s4&quot;&gt;“잘” 구조화된 SSM을 사용하면 되지 않을까?? (S4)&lt;/h3&gt;

&lt;p&gt;따라서 지금 글에서 다루고자 하는 논문인 “&lt;strong&gt;Efficiently Modeling Long Sequences with Structured State Spaces&lt;/strong&gt;” (S4)에서의 목적은 다음과 같다. 기존에 LSSL(단순 SSM)의 높은 연산량을 해결하기 위한 방법들이 제안되었지만, 모두 연산상에 numerical stability(연산 안정성 혹은 연산 엄밀성)이 부족했다. 그렇기 때문에 연산 안정성도 높임과 동시에 기존에 존재하던 “잘” 구조화된 행렬에 적용 가능한 알고리즘들을 활용하기 위해 SSM의 토대가 되는 matrix $A$를 &lt;strong&gt;다시 구조화하는 전략을 사용&lt;/strong&gt;하였다. 바로 원래 $A$의 rank보다 훨씬 낮은 rank(서로 독립인 column의 갯수를 의미하며, 낮은 rank를 가지는 matrix는 독립이 아닌 column을 모두 배제할 경우 그만큼 차원 수를 줄여서 표현 가능하다)를 가지는 요소와 normal matrix(특수한 케이스의 정사각 행렬로, commutable한 특징이나 diagonal 요소로 분리가 가능하다는 등등의 특징을 사용하여 non-normal matrix에 비해 빠르게 연산이 가능하다) 요소로 분리가 가능하다는 점을 사용한다.&lt;/p&gt;

&lt;p&gt;또한 기존의 SSM이 coefficient space(latent를 표현하는 함수는 사전 정의된 여러 orthogonal한 함수들의 coefficient 가중합으로 표현하고자 했었다.)로 접근하는 방식을 사용했다면, 이번에는 주파수 차원으로 올려서 계산하게 된다. 시간 단위에서의 콘볼루션은 주파수 단위에서의 곱연산으로 표현된다.&lt;/p&gt;

&lt;p&gt;이를 통해 &lt;strong&gt;Low-Rank 행렬은 Woodbury identity로&lt;/strong&gt;, &lt;strong&gt;Normal 행렬은 Cauchy kernel로&lt;/strong&gt; 교정 가능하며 이를 토대로 연산량을 $O(N^2L)$에서 $\tilde{O}(N+L)$로, 메모리는 $O(NL)$에서 $O(N+L)$로 줄일 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;기존-ssm과-표현-방식&quot;&gt;기존 SSM과 표현 방식&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;긴 문맥을 보존할 수 있게끔 모델링&lt;/strong&gt;된 matrix $A$ (e$.g.$ HiPPO 행렬)을 사용한다. 그렇게 되면 다음과 같은 연립 미분 방정식으로 표현되는 시스템을 구축할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{x}^\prime = A\mathbf{x} + B\mathbf{u} \newline
\mathbf{y} = C\mathbf{x}+D\mathbf{u}
\end{aligned},\quad A^{\text{HiPPO}}_{n, k}= -\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
n+1,&amp;amp;\text{if }n = k \newline
0,&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;이때 일반적인 컴퓨팅 시스템에서는 Continuous system을 Discrete(이산화된 입력)으로 변화하는 과정이 필요하다. 이를 적용한 식이 실제 SSM에서 적용될 Discrete SSM이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_k = \overline{A}x_{k-1} + \overline{B}u_k\quad &amp;amp;\overline{A} = (I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A)  \newline
y_k = \overline{C}x_k,\quad &amp;amp;\overline{B} = (1-\Delta/2\cdot A)^{-1}\Delta B \newline
&amp;amp;C = \overline{C}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이에 대한 증명이나 보다 자세한 내용은 &lt;a href=&quot;https://junia3.github.io/blog/lssl&quot;&gt;이전 게시글인 LSSL&lt;/a&gt;을 보고 오면 좋다. 아무튼 Discrete SSM이 의미하는 바는 SSM이 결국 recurrent 연산 구조를 가지기 때문에 &lt;strong&gt;RNN&lt;/strong&gt;의 연산 특징을 가진다는 것. 이산화된 Matrix $\overline{A}$가 hidden state $x$의 transition matrix 역할을 수행한다. 헌데, 위의 식에 대한 hidden state와 output을 $x_{-1} = 0$라 가정한 후에 전개하면, convolution kernel에 대한 연산으로도 표현이 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_0 =&amp;amp; \overline{B}u_0,\quad x_1 = \overline{AB}u_0 + \overline{B}u_1,\quad x_2 = \overline{A}^2\overline{B}u_0+\overline{A}\overline{B}u_1 +\overline{B}u_2,~\cdots \newline
y_0 =&amp;amp; \overline{CB}u_0,\quad x_1 = \overline{CAB}u_0 + \overline{CB}u_1,\quad x_2 = \overline{CA}^2\overline{B}u_0+\overline{CA}\overline{B}u_1 +\overline{CB}u_2,~\cdots \newline
y_k =&amp;amp; \overline{K} \ast \mathbf{u},\quad \overline{K}\in\mathbb{R}^L := \mathcal{K}_L(\overline{A}, \overline{B}, \overline{C}) := (\overline{C}\overline{A}^i\overline{B})_{0\le i &amp;lt; L}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;그러므로 만약 convolutional filter $\overline{K}$에 대해 알고 있다는 가정을  한다면 FFT(빠른 콘볼루션 연산 알고리즘)을 통해 연산 속도를 개선할 수 있지만, 이 필터를 연산하는 과정 자체도 행렬곱이 필요하며 non-normal matrix $\overline{A}$에 대해 일반화된 연산을 하기에는 어려움이 따르게 된다. 즉, 이 논문에서 하고자 하는 것은 해당 필터를 어떻게 효율적으로 연산하는가에 대한 부분이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;방법론--diagonalization-대각화&quot;&gt;방법론 : Diagonalization (대각화)&lt;/h3&gt;

&lt;p&gt;행렬의 대각화 (Diagonalization)는 행렬의 고유값(eigenvalue)인 $\lambda$와 고유벡터(eigenvalue)를 활용하여 대상이 되는 행렬을 고유값이 대각선 성분인 행렬로 만드는 과정이다. 예컨데 대각화(Diagonalization)이 가능한 행렬 $A \in \mathbb{R^{n \times n}}$가 존재한다면, 이 행렬의 eigenvalue $n$개와 이에 대응되는 eigenvector $n$에 대해서 $\Lambda = V^{-1}AV$로 표현 가능하다. 이때, $\Lambda$와 $V$는 각각 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\text{For eigenvalues }\{ \lambda_i\}_{i=1}^n \text{ and eigenvectors } \{v_i\}_{i=1}^n,\quad\Lambda = \begin{bmatrix}
\lambda_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \newline
0 &amp;amp; \lambda_2 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \newline
0 &amp;amp; 0 &amp;amp; 0&amp;amp; \lambda_{n-1} &amp;amp; 0 \newline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \lambda_n 
\end{bmatrix},\quad V = \begin{bmatrix}
\mid &amp;amp; \mid &amp;amp; \cdots &amp;amp; \mid \newline
v_1 &amp;amp; v_2 &amp;amp; \cdots &amp;amp; v_n \newline
\mid &amp;amp; \mid &amp;amp; \cdots &amp;amp; \mid \newline
\end{bmatrix}
]&lt;/p&gt;

&lt;p&gt;이런 상황에서, 기존의 식을 조금 바꿔보면 다음과 같이 정리할 수 있다. 우선, 일반적으로 SSM에서 $D = 0$으로 간소화하여 사용하기 때문에 $\mathbf{y} = C\mathbf{x}$로 표현하도록 하겠다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{x}^\prime =&amp;amp; A\mathbf{x} + B\mathbf{u} \newline
\mathbf{y} =&amp;amp; C\mathbf{x}
\end{aligned}\quad\rightarrow\quad
\begin{aligned}
\tilde{\mathbf{x}}^\prime =&amp;amp; V^{-1}AV\tilde{\mathbf{x}} + V^{-1}B\mathbf{u} \newline
\mathbf{y} =&amp;amp; CV\tilde{\mathbf{x}}
\end{aligned} 
]&lt;/p&gt;

&lt;p&gt;두 식이 서로 조금 달라보이지만, 우측과 같이 전개된 시스템의 좌측에 전부 $V$를 곱하게 되면, $x = V\tilde{x}$인 SSM과 동치인 것을 알 수 있다. 즉 입력 대비 출력으로 이어지는 관계성 $\mathbf{u} \rightarrow \mathbf{y}$은 동일한 SSM 시스템이고, 이때의 system latent는 행렬 $A$의 eigenvector matrix $V$에 의해 바뀌게 된다. 위의 식처럼 새로운 시스템에서의 $A$행렬에 해당되는 대각 행렬 $V^{-1}AV$을 구축할 수만 있다면, 앞서 보았던 콘볼루션에서 $A$의 곱연산의 연산량을 효과적으로 줄일 수 있다. (이를 &lt;strong&gt;Vandermonde product&lt;/strong&gt;라 한다. 그냥 그렇다고 생각하고 넘어가자)&lt;/p&gt;

&lt;p&gt;그러나 유감이지만, &lt;strong&gt;HiPPO 행렬에 대한 대각화는 진행할 수 없다&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1f009a21-3ccd-4c23-8f98-e07852ef9bb6&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그 이유는 슬프게도 HiPPO 행렬이 대각화되면서 고유벡터로 이루어진 matrix $V$의 성분이 &lt;strong&gt;너무 커지기 때문&lt;/strong&gt;이다. 쉽게 말하면, 컴퓨터 상에서 연산이 안정적이려면(실제 수학적 계산값과 일치하려면), 행렬 연산 과정에서 행렬 요소가 너무 큰 값을 가지면 안되기 때문에 불가능하다는 것이다. 앞서 보여준 HiPPO 행렬 $A$를 대각화하면 요소가 $V_{ij} = {i+j \choose i-j}$가 되는데 (combination), state size $N$이 커지면 커질수록 최대 $2^{4N/3}$까지 커지는 요소를 감당할 수 없다 ($e.g.$ 출력값인 $CV\tilde{\mathbf{x}}$ 연산에 문제가 생길 수 있다.).&lt;/p&gt;

&lt;h3 id=&quot;방법론--normal--low-rank-matrix&quot;&gt;방법론 : Normal + Low Rank Matrix&lt;/h3&gt;

&lt;p&gt;위에서 다룬 것은 대각화를 통해 연산의 용이성을 올려보자!라는 내용이지만, 결국 기본적인 HiPPO 행렬에 대해서 적용하기는 힘들고 추가 작업이 필요하다는 것을 암시한다. 가장 이상적인 조건은 대각화의 대상이 되는 행렬 $A$가 unitary matrix와 같이 특수 행렬로 대각화가 가능한 경우에 해당된다. 선형 대수에서 이러한 조건이 만족하는 행렬 $A$의 모음을 &lt;strong&gt;“normal matrices”&lt;/strong&gt;라 부른다. 눈치챘을 수도 있지만 당연히 HiPPO matrix는 normal matrix가 아니고, 그렇기 때문에 대각화할 때 &lt;strong&gt;고유벡터 요소가 커지는 문제&lt;/strong&gt;가 발생한다.&lt;/p&gt;

&lt;p&gt;다행이지만 저자는 HiPPO matrix $A$는 normal matrix가 아니지만, 해당 행렬을 normal matrix와 low rank matrix의 합으로 나타낼 수 있음을 발견하였다. 하지만 문제는 콘볼루션 필터 연산 시 합(Normal + Low Rank)에 대한 제곱 연산이 필요한데, 이 역시 시간이 오래 걸리고 최적화가 필요한 부분이라는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;[
\overline{K}\in\mathbb{R}^L := \mathcal{K}_L(A, B, C) := (CA^iB)_{0\le i &amp;lt; L}
]&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 &lt;strong&gt;세 가지&lt;/strong&gt; 알고리즘을 추가로 적용하여 커널 필터를 계산하게 된다. 각 알고리즘에 대한 내용을 아래 그림과 관련지어 정리하면 다음과 같다. 아직 디테일하게 설명한 부분이 없어 그림의 내용에 대한 이해는 못하지만 순차적으로 보도록 하자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/58fb4f95-545d-4853-9a35-073c447c990a&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 알고리즘을 따라가기 위해서는 행렬 $A$가 NPLR (Normal + Low-Rank) 혹은 DPLR (Diagonal + Low-Rank)로 표현이 가능함을 이해하고 넘어가야한다. 이 부분은 논문에 Appendix C.1. 파트를 보면 empirical하게 모든 HiPPO 행렬에 대해 입증해 놓았다. 사실 수식으로의 이해는 필요가 없는 부분이고, &lt;strong&gt;그냥 받아들이면 되는 파트&lt;/strong&gt;다.&lt;/p&gt;

&lt;p&gt;그런 뒤, 기존의 콘볼루션 커널을 계산하던 방식에서 차이를 두게 된다. $\overline{K}$를 직접 계산하지 않고  $\overline{K}$의 Discrete Fourier Transform(DFT) 변환 형태인 $\hat{K}$룰 사용한다. DFT는 이산화된 시간 축에서의 신호를 (여기서는 필터의 요소인 $CB, CAB, CA^2B, \cdots$ 를 연속된 시간 축에서의 신호로 생각하면 된다) 이산화된 주파수 축으로의 스펙트럼으로 바꿔주는 변환에 해당되고, DFT 변환 및 이의 역변환 IDFT 알고리즘은 Fast Fourier Transform (FFT)라 하며 연산 속도는 $O(L\log L)$ 선에서 해결 가능하다.&lt;/p&gt;

&lt;p&gt;[
\hat{K}_j = \sum_{k=0}^{L-1} \overline{K}_k\exp(-2\pi j\frac{k}{L})
]&lt;/p&gt;

&lt;p&gt;뒤에서 더 자세히 정리하겠지만, Truncate SSM을 구성하여 &lt;strong&gt;스펙트럼 단위에서 연산&lt;/strong&gt;하게 되면 필터 연산 시 $A$를 여러 번 곱하는 방식에서 벗어나 &lt;strong&gt;한번의 행렬 연산으로도 연산이 가능&lt;/strong&gt;하게 된다. 이 과정에서 골칫거리인 term인 $(1-\overline{A}^L)$가 발생하는데 (결국 powering이 필요), 이를 &lt;strong&gt;reparameterization&lt;/strong&gt;을 통해 반복된 연산을 피해 메모리 절약 및 속도 향상을 할 수 있게 된다. 그리고 위에서 가정한 구조화를 통해 스펙트럼 커널 $\hat{K}$를 연산하는 것이 “Cauchy kernel”과 동일함을 알 수 있고, 효율적인 알고리즘을 적용할 수 있다. 짧게 정리했지만 실제 순서대로 간단하게 표현하면 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;모든 HiPPO 행렬 $A$를 &lt;strong&gt;NPLR (DPLR)로 표현이 가능&lt;/strong&gt;하고, 이를 적용하여 $A \rightarrow \overline{A}$ (Discretize)를 $O(N)$ 연산으로 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;$\overline{K}$의 truncate SSM generating function은 DFT랑 동일하다. 따라서 &lt;strong&gt;주파수 축으로의 변환 및 역변환을 통해 연산 가능&lt;/strong&gt;하며, 이때 $\overline{A}$의 반복된 제곱 연산 대신 단일 연산으로 바꿀 수 있다.&lt;/li&gt;
  &lt;li&gt;위의 연산 과정이 &lt;strong&gt;Cauchy kernel 연산 구조와 동일&lt;/strong&gt;하므로 &lt;strong&gt;효율적인 알고리즘이 적용 가능&lt;/strong&gt;하다.&lt;/li&gt;
  &lt;li&gt;이때 inverse는 &lt;strong&gt;Woodbury’s Identity를 사용하면 간소화가 가능&lt;/strong&gt;하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;방법론에-대한-보다-자세한-설명&quot;&gt;방법론에 대한 보다 자세한 설명&lt;/h3&gt;

&lt;p&gt;앞서 소개한 효율화 과정에 대해서 자세하게 살펴보자. 개인적으로는 디퓨전 논문보다 어려운 것 같다. 그렇지만 힘내보자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/63ca889d-5447-4ed1-bb3c-771d3a6f4c45&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;HiPPO 행렬은 모두 “NPLR (Normal Plus Low-Rank)”로 표현 가능하다. HiPPO 행렬은 총 4가지가 존재하지만, 가장 보편적인 케이스인 LegS에 대해서만 살펴보면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
A^{\text{HiPPO}} _{n, k}= -\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
n+1,&amp;amp;\text{if }n = k \newline
0,&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;요 식 모든 요소에 $\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2}$를 더해주게 되면 다음과 같은 식이 된다.&lt;/p&gt;

&lt;p&gt;[
-\begin{cases}
\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
\frac{1}{2},&amp;amp;\text{if }n = k \newline
-\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;이 식의 대각 성분을 따로 분리하게 되면 $-\frac{1}{2}I + S$로 표현 가능하고, 이때 $S$는 normal matrix에 속하는 &lt;strong&gt;skew-symmetric matrix&lt;/strong&gt;가 된다. 또한 원래의 행렬의 모든 요소에 같은 값을 더하는 행렬은 rank가 $1$인 행렬이다. 모든 HiPPO 행렬에 대한 증명은 논문을 참고하면 되고, 이를 통해 암시할 수 있는 사실은 모든 HiPPO state 행렬을 Diagonal part + Low-Rank part로 분리 가능하다는 사실이다. 이를 다음과 같이 표현하도록 하겠다. 논문에서는 증명 과정에서 conjugate($\ast$) 표시를 사용했는데, 본인은 이 기호가 조금 익숙치 않아서 &lt;strong&gt;transpose 기호($\top$)로 대체하여 사용&lt;/strong&gt;하도록 하겠다.&lt;/p&gt;

&lt;p&gt;[
A = \Lambda - PQ^\top
]&lt;/p&gt;

&lt;p&gt;이제 이렇게 대체된 식으로 discrete system matrix를 표현해보면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\overline{A} &amp;amp;= (I-\Delta / 2 \cdot A)^{-1} (I+\Delta / 2 \cdot A) \newline
\overline{B} &amp;amp;= (1-\Delta / 2 \cdot A)^{-1} \Delta B \newline \newline
I + \Delta/2\cdot A &amp;amp;= I + \Delta / 2 \cdot\left(\Lambda - PQ^\top\right) \newline
&amp;amp;= \frac{\Delta}{2} \left(\frac{2}{\Delta}I + \Lambda - PQ^\top\right) \newline
&amp;amp;= \frac{\Delta}{2}A_0 \newline \newline
(I-\Delta/2 \cdot A)^{-1} &amp;amp;= \frac{2}{\Delta}\left(\frac{2}{\Delta}-\Lambda+PQ^\top\right)^{-1} \newline
&amp;amp;= \frac{2}{\Delta}\left(D-DP(I+Q^\top DP)^{-1}Q^\top D \right) \newline
&amp;amp;= \frac{2}{\Delta}A_1, \text{ where } D = \left(\frac{2}{\Delta}-\Lambda\right)^{-1} \text{ (Diagonal term)}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;$A_0$을 계산하는 것은 크게 문제가 없는데 $A_1$ 연산에는 큰 문제가 있는데, 바로 역행렬 연산이다. 행렬 차원 수 $N$이 증가할수록 연산량이 기하급수적으로 늘어난다 . 따라서 역행렬 연산을 DPLR 행렬에 대해 위와 같이 &lt;strong&gt;Woodbury’s Identity를 통해 단순화&lt;/strong&gt;할 수 있다. 대각화된 행렬에 대한 inverse는 쉽게 구할 수 있으며, 뒤에 붙는 low-rank term에는 무관하게 연산이 가능하므로 전체 계산식에 대한 역행렬 연산보다 단순화할 수 있다는 것이다. Woodbury’s Identity의 경우에는 앞으로 전개될 증명 과정에 계속 활용되기 때문에 계속 인지하고 있는 편이 용이하다 (DPLR 구조의 행렬만 가지면 계속 효율적으로 적용이 가능).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Woodbury’s Identity&lt;/strong&gt;는 다음과 같이 적용할 수 있다. &lt;strong&gt;교환 법칙이 성립하는 세 행렬&lt;/strong&gt; $A\in\mathcal{R}^{N \times N}, U, V \in \mathcal{R}^{N \times p}$에 대해 (여기서 $\mathcal{R}$은 commutative ring으로, 이에 속하는 원소들에 대해서는 곱연산에 대해 교환 법칙이 성립한다고 생각하면 된다.)&lt;/p&gt;

&lt;p&gt;[
(A+UV^\top)^{-1} = A^{-1}-A^{-1}U(I_p + V^\top A^{-1}U)^{-1}V^\top A.
]&lt;/p&gt;

&lt;p&gt;위의 식을 만족하게 된다.&lt;/p&gt;

&lt;p&gt;암튼 구한 식으로 다시 discrete system을 정의해보면, DPLR인 행렬 $A_1,$ $A_0$에 대해 $O(N)$의 연산량으로 해결 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_k =&amp;amp; A_1A_0x_{k-1} + 2A_{1}Bu_k \newline
y_k =&amp;amp; C^\top x_k
\end{aligned}.
]&lt;/p&gt;

&lt;p&gt;그리고 저자는 이 부분에서 row vector였던 $C$를 column vector로 간주하여 다른 파라미터($B, P, Q$)들과 shape을 맞추었기 때문에, 필자도 이에 따라 요 부분부터는 기존 시스템 식의 $C$를 $C^\top$으로 바꿔 표기하도록 하겠다.&lt;/p&gt;

&lt;p&gt;이제 이산화된 시스템 행렬은 얼추 알겠고, 사실 가장 중요한 것은 Recurrent system에서의 콘볼루션 필터 $\overline{K}$를 빠르게 연산하는 것이다. DPLR이 행렬의 이산화 과정에서 Woodbury’s Identity를 활용할 수 있게 되면서 효율성을 올려준다는 사실을 인지하였으나, 실제로 콘볼루션 필터를 연산하는 과정에서의 반복곱 연산에서는 큰 도움이 되지 않는다는 것을 알 수 있다. 반복곱 연산 대신, DPLR를 활용하기 위해서는 역행렬 연산이 필요하므로 스펙트럼 단위로 넘기는 (coefficients) generating function을 생각해볼 수 있다. 예컨데 무한한 길이의 콘볼루션 필터 신호가 있다고 가정해보자.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}(\overline{A}, \overline{B}, \overline{C}) = \{\overline{C}^\top\overline{B},\overline{C}^\top \overline{A}\overline{B},\overline{C}^\top \overline{A}^2\overline{B},\ldots\}
]&lt;/p&gt;

&lt;p&gt;우리가 현재 신호에 대해 가질 수 있는 것은 이상적인 콘볼루션 필터 중 $L$의 길이를 가진 한정된 길이의 필터이다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}_L(\overline{A}, \overline{B}, \overline{C}) = \{\overline{C}^\top\overline{B},\overline{C}^\top \overline{A}\overline{B},\overline{C}^\top \overline{A}^2\overline{B},\ldots,\overline{C}^\top \overline{A}^{L-1}\overline{B}\}
]&lt;/p&gt;

&lt;p&gt;길이가 $L$인 이산(discrete) 신호는 주파수 $2\pi\times\frac{0}{L} \sim 2\pi \times \frac{L-1}{L}$의 성분으로 분해가 가능하다. 이 주파수를 표현하는 unit $z$라는 변수로 표현한다면, 이를 $z$함수에 대한 coefficient의 집합으로 대체 가능하며 이를 $z$-transform이라고 부른다. 이때, 일반적으로 $z$는 복소수 단위(&lt;strong&gt;Real + Imag&lt;/strong&gt;)를 의미하며 주파수 단위에서는 이를 오일러 각도 변환 식인 ($e^{-i\Omega}$)에서 $\Omega =\{\frac{2\pi l}{L}\}_{l=0}^{L-1}$의 합으로 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\hat{K}_L(z; \overline{A}, \overline{B}, \overline{C}) := \sum_{i=0}^{L-1} \overline{C}^\top\overline{A}^i\overline{B}z^i = \overline{C}^\top(I-\overline{A}^L)(I-\overline{A}z)^{-1}\overline{B}
]&lt;/p&gt;

&lt;p&gt;이러한 변환을 DFT(Discrete Fourier Transform)이라 부르며, &lt;strong&gt;이산화된 시간 축의 신호를 이산화된 주파수 축으로 변환하는 과정&lt;/strong&gt;에 주로 활용된다. 맨 앞단의 $\overline{C}^\top(I-\overline{A}^L) = \tilde{C}$로 두게 되면,&lt;/p&gt;

&lt;p&gt;[
\hat{K}_L(z; \overline{A}, \overline{B}, \overline{C}) = \tilde{C} (1-\overline{A}z)^{-1}\overline{B}
]&lt;/p&gt;

&lt;p&gt;이 식에서 discretized matrix $\overline{A}, \overline{B}$ 의 closed form으로 대체하여 $A, B$에 대해 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\overline{A} =&amp;amp; (I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A) \newline
\overline{B} =&amp;amp; (1-\Delta/2\cdot A)^{-1}\Delta B
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이 식을 그대로 위에 대입하게 되면,&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\tilde{C}^(I-\overline{A}z)^{-1}\overline{B} =&amp;amp; \tilde{C}\left(I-(I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A)z \right)^{-1}\overline{B} \newline
=&amp;amp; \tilde{C}\overline{B}\left(I-\frac{\Delta}{2}A\right)\left(\left(I-\frac{\Delta}{2}A\right)-\left(I+\frac{\Delta}{2}A\right)z\right)^{-1} \newline
=&amp;amp; \tilde{C} \Delta B\left( I(1-z) - \frac{\Delta}{2}A(1+z)\right) \newline
=&amp;amp; \frac{\Delta}{1-z}\tilde{C} \left( I - \frac{\Delta A}{2\frac{1-z}{1+z}} \right)^{-1}B \newline
=&amp;amp; \frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -A \right)^{-1}B
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;+앞서 우리는 시스템 행렬이 DPLR(Diagonal Plus Low-Rank)임을 보였기 때문에 다시 표현하게 되면,&lt;/p&gt;

&lt;p&gt;[
\frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -A \right)^{-1}B = \frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -\Lambda+PQ^\top  \right)^{-1}B
]&lt;/p&gt;

&lt;p&gt;이제 앞서 언급했던 Woodbury’s Identity를 사용해볼 수 있다. 식을 간소화하기 위해 다음과 같이 두개 되면,&lt;/p&gt;

&lt;p&gt;[
R(z) = \left( \frac{2}{\Delta}\frac{1-z}{1+z} - \Lambda \right)^{-1}
]&lt;/p&gt;

&lt;p&gt;최종적으로는 다음 식으로 전개할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\tilde{C}(1-\overline{A}z)^{-1}\overline{B} = \frac{2}{1+z}\left(\tilde{C}R(z)B - \tilde{C}R(z)P(1+Q^\top R(z)P)^{-1} Q^\top R(z)B \right).
]&lt;/p&gt;

&lt;p&gt;여기서 생길 수 있는 의문점은, 앞서 식을 전개하는 과정 상에서 앞단의 $\overline{C}^\top(I-\overline{A}^L) = \tilde{C}$로 정의된 부분이다. 원래대로라면 매번 $\overline{A}^L$를 연산해야 하지만, 단순히 학습 파라미터를 초기에 $\tilde{C}$로 초기화한 상태로 학습 가능하게끔 &lt;strong&gt;reparameterization&lt;/strong&gt;을 하게될 경우 이에 따른 연산 코스트를 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;이제 마지막 단계까지 왔다. 결국 전개한 식을 요약하자면, $\overline{K}$를 연산하는 부분을 generating function $\hat{K}_L(\Omega; \overline{A}, \overline{B}, \overline{C})$로 계산하고, 이때 $\overline{A}$가 Diagonal 성분을 가짐을 사용하여 풀어낸 식이다. 그런데 이렇게 풀어낸 식이 결국 Cauchy kernel이랑 정확하게 일치하고, Cauchy kernel은 효율적으로 연산할 수 있는 알고리즘이 존재한다. 우선 Cauchy matrix / kernel의 구조는 다음과 같이 정의된다.&lt;/p&gt;

&lt;p&gt;[
M \in \mathbb{C}^{M \times N} = M(\Omega, \Lambda) = (M_{ij})_{0&amp;lt;=i &amp;lt; M,~0&amp;lt;=j&amp;lt;N},\quad M_{ij} = \frac{1}{\omega_i - \lambda_j}
]&lt;/p&gt;

&lt;p&gt;최종 식에서의 $Q^\top R(\Omega, \Lambda)P$ 부분을 살펴보게 되면(기존의 $z$를 unit $\Omega$의 각 요소에 대해 생각해볼 수 있다), 이를 계산하는 과정이 &lt;strong&gt;Cauchy matrix-vector multiplication 연산량으로 계산 가능&lt;/strong&gt;하다는 것이다. 예컨데 원래대로라면 길이 $L$인 콘볼루션 커널을 총 $N$만큼의 hidden state에 대해 연산하려면 $O(LN)$ 만큼의 연산량이 소모되는데, 약간의 오차를 허용하면 이를 $O((L+N) \log(L+N) \log \frac{1}{\epsilon})$의 연산량으로 처리가 가능하다. 증명하는 과정은 거의 불필요한데 요약하자면, $Q^\top R(\Omega, \Lambda)P$를 계산하는 것은 $\Omega$에 속하는 모든 $\omega$에 대해 우리는 $\sum_{j} \frac{q_j^\top p_j}{\omega-\lambda_j}$를 구하고자 하는 것과 같으며, 결국 이 식은 Cauchy kernel의 형태와 같기 때문이다 ($\omega \neq \lambda_j$ 라는 조건은 항상 성립함).&lt;/p&gt;

&lt;p&gt;이렇게 알고리즘이 모두 정리가 되었다. 다시 앞서 간단하게 언급했던 알고리즘을 다시 가져오면 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저, $A$가 DPLR이라는 점에서 시작하여 식을 전개하였고, 이때 $\overline{K}$를 다이렉트로 계산하지 않고 generating function $\hat{K}$를 계산하기 위해 푸리에 변환을 실시하였다. 이때 나온 식에서 $\overline{C}$를 reparameterization해준다. 그런 뒤, discretized된 모든 matrix를 state matrix의 closed form으로 표현한 뒤, Woodbury’s Identity를 통해 식을 다시 전개하면,&lt;/p&gt;

&lt;p&gt;[
\begin{bmatrix}\tilde{C}^\top &amp;amp; Q\end{bmatrix}^\top \left(\frac{2}{\Delta} \frac{1-\omega}{1+\omega} -\Lambda \right)^{-1} \begin{bmatrix}B&amp;amp;P\end{bmatrix}
]&lt;/p&gt;

&lt;p&gt;가 된다. 다만 본인의 식 전개와 실제 논문 알고리즘에서 살짝 다른 부분이 있다면 필자는 $\tilde{C}^\top = \tilde{C}$로 쭉 전개해왔다는 사실이다. 같은 구조이기 때문에 큰 문제는 없다고 생각된다. 아무튼 이렇게 계산된 각 요소들로 효율적으로 계산된 $\hat{K}$에 다시 푸리에 역변환을 적용하면 $\overline{K}$를 구할 수 있게 된다. 드디어 이 논문의 아이디어가 되는 모든 알고리즘을 이해할 수 있었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/77edbbc5-5a6d-475b-923a-4a3814a9175e&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;s4로-구성한-deep-layer의-구조&quot;&gt;S4로 구성한 Deep layer의 구조&lt;/h3&gt;

&lt;p&gt;앞에서 본 내용을 통해 S4에 필요한 파라미터는 다음과 같다는 것을 알 수 있다. $A$는 HiPPO 행렬의 어떠한 형태로 초기화가 될 예정이고, 해당 $A$는 그 형태에 맞게끔 특정 Diagonal 및 vector인 $\Lambda, P, Q, B, C$로 구성된다. Diagonal도 실질적으로 대각 성분 이외에는 다른 파라미터를 저장할 필요가 없기 때문에 S4는 state dimension $N$에 대해 총 $5N$의 학습 가능한 파라미터 수를 가진다. S4 자체는 Linear mapping 이지만 (동일한 길이의 시퀀스를 뽑아내는 구조), 이를 여러층 쌓고 Non-linearity를 더하게 되면 결국 Deep layer로 사용될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;결론은&quot;&gt;결론은…&lt;/h3&gt;

&lt;p&gt;실험 결과를 따로 보여주기보다는 이 상태로 마무리하는게 좋을 것 같다. 실험 결과야 당연히 efficiency를 보여주면서 long-range 효과성을 유지하는 그림이겠거니, 예상했고 논문에 예상한 그대로가 display된 것을 볼 수 있다. 가장 중요하다고 생각한 점은 모델링의 기초가 되는 SSM을 풀어냈던 이전 논문으로부터 개선점을 지속적으로 잡아내고 (예컨데, 행렬곱을 단순화하기 위해 구조를 분해하고 이를 수식으로 증명하는 것) 이러한 과정에서 연속으로 contribution을 낼 수 있다는 점을 배우게 된 것 같다.&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Jul 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/s4</link>
        <guid isPermaLink="true">http://localhost:4000/blog/s4</guid>
        
        <category>Mamba</category>
        
        <category>HiPPO</category>
        
        <category>LSSL</category>
        
        <category>S4</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>InfoBatch - Lossless Training Speech-Up By Unbiased Dynamic Data Pruning 논문 리뷰</title>
        <description>&lt;h1 id=&quot;개요-및-문제&quot;&gt;개요 및 문제&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Large-scale dataset(대용량의 데이터셋)&lt;/strong&gt;을 활용하는 &lt;strong&gt;딥러닝 알고리즘&lt;/strong&gt;은 상당한 발전을 이루고 있지만,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(1) 학습 시간이 오래 걸린다는 점&lt;/strong&gt;, &lt;strong&gt;(2)&lt;/strong&gt; 학습 시간을 줄이기 위해서는 &lt;strong&gt;Resource(리소스)가 많이 든다는 점&lt;/strong&gt;에서 활용하기 어렵다. 따라서 이를 해결하기 위한 여러 방법들이 소개되었다.&lt;/p&gt;

&lt;h3 id=&quot;절대적인-샘플-수-줄이기&quot;&gt;&lt;strong&gt;절대적인 샘플 수 줄이기&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;가장 대표적으로 생각해볼 수 있는 방법은 원래의 데이터셋을 &lt;strong&gt;그대로 활용하는 것과 엇비슷한 성능을 낼 수 있는&lt;/strong&gt; 상대적으로 작은 용량의 데이터셋을 구하거나 합성해내는 것이다. 소규모 데이터셋 집합을 &lt;strong&gt;Synthesize(합성)&lt;/strong&gt;하는 &lt;strong&gt;Dataset distillation&lt;/strong&gt;과 &lt;strong&gt;Choose(선택)&lt;/strong&gt;하는 &lt;strong&gt;Coreset selection&lt;/strong&gt;이 대표적인 방법에 속한다. 그러나 이 방법들은 소규모 데이터셋을 구성하는 과정에서 많은 리소스와 시간이 소모되며, 성능 하락을 피할 수 없다는 문제가 있다.&lt;/p&gt;

&lt;h3 id=&quot;모델-수렴에-도움이-되는-샘플-위주로-샘플링&quot;&gt;&lt;strong&gt;모델 수렴에 도움이 되는 샘플 위주로 샘플링&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;데이터셋 중 상대적으로 모델 학습에 &lt;strong&gt;도움이 되는 샘플&lt;/strong&gt;과 &lt;strong&gt;도움이 되지 않는 샘플&lt;/strong&gt;로 구분이 가능하다고 가정하면(각각 수렴 속도가 빠르고 느리다는 것과 대응된다), 이들 모두를 동등한 확률로 샘플링하여 독립 항등 분포(&lt;em&gt;i.i.d.&lt;/em&gt;) 배치 단위로 학습에 활용되는 것보다 빠른 수렴 속도에 기여할 수 있는 샘플을 더 높은 확률로 뽑는 방법을 생각해볼 수 있다. 이를 &lt;strong&gt;Weighted sampling&lt;/strong&gt;이라 부르지만, 데이터셋과 모델 구조의 변화에 따라 성능 차이가 심한 문제가 있다.&lt;/p&gt;

&lt;h3 id=&quot;큰-배치-사이즈로-학습&quot;&gt;&lt;strong&gt;큰 배치 사이즈로 학습&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;리소스가 한정되어있지 않다고 해서 배치 사이즈를 &lt;strong&gt;무작정 키우는 것&lt;/strong&gt;이 모델 수렴에 도움이 되지 않는 경우도 있다. 이러한 문제를 해결하기 위해 배치 크기가 큰 세팅에서의 학습을 위한 &lt;strong&gt;LARS, LAMB&lt;/strong&gt;와 같은 &lt;strong&gt;Optimizer(최적화 알고리즘)&lt;/strong&gt;이 제안되었다. 하지만 앞서 말했던 바와 같이 &lt;strong&gt;한정된 리소스 때문에&lt;/strong&gt; 무작정 배치 사이즈를 키울 수는 없다.&lt;/p&gt;

&lt;h3 id=&quot;iteration-수-줄이기&quot;&gt;&lt;strong&gt;Iteration 수 줄이기&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Dataset pruning&lt;/strong&gt;은 학습 시 &lt;strong&gt;최적화에 사용되는 샘플 수를 줄인다는 점&lt;/strong&gt;에서 Coreset selection과 크게 보면 비슷하지만, Coreset selection의 경우 전체 데이터셋을 대표하는 소규모의 데이터셋을 구성하는 것이 목적이라면, 이 방법은 &lt;strong&gt;정보량이 높은 샘플들을 학습에 활용한다는 점&lt;/strong&gt;에서 차이가 있다.
정보량에 대한 필터링은 각 샘플의 score를 예측하는 방법을 사용하게 되는데, 정보량이 적은 샘플을 학습 과정에서 아예 제외하는 방식&lt;strong&gt;(static pruning)&lt;/strong&gt; 방법은 제대로된 score 예측을 위해 데이터셋 전체에 대해 사전 작업 과정인 trial의 연산량이 높다는 단점이 있다. 이는 원래의 데이터셋의 크기가 커질 수록 더 심각한 문제가 된다. 따라서 정보량이 적은 샘플을 학습 과정에서 유동적으로 제외하는 방식&lt;strong&gt;(dynamic pruning)&lt;/strong&gt; 방법이 제안되었고, 연산량을 줄이기 위해 &lt;strong&gt;얻기 쉬운 logit, loss 혹은 gradient를 활용&lt;/strong&gt;하여 사전 작업 과정인 trial을 없애고 학습 과정에 pruning을 통합시킬 수 있었다.&lt;/p&gt;

&lt;p&gt;앞서 소개한 여러 방법 중 &lt;strong&gt;InfoBatch&lt;/strong&gt;는 4번 approach 중 &lt;strong&gt;dynamic pruning&lt;/strong&gt;을 다루게 된다. 하지만 단순한(naive approach) dynamic pruning은 &lt;strong&gt;학습이 치우치는(gradient biasing) 문제&lt;/strong&gt;가 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/b6021859-0b9c-49e0-b368-71a6b5d4c6dd&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그림에서 보여지는 &lt;strong&gt;EL2N Score&lt;/strong&gt;는 샘플 별로 예측된 확률과 실제 확률(one-hot vector)간의 차이에 대한 L2-Norm을 수치화한 값이다. 상대적으로 실제 확률과 차이가 큰 샘플들이 큰 loss value를 가질 것이고, 학습에 활용되었을때 모델의 최적화와 빠른 수렴에 도움이 될 것은 어느 정도 직관적인 결과로 예측된다. 하지만 해당 score를 기준으로 모든 샘플을 pruning하는(EL2N score 임계치보다 작은 샘플을 모두 학습에서 제외) 방법은 gradient biasing 문제를 가져오며, gradient biasing이란 원래의 데이터셋을 사용했을 때 모델이 최적화되는 방향과 다른 방향으로 모델이 학습되는 것을 의미한다. 즉, 단순한 data pruning로는 loss의 크기에 대해서는 고려해줄 수 있지만 &lt;strong&gt;gradient의 방향을 제대로 고려해줄 수 없다는 것&lt;/strong&gt;이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;contribution&quot;&gt;Contribution&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/2dbd05fb-0168-43f8-a56a-4c2dcee07330&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;따라서 논문에서 제안한 방법은 위와 같다. 각 샘플별로 구한 score를 기준으로 임계치보다 작은 샘플들을 전부 pruning하는 것이 아닌, 이들 중 &lt;strong&gt;랜덤 샘플링을 통해 적은 갯수의 샘플만 학습에 활용&lt;/strong&gt;하게 된다. 또한 EL2N Score를 사용하지 않고 forward propagation을 통해 직접 구해지는 &lt;strong&gt;loss value를 score로 활용&lt;/strong&gt;한다. 랜덤 샘플링을 통해 score가 임계치보다 작은 샘플들의 gradient를 보존할 수 있고, 샘플 수에 의한 gradient bias는 &lt;strong&gt;보존된 gradient를 rescaling함으로써 조정&lt;/strong&gt;할 수 있다. 논문의 Contribution을 다음과 같이 정리해볼 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존 dynamic batch 방법이 가지고 있던 gradient bias 문제를 해결하였다.&lt;/li&gt;
  &lt;li&gt;EL2N은 pruning cycle마다 소팅에 $O(\log N)$ 만큼의 연산(시간)량이 요구되었으나, 이 방법의 경우 소팅 과정이 불필요하므로 $O(1)$의 연산(시간)량으로 pruning이 가능하다.&lt;/li&gt;
  &lt;li&gt;데이터셋을 전부 사용하지 않고도 성능을 유지할 수 있었으며, classification, segmentation, SSL, LLM 등 다양한 task에 적용 (plug-and play)이 가능하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h3 id=&quot;previous-approach&quot;&gt;Previous approach&lt;/h3&gt;

&lt;p&gt;이전 static/dynamic pruning 접근법의 문제점과 InfoBatch 방법을 보다 디테일하게 보면 다음과 같다.&lt;/p&gt;

&lt;p&gt;원래의 데이터셋인 $\mathcal{D} = \{z_i = (x_i, y_i)\}_{i=1}^{\vert \mathcal{D} \vert}$와 각 샘플마다 정의되는 score $\mathcal{H}(z)$가 있을때, static pruning은 다음 조건과 같이 특정 score 임계치 $\bar{\mathcal{H}}$보다 작은 모든 샘플을 학습에서 제외시킨다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{S} = \{z \vert \mathcal{H}(z) \ge \bar{\mathcal{H}}\}
]&lt;/p&gt;

&lt;p&gt;이를 training step 단위로 진행하는 방식이 dynamic pruning이다. Score $\mathcal{H}_t(z)$ 가 시간에 따라 변할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{S}_t = \{z \vert \mathcal{H}_t(z) \ge \bar{\mathcal{H}}_t\}
]&lt;/p&gt;

&lt;p&gt;두 방식 모두 학습 시 $\mathcal{S}$ 혹은 $\mathcal{S}_t$에 &lt;strong&gt;포함되지 않은 샘플들을 제외시킨다&lt;/strong&gt;는 공통점이 있지만, dynamic pruning이 학습 과정에서 전체 데이터셋에 참조가 가능하기 때문에 gradient bias 문제는 상대적으로 적을 수 있다. 하지만 pruning cycle마다 소팅하여 샘플링한다고 하더라도 &lt;strong&gt;low-score sample(학습에서 제외되는 샘플)이 지속적으로 겹칠 수 있기 때문&lt;/strong&gt;에 bias되는 문제는 근본적으로 해결할 수 없다.&lt;/p&gt;

&lt;p&gt;Biasing 문제와 더불어 &lt;strong&gt;데이터셋  전체를 참조할 수 없게될 확률이 높아진다는 것&lt;/strong&gt;은 전체 데이터셋을 학습에 활용할 때보다 성능 하락 문제를 가져오며, 매번 pruning 과정에서 scoring / pruning을  진행하기 때문에 데이터셋 크기가 커질수록 학습 시간에 영향을 미치는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;infobatch&quot;&gt;InfoBatch&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/d739728f-c050-45e5-b64e-fd8b3344bcf9&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;일반적인 dynamic pruning과는 다르게, 임계치보다 작은 score를 가지는 샘플들 중에서 &lt;strong&gt;랜덤하게 추출된 샘플만 pruning&lt;/strong&gt;하게 된다. InfoBatch에서는 하이퍼파라미터인 pruning probability $r$에 대해 다음과 같은 pruning policy를 적용한다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{P}_t(z) = \begin{cases}
r,&amp;amp;\mathcal{H}_t(z) &amp;lt; \bar{\mathcal{H}}_t \newline
0,&amp;amp;\mathcal{H}_t(z) \ge \bar{\mathcal{H}}_t
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;임계치보다 score가 큰 샘플&lt;/strong&gt;은 $100\%$ 모두 사용하고, &lt;strong&gt;임계치보다 score가 작은 샘플&lt;/strong&gt;은 $100(1-r)\%$ 만 사용한다. 또한 Forward propagation에서 계산된 loss 값의 평균을 임계치로 사용하게 되어 추가 연산 및 소팅하는 과정없이 pruning할 수 있다. Loss 값을 기준으로 삼은 이유를 저자는 두 가지로 밝혔다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Extra-cost (추가 연산)없이&lt;/strong&gt; 바로 구할 수 있기 때문이다.&lt;/li&gt;
  &lt;li&gt;Loss 값이 &lt;strong&gt;각 샘플의 learning status(학습 정도)를 대표&lt;/strong&gt;할 수 있기 때문이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;샘플의 score에 해당되는 $\mathcal{H}_t$는 매 epoch마다 업데이트되는데, 이때 $t$번째 epoch에서 학습에 관여되지 못한 샘플들(pruning된 샘플)은 optimization에 사용되지 않았기 때문에 score를 업데이트하지 않고, 학습에 관여한 샘플들($\mathcal{S}_t$에 속한 샘플)은 $t$번째 계산된 loss로 score를 업데이트하게 된다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{H}_{t+1}(z) = \begin{cases}
\mathcal{H}_t(z),&amp;amp;z \in \mathcal{D} \backslash\mathcal{S}_t \newline
\mathcal{L}(z),&amp;amp;z \in \mathcal{S}_t
\end{cases}
]&lt;/p&gt;

&lt;p&gt;첫번째 epoch 학습 시에는 이전 학습 결과가 없기 때문에 score를 $1$로 초기화한 상태로 시작한다.&lt;/p&gt;

&lt;h3 id=&quot;이론적-배경과-rescaling-방법&quot;&gt;이론적 배경과 rescaling 방법&lt;/h3&gt;

&lt;p&gt;앞서 설명한 내용은 InfoBatch에서 기본적으로 사용한 dynamic pruning 방법에 대한 리뷰였고, 실제로 &lt;strong&gt;랜덤하게 pruning된 dataset을 제대로 활용&lt;/strong&gt;하기 위한 이론적 내용을 정리하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;모든 딥러닝 모델은 학습하고자 하는 loss function $\mathcal{L}$을 가지고 있으며, 만약 연속 확률 밀도 분포 $\rho(z)$에서 추출되는 모든 데이터셋 $\mathcal{D}$를 사용하여 모델을 학습할 경우, 다음과 같이 &lt;strong&gt;loss function의 전체 평균을 최소화&lt;/strong&gt;하는 모델 파라미터 $\theta \in \Theta$를 찾고자 한다.&lt;/p&gt;

&lt;p&gt;[
    \underset{\theta \in \Theta}{\arg \min} \underset{z\in\mathcal{D}}{\mathbb{E}} (\mathcal{L}(z, \theta)) = \int_z \mathcal{L}(z,\theta)\rho(z)dz.
]&lt;/p&gt;

&lt;p&gt;앞서 설명한 pruning을 적용하면 score가 임계치보다 낮은 샘플에 대해서는 $1-r$ 만큼 normalized(혹은 scaling)된 확률 밀도 분포인 $(1-r)\rho(z)$, score가 임계치보다 높은 샘플에 대해서는 원래 확률 밀도 분포인 $\rho(z)$를 따르는 데이터셋을 사용하게 된다. 각각의 케이스에 대한 확률 밀도 분포를 통합하여 $(1-\mathcal{P}_t(z))\rho(z)$ 로 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;Empirical한 관점에서 &lt;strong&gt;pruning이 적용된 확률 밀도 분포에 대한 loss는 그만큼 손해를 봤기 때문&lt;/strong&gt;에 이를 &lt;strong&gt;renormalize(혹은 rescaling)&lt;/strong&gt;을 해주기 위해 각 샘플에 대한 loss에 확률 밀도 분포에 곱한 factor인 $(1-\mathcal{P}_t(z))$의 역수를 곱하게 된다. 이렇게 곱해지는 역수 term을 $\gamma_t(z)$라고 하면, pruning 이후 남은 데이터셋인 $\mathcal{S}_t$에 대해서는 다음과 같은 &lt;strong&gt;weighted loss function의 전체 평균을 최소화&lt;/strong&gt;하는 모델 파라미터 $\theta \in \Theta$를 찾고자 한다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \underset{\theta \in \Theta}{\arg \min} &amp;amp;\underset{z\in\mathcal{S}_t}{\mathbb{E}} (\gamma_t\mathcal{L}(z, \theta)) = \frac{1}{c_t}\int_z \mathcal{L}(z, \theta)\rho(z)dz \newline
    c_t =&amp;amp; \mathbb{E}_{z \sim \rho}(1-\mathcal{P}_t(z)) = \int_z \rho(z)(1-\mathcal{P}_t(z))dz,~c_t \in (0, 1)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;또한 scale factor $\frac{1}{c_t}$는 데이터셋 샘플 갯수(혹은 iteration의 비율)인 $\frac{\vert\mathcal{D}\vert}{\vert\mathcal{S}_t\vert}$에 근사하게 되고, 따라서 &lt;strong&gt;gradient가 샘플 pruning에 의해 감소된 만큼 보상받을 수 있다&lt;/strong&gt;고 가정할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\mathbb{E}(\nabla_\theta \mathcal{L}(\mathcal{S}_t)) \simeq \frac{\vert \mathcal{D}\vert}{\vert \mathcal{S}_t \vert} \mathbb{E}(\nabla_\theta\mathcal{L}(\mathcal{D}))
]&lt;/p&gt;

&lt;h3 id=&quot;gradient-bias-from-pruningannealing&quot;&gt;Gradient bias from pruning(Annealing)&lt;/h3&gt;

&lt;p&gt;만약 &lt;strong&gt;상대적으로 초반에 pruning된 샘플&lt;/strong&gt;의 경우, 이후 학습 과정에서 랜덤하게 추출될 확률이 있으나, &lt;strong&gt;후반에 pruning된 샘플&lt;/strong&gt;의 경우 이후 학습 과정에서 다시 추출될 확률이 줄어든다.&lt;/p&gt;

&lt;p&gt;Score가 낮은 샘플 중 초/중반에 pruning된 샘플의 경우 운이 좋지 않아 샘플링될 확률에 들지 못했음에도 불구하고, 학습 후반이 되면서 점점 pruning될 확률이 높아지게 된다. 이는 학습 후반에 가까워지면서(revisiting 확률이 줄어들면서) 초반에 샘플링된 gradient에 bias될 확률이 높아지는 결과를 가져오게 된다. 따라서 저자는 &lt;strong&gt;일정 epoch까지는 pruning을 통한 학습&lt;/strong&gt;을 진행하다가, &lt;strong&gt;학습 후반에는 전체 데이터로 학습하는 방법&lt;/strong&gt;을 고안하였다. 전체 epoch을 $C$라고 했을 때, $1$에 가까운 하이퍼파라미터인 $\delta$를 조건에 추가해주게 된다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{P}_t(z) = \begin{cases}
r,&amp;amp;\mathcal{H}_t(z) &amp;lt; \bar{\mathcal{H}}_t \wedge t &amp;lt; \delta \cdot C\newline
0,&amp;amp;\mathcal{H}_t(z) \ge \bar{\mathcal{H}}_t \lor t \ge \delta \cdot C
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;이를 마지막으로 해석하면 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;학습 epoch이 $\delta \cdot C$보다 작고(and) score가 임계치보다 작을 때 $r$의 pruning 확률을 적용하여 샘플링.&lt;/li&gt;
  &lt;li&gt;학습 epoch이 $\delta \cdot C$보다 크거나 (or) score가 임계치보다 크거나 같을 때 전체를 샘플링.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;주요-실험-결과&quot;&gt;주요 실험 결과&lt;/h1&gt;

&lt;p&gt;논문에서는 InfoBatch 방법이 다양한 방법에 적용 (plug-and play) 가능하다고 밝혔으며, 이를 실제로 다양한 벤치마크에 대한 실험 결과로 보여준다. 현재 &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2303.04947.pdf&quot;&gt;아카이브에 있는 가장 최근 버전&lt;/a&gt;&lt;/strong&gt;을 기준으로 다음 데이터셋에 대한 Quantitative / Qualitative 결과가 제공된다. 데이터셋과 각각을 활용한 task는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CIFAR10/100, ImageNet1K&lt;/strong&gt; &lt;strong&gt;:&lt;/strong&gt; Classification (이미지 분류)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ADE20K :&lt;/strong&gt; Semantic segmentation (이미지 세그멘테이션)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FFHQ :&lt;/strong&gt; Image generation (이미지 생성)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Instruction Dataset&lt;/strong&gt; : Instruction fine-tuning (LLM 미세 조정)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classification-benchmarks-cifar-imagenet&quot;&gt;Classification Benchmarks (CIFAR, ImageNet)&lt;/h3&gt;

&lt;p&gt;좌측 테이블은 &lt;strong&gt;CIFAR 10/100&lt;/strong&gt;에 대한 정량적 평가, 우측 테이블은 &lt;strong&gt;ImageNet1K&lt;/strong&gt;에 대한 정량적 평가에 해당된다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/18a8e1ae-aa20-4929-8956-e1c368fe9dd2&quot; width=&quot;700&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/84937572-8ce8-4162-b07d-8d4f0c5a0665&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;InfoBatch를 포함하여 기존 pruning 방식을 적용한 다른 approach에 대해 모두 비교한 모습이다. &lt;strong&gt;CIFAR 벤치마크&lt;/strong&gt;의 경우 Fair comparison을 위해 동일한 pruning ratio에 대해 실험을 진행하였고, 30%의 pruning rate를 적용했을때 전체 데이터셋을 사용하는 것과 성능차이가 없는 것을 확인할 수 있다. &lt;strong&gt;CIFAR 벤치마크&lt;/strong&gt;에서 InfoBatch 방법은 pruning ratio가 달라질 때마다 forward propagation number (학습 iteration) 수가 달라지기 때문에 이를 epoch number를 조정하여 같도록 맞추어 비교하였다고 밝혔다. &lt;strong&gt;ImageNet 벤치마크&lt;/strong&gt;에서도 마찬가지로 다양한 모델 구조에 대해 실험을 진행하였고, 조정된 prune ratio에 대해 성능이 떨어지지 않고 유지되는 모습을 확인할 수 있다. &lt;strong&gt;ImageNet 벤치마크&lt;/strong&gt;에 대한 학습 시간에 대한 효율성은 아래 그래프 및 표에서 확인해볼 수 있다. 학습 후반으로 갈수록 전체 데이터셋을 모두 활용하는 방식(baseline)과 비교하여 학습 시간이 훨씬 줄어드는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/57357d4f-1533-4cef-b63b-34c4055343ec&quot; width=&quot;300&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/9dce7285-459d-4e11-96dc-f118f3a3b298&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;semantic-segmentation-ade-20k&quot;&gt;Semantic segmentation (ADE 20k)&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/544c839d-de0f-4f2d-9bd1-54cfa13c883a&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ADE20K 벤치마크&lt;/strong&gt;에 대한 결과는 위와 같이 나타났으며, 세그멘테이션 task의 경우에는 &lt;strong&gt;mmseg 모듈을 사용&lt;/strong&gt;하게 되는데, 일반적인 task와는 다르게 &lt;strong&gt;iteration을 기준으로 학습&lt;/strong&gt;을 하므로(epoch 단위가 아니라 데이터로더가 무한정 랜덤한 &lt;em&gt;i.i.d.&lt;/em&gt; 샘플링이 가능하다고 가정하고, 최소 $40K$ 부터 최대 $160K$까지 학습한다), 연산 시간으로 비교하지 않고 목표 성능에 다다를 때까지의 iteration(index)로 비교하였다. 대략 $40\%$의 iteration 절감 효과가 있는 것으로 드러났다.&lt;/p&gt;

&lt;h3 id=&quot;image-generation-ffhq&quot;&gt;Image generation (FFHQ)&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/c9139b76-42ec-4d85-b33d-2b36dc5760ba&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FFHQ 벤치마크&lt;/strong&gt;를 사용하여 &lt;strong&gt;LDM(Latent diffusion)&lt;/strong&gt; 모델을 학습했을 때 생성한 임의의 안면 이미지는 좌측과 같이 나타났고, 전체 데이터셋으로 학습된 LDM과 pruning을 통해 대략 $\sim27\%$의 학습량을 줄였음에도 이미지 퀄리티가 떨어지지 않았으며, 이미지 생성 task에서 활용하는 주요 평가 지표인 FID score 또한 좋은 모습을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/a459ec50-9bbc-4e64-89ba-f2aee1776d8b&quot; width=&quot;200&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;llama-instruction-fine-tuning-instruction-dataset&quot;&gt;LLAMA Instruction fine-tuning (Instruction dataset)&lt;/h3&gt;

&lt;p&gt;LLM의 zero-shot 성능을 높이는 방법으로 &lt;strong&gt;instruction dataset을 활용&lt;/strong&gt;하여 fine-tuning을 진행하는 연구가 진행되었고, 이를 &lt;strong&gt;Instruction fine-tuning&lt;/strong&gt;이라 부른다. 저자는 앞서 제시한 computer vision task 이외에도 Instruction dataset을 활용한 &lt;strong&gt;LLM task에도 InfoBatch가 활용될 수 있음을 보이는 실험&lt;/strong&gt;을 진행하였다. 표에 나타난 &lt;strong&gt;DQ(Data Quantization)&lt;/strong&gt;를 적용하여 instruction dataset size를 1차적으로 줄이고, 이에 추가로 InfoBatch를 적용하여 학습하게 되면 학습 시간을 더 줄이고도 &lt;strong&gt;평균 성능이 유지되는 것&lt;/strong&gt;을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/94d6171f-ed9d-4eec-a6d4-461f529619e3&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;ablation-실험&quot;&gt;Ablation 실험&lt;/h1&gt;

&lt;p&gt;논문에서는 &lt;strong&gt;실제로 적용한 각 방법&lt;/strong&gt;들이 효과적인지 확인하기 위해 ablation을 진행하였다. Ablation은 특별한 상황이 아니면 모두 &lt;strong&gt;CIFAR100(Image classification) 벤치마크&lt;/strong&gt;를 사용하였다.&lt;/p&gt;

&lt;p&gt;Random pruning(일반적인 dynamic pruning, hard pruning이 적용됨)을 적용했을때, 앞서 언급했던 biased gradient 문제 때문에 sub-optimal solution(저하된 성능)이 나타났으며, soft sampling을 적용하더라도 rescale(score가 낮은 샘플에 대한 gradient 크기 조정) 없이는 여전히 성능이 낮은 것을 확인할 수 있다. Rescaling만 적용하더라도 충분한 성능이 나오지만($78.1\%$), 후반 학습에 전체 데이터셋을 활용하는 annealing을 적용했을때 &lt;strong&gt;전체 데이터셋을 모두 사용하여 학습했을때와 비교하여 같은 성능을 확보&lt;/strong&gt;($78.2\%$)할 수 있는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/460c9958-1e39-4d59-8fa4-8ce6debeab2f&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;추가로 pruning condition에 대한 실험도 진행하였다. 원래대로 score가 낮은 샘플을 pruning하는 방식 이외에 score가 높은 샘플($\mathcal{H}_t(z) &amp;gt; \bar{\mathcal{H}}_t$)을 pruning하거나 랜덤하게 pruning하는 방법을 생각해볼 수 있다. 의외로 전반적으로 성능은 default setting ($\mathcal{H}_t(z) &amp;lt; \bar{\mathcal{H}}_t$)과 비교해서 큰 차이는 없는 것으로 나타났다. 그러나 prune condition을 다르게 설정하게 될 경우에 prune ratio를 줄이는 것이($33\% \rightarrow 16\%$) 상대적으로 original loss distribution을 크게 왜곡시키지 않을 수 있고(좌측 figure), 이는 실제로 &lt;strong&gt;loss value 평균을 기준&lt;/strong&gt;으로 loss값이 &lt;strong&gt;작은 샘플들이 차지하는 비율&lt;/strong&gt;(density)이 높기 때문에 pruning을 많이 하더라도 remained sample로 표현되는 &lt;strong&gt;probability distribution이 원래 분포와 비슷할 것&lt;/strong&gt;이기 때문이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/7d6f5e6f-4c28-44ea-8006-f3dc15cfadf3&quot; width=&quot;350&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/38f970fa-03c6-4193-8160-f16c00cbe3db&quot; width=&quot;350&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;추가로, loss 값이 작은 샘플들은 작은 gradient 값을 가지게 되고, 이를 사용하여 실제로 rescaling을 통해 &lt;strong&gt;복구된 gradient의 분산은 원래 gradient의 분산을 upper-bound로 가진다는 사실&lt;/strong&gt;이 증명된다. Expectation과 비율을 생각하면, &lt;strong&gt;더 안정적으로 원래 분포를 예측할 수 있게 된다&lt;/strong&gt;는 의미로 작용한다.&lt;/p&gt;

&lt;p&gt;[
Var(G_{\mathcal{S}_t}) \le \frac{\vert \mathcal{D} \vert^2}{\vert \mathcal{S}_t\vert^2}\mathbb{E}_{z\sim D}(G^2_z) -\frac{\vert \mathcal{D} \vert^2}{\vert \mathcal{S}_t\vert^2}G^2 = \frac{\vert \mathcal{D} \vert^2}{\vert \mathcal{S}_t\vert^2}Var(G_D)
]&lt;/p&gt;

&lt;p&gt;따라서 실험 결과로는 비슷한 성능을 보였음에도, 저자가 주장하듯 &lt;strong&gt;low-score sample에 대한 pruning이 보다 효과적&lt;/strong&gt;임을 알 수 있다. 이러한 분석을 사용하게 되면 &lt;strong&gt;학습 안정성&lt;/strong&gt;까지 이어지는데, Loss 분포를 2차 Taylor expansion로 전개한 후의 &lt;strong&gt;Hessian&lt;/strong&gt;과 &lt;strong&gt;SGD optimizer의 관계&lt;/strong&gt;를 통해 확인할 수 있듯이 &lt;strong&gt;gradient를 rescale&lt;/strong&gt;하는 것이 &lt;strong&gt;step size의 rescale&lt;/strong&gt;로 이어지는 것을 확인할 수 있다. 이는 학습 불안정으로 이어지지만, 앞서 확인했던 바와 같이 variance가 rescale되는 경우, &lt;strong&gt;step size의 rescale 효과를 상쇄시킬 수 있기 때문&lt;/strong&gt;에 안정적인 학습이 가능하다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/914d8fb0-5502-4751-b8d9-1b4dfbf5b65f&quot; width=&quot;500&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3/assets/79881119/6eaf3382-f0bd-4732-bed7-b6cf9b83b353&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;좌측 그래프는 학습 시 사용된 &lt;strong&gt;하이퍼파라미터&lt;/strong&gt;인 $r$(pruning 비율)와 $\delta$(pruning을 진행할 최대 에폭 비율)에 대한 &lt;strong&gt;CIFAR100 벤치마크&lt;/strong&gt; 실험 결과에 해당된다. 우측 표는 &lt;strong&gt;CIFAR10 벤치마크&lt;/strong&gt;에 여러 optimizer를 사용했을때의 결과를 보여주며, optimizer에 상관없이 &lt;strong&gt;InfoBatch를 사용&lt;/strong&gt;했을때와 &lt;strong&gt;전체 데이터셋(full dataset)을 사용&lt;/strong&gt;했을때와 성능 차이가 거의 없는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;실제 오피셜 깃허브를 참고하여 실험을 reproduction 해보았는데, 결과는 &lt;a href=&quot;https://github.com/junia3/InfoBatch&quot;&gt;깃허브 링크&lt;/a&gt;에 업로드하였다.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 May 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/infobatch</link>
        <guid isPermaLink="true">http://localhost:4000/blog/infobatch</guid>
        
        <category>Data pruning</category>
        
        <category>Training time</category>
        
        <category>Efficiency</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (1) - Linear State-Space Layer (LSSL)에 대하여</title>
        <description>&lt;h1 id=&quot;연속-데이터-구조에-대한-dnn의-발전&quot;&gt;연속 데이터 구조에 대한 DNN의 발전&lt;/h1&gt;

&lt;p&gt;Sequential한 데이터를 처리하기 위해 딥러닝 모델은 수많은 변화와 발전을 이루었다. 그 중 요즘 대표적으로 &lt;strong&gt;LLM&lt;/strong&gt; 및 &lt;strong&gt;multimodal&lt;/strong&gt; 연구에서 활발하게 활용되는 것은 &lt;u&gt;Transformer 구조&lt;/u&gt;이지만, 그 이전에는 &lt;strong&gt;LSTM&lt;/strong&gt;이나 &lt;strong&gt;GRU&lt;/strong&gt;같이 Long term(거리가 먼 문맥 간의 관계성 파악) 모듈과 함께 연구된 Recurrent Neural Network (RNN), 그리고 가장 베이직한 DNN 구조인 CNN(Convolutional Neural Network)를 temporal dataset에 적절하게 변형시켜사용하는 방법이 있었다(예컨데, 비디오 데이터셋에는 temporal information 간의 정보도 사용하기 위해 시간 축을 추가한 3D convolution을 사용하였다).&lt;/p&gt;

&lt;p&gt;이외의 방법으로는 신경망 자체의 발전으로는 유명하지는 않지만 &lt;u&gt;보다 복잡한 continuous data를 처리하기 위해&lt;/u&gt; neural differential equations (NDEs)를 직접 모델링하는 방법이 주로 사용되었다.&lt;/p&gt;

&lt;p&gt;하지만 모든 네트워크는 &lt;strong&gt;나름의 장단점이 확실&lt;/strong&gt;했다. RNN (Recurrent Neural Network)은 모두가 알다시피 Long-term module의 발전이 있었음에도 불구하고 여전히 긴 문맥을 처리하는데 연산량이나 시간이 비례해서 증가한다는 문제점이 있었으며, 또한 Long-term 모듈에 의존하기에 복잡한 데이터에서의 문맥 파악을 학습시키기 어렵다는 근본적인 문제가 있었다.&lt;/p&gt;

&lt;p&gt;대체로 &lt;u&gt;gradient vanishing problem&lt;/u&gt;이나 &lt;u&gt;gradient exploding problem&lt;/u&gt;은 continual learning에서와 더불어 RNN과 같은 연속 데이터를 학습함에 있어 catastrophic forgetting의 주된 이유로 등장하기도 했다.&lt;/p&gt;

&lt;p&gt;CNN(Convolutional Neural Network)는 local한 정보에 대해 (서로 차원이 붙어있는 특징) 최적화가 빠르다는 장점이 있으며, 어느 정도 문맥이 명확한 비디오 데이터셋이라던지, 이미지와 같은 object centric/semantic centric 데이터에 대해 inductive bias를 가진다는 장점이 있었다. 하지만 연산 자체가 sequence에 대응할 수 있는 구조가 아니다보니, 길이가 길어질수록 RNN과 같은 문제가 발생하였다. 결국 convolution 연산 또한 &lt;u&gt;정해진 context 내에서의 local information만 뽑아내는 구조&lt;/u&gt;다 보니, context length에 따라 연산량이나 시간이 비례한다는 문제는 똑같이 생기게 되었다.&lt;/p&gt;

&lt;p&gt;NDE (Neural Differential Equation) 모델링은 특정 modality나 정해진 문제를 수학 모델링을 통해 이론화했지만, 그리 효율적이지 않다는 문제가 있다. 대표적으로는 diffusion modeling을 생각해볼 수 있는데, 생성 모델인 diffusion을 이런 효율의 문제를 score function의 이산화로 해결했다. Implicit model의 부담을 줄여주어서 간단한 U-Net 구조를 사용했고, consistency modeling과 같이 또다른 implicit mapping을 통해 해결할 수 있었지만, 이는 각 구간에서의 미분 방정식 solution을 numerical하게 구할 수 있었기 때문이었고 모든 형태의 미분 방정식에서 &lt;strong&gt;일괄적&lt;/strong&gt;으로 신경망이 &lt;u&gt;효율적으로 학습될 수 있는 구조를 찾는 것&lt;/u&gt;은 불가능하다.&lt;/p&gt;

&lt;p&gt;결국 가장 이상적인 모델 구조의 발전 방향은&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convolution과 같이 병렬화 연산이 가능한 구조여야 효율적일 수 있음.&lt;/li&gt;
  &lt;li&gt;Recurrence 형태의 상태 추론과정을 통한 문맥 처리가 되어야함.&lt;/li&gt;
  &lt;li&gt;Differential equation과 같이 이산화된 신호가 아닌 time-scale에 적용 가능해야함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;로 요약할 수 있다. 이러한 모델링을 찾기 위해 끊임없는 시도가 있었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;여러-모델링-방법들-소개&quot;&gt;여러 모델링 방법들 소개&lt;/h1&gt;

&lt;h3 id=&quot;ckconv&quot;&gt;CKConv&lt;/h3&gt;

&lt;p&gt;그 중 하나인 &lt;a href=&quot;https://arxiv.org/abs/2102.02611&quot;&gt;CKConv(Continuous Kernel Convolution)&lt;/a&gt;는 &lt;strong&gt;콘볼루션 커널&lt;/strong&gt;을 일종의 vector continuous function $\psi : \mathbb{R} \rightarrow \mathbb{R}^{N_{out} \times N_{in}}$ 으로 보는 방식이다. 이때 연속 함수 $\psi$는 작은 신경망 MLP로 parameterize하여 학습시키게 되는데, MLP는 value로 time-step을 &lt;strong&gt;스칼라 값&lt;/strong&gt;으로 받아 해당 position에서의 &lt;u&gt;convolution kernel을 벡터로 내보내는 형식&lt;/u&gt;이 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/b01b6b5a-c037-4918-8185-13f272403618&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;unicornn&quot;&gt;UnICORNN&lt;/h3&gt;

&lt;p&gt;RNN 계열에서 ODE 기반의 모델링 (time-scaling을 통한 long-time dependency 확보)에서는 &lt;a href=&quot;UnICORNN&quot;&gt;UnICORNN&lt;/a&gt;과 같은 연구가 진행되기도 하였다. 간단하게 방법만 소개하면 해당 RNN은 2차 ODE(일반 미방)을 시간 축으로 이산화 (discretization)할 수 있는 오일러 메소드를 사용한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/ac666a29-b8d4-446d-a1ef-3ef782ba7c0f&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 그림에서 나와있는 $y$가 얻고자 하는 함수이고 $z$는 얻고자 하는 함수의 1차 미분 함수에 해당된다. 2차 ODE를 직접 풀어서 &lt;u&gt;원하는 함수를 얻기가 힘들기 때문&lt;/u&gt;에 $y$의 1차 미분 함수인 $y^\prime$을 $z$라는 임시 변수로 선언함으로써 2차 미분 ODE를 $z, y$ 간의 1차 미방으로 바꿀 수 있다.&lt;/p&gt;

&lt;p&gt;이렇게 변경된 ODE 시스템을 &lt;strong&gt;“Hamiltonian system”&lt;/strong&gt;이라고 부른다. 이 Hamiltonian system을 풀어내는 과정에서 시간별 input에 의존하는 연속 함수가 구현이 되고,&lt;/p&gt;

&lt;p&gt;[
    H(y, z, t) = \frac{\alpha}{2} \parallel y \parallel^2 + \frac{1}{2}\parallel z \parallel^2 + \sum_{i=1}^m\frac{1}{w_i} \log (\cosh (w_iy_i + (Vu(t))_i + b_i))
]&lt;/p&gt;

&lt;p&gt;각 벡터 $y, z$의 유클리디안 norm 연산인 $\parallel \cdot \parallel$ 을 통해 위와 같이 정리된다. 이 연속 신호 미분 방정식을 오일러 메소드를 통해 이산화하면 얻고자 하는 discrete dynamical system이 추출된다.&lt;/p&gt;

&lt;h3 id=&quot;lmu&quot;&gt;LMU&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.11417&quot;&gt;Parallelizing Legendre Memory Unit Training (LMU)&lt;/a&gt; 에서는 RNN의 단점 중 하나인 병렬화 불가능 문제를 linear recurrence convolution으로 해결하는 시도를 보였다. 만약 우리가 특정 input의 이전/이후 state를 가져올 수 있는 딜레이 구조의 시스템을 구축할 수 있다면, 해당 시스템의 output으로 input의 recurrence 구조를 확보할 수 있다는 장점이 생긴다. 우리는 Linear system을 찾고자 하기 때문에 (애초에 학습하고자 하는 신경망 연산 자체가 텐서 및 행렬 기반이기 때문이라 생각하면 편하다), 다음과 같이 네 개의 matrices $&amp;lt;A, B, C, D&amp;gt;$ 로 표현되는 &lt;strong&gt;LTI system을 찾는 것&lt;/strong&gt;이 목표가 된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{m} =&amp;amp; Am + Bu \newline
    y =&amp;amp; Cm + Du
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;그리고 I/O 의 라플라스 변환 형태인 $u(s), y(s)$로 SISO system의 transfer function $G(s)$를 정의할 수 있게 된다. 하지만 해당 &lt;strong&gt;transfer function&lt;/strong&gt;이 내포하는 어려움은 infinite dimensional하며, continous delay $\theta$를 모두 커버치기 불가능하다는 문제가 생긴다.&lt;/p&gt;

&lt;p&gt;[
    G(s) = \frac{y(s)}{u(s)} = e^{-\theta s}
]&lt;/p&gt;

&lt;p&gt;이제 finite하고 causal한 state space realization 차원으로 가져오기 위해서는 transfer function $G(s)$를 $s$에 대한 polynomial로 구성을 해야한다. 보통 transfer function은 분자와 분모가 각각 특정 차수를 가지는 $s$의 다항식으로 표현되는데, proper 한 dimension을 가지는 시스템은 분모의 차수가 더 높아야한다(그래야 시스템의 convergence를 보장할 수 있기 때문이다). 아무튼 위에 있는 저 식을 approximation 해야한다는 결론에 다다르게 된다. 이를 Linear system에서 구현하기 위해서 앞서 확인했던 것처럼 matrices를 구해야하고, $i,~j \in [0,d-1]$ 에 대해서 &lt;strong&gt;다음이 성립하는 행렬 요소&lt;/strong&gt;를 사용하게 된다.&lt;/p&gt;

&lt;p&gt;디테일한 내용이나 증명 과정은 해당 페이퍼의 이전 논문인 &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf&quot;&gt;LMU&lt;/a&gt;를 보거나 아래에 있는 증명 과정을 보면 된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    A_{i,j} =&amp;amp; \frac{(2i+1)}{\theta}\begin{cases}
    -1 &amp;amp; i &amp;lt; j \newline
    (-1)^{i-j+1} &amp;amp; i \ge j
    \end{cases}\newline
    B_i =&amp;amp; \frac{(2i+1)(-1)^i}{\theta}
    \newline
    C_i =&amp;amp; (-1)^i \sum_{l=0}^i {i \choose l}{i+l \choose j}(-1)^l \newline
    D =&amp;amp; 0
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;해당 매트릭스들 중 세번째 matrix인 $C$가 &lt;u&gt;가장 주요 아이디어&lt;/u&gt;에 해당된다. $C$는 풀게 되면 르장드르 다항식으로 표현되며, $D = 0$이기 때문에 shifted input $u(t-\theta)$ 의 정확도를 현재 state $m_t$를 기준으로 판별할 수 있다. 예컨데, $\theta^\prime$만큼의 phase가 이동된 신호를 예측하고자 한다면 다음과 같이 &lt;strong&gt;shifted Legendre polynomial&lt;/strong&gt;를 통해 근사할 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    C_i(\theta^\prime) = (-1)^i \sum_{l=0}^i {i \choose l}{i+1 \choose j}&amp;amp;\left(-\frac{\theta^\prime}{\theta}\right)^l,~0 \le \theta^\prime \le \theta \newline
    u(t-\theta^\prime) \approx&amp;amp; C(\theta^\prime)^\top m_t
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;설명이 길었지만 다시 풀어서 설명하자면, 이상적인 딜레이 시스템을 LTI 시스템으로 구축하여 표현한 것이 기존의 Linear state machine 디자인이었고, 이를 다시 non-linear neural network system을 사용하여 학습한 것이 LMU 구조가 되겠다. &lt;u&gt;딜레이 시스템을 솔루션으로 삼아&lt;/u&gt; 네트워크를 학습하려고 한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;hippo&quot;&gt;HiPPO&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.07669.pdf&quot;&gt;HiPPO&lt;/a&gt;는 LMU를 &lt;strong&gt;일반화한 구조&lt;/strong&gt;에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/10fb0ea0-6866-42a6-b73b-440873745c62&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HiPPO의 방법&lt;/strong&gt;은 다음과 같다:&lt;/p&gt;

&lt;p&gt;원래 함숫값과 예측된 함숫값 사이의 차이를 measure할 수 있는 Hilbert space $\mu$상에서 각 구간의 연속 함수인 $f$를 $g$라는 subspace로 보내는 과정을 거친 뒤, 이를 적당한 vector basis의 coefficient의 배열로 표현한다. 그렇게 되면 Continous-time ODE를 LTI system의 미분 방정식으로 표현할 수 있게 되며, 이때 system의 주축이 되는 $A(t)$와 $B(t)$ 함수의 형태를 결정하여 시퀀스 메모리에 대한 중요도를 매핑한다. 이를 통해 기존 LMU를 continuous-time memorization으로 일반화시켰다. 왜냐하면 기존 LMU(르장드르 메모리 유닛)에서는 특정 슬라딩 윈도우 크기($\theta$)를 가지는 이상적인 delay system의 &lt;u&gt;LTI 미분 방정식을 그대로 이산화하여 사용&lt;/u&gt;하기 때문이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl-모델링&quot;&gt;LSSL 모델링&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/3e50bc00-c112-4ba5-acd9-b8c1db11a482&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이런 이전 모듈들의 발전은 모두 공통적으로 기존 CNN/RNN의 구조 및 단점을 &lt;u&gt;time-step 차원에서 접근했다&lt;/u&gt;는 점이다. 하지만 모든 방법들은 &lt;strong&gt;convolutional/recurrent model&lt;/strong&gt;의 문제점을 근본적으로 해결하지 못했다는 점이 한계점으로 작용했다.&lt;/p&gt;

&lt;p&gt;Linear State-Space Layer (LSSL)은 위의 그림에서 나오는 각각의 장점을 통합한 구조를 고안하는 것을 주된 목적으로 삼았다. 결국 formulation은 이전 approach와 큰 차이는 없다. LSSL은 1-dimensional function 혹은 sequence $u(t) \rightarrow y(t)$를 implicit function $x(t)$를 통해 mapping하고자 하는 방법이다.&lt;/p&gt;

&lt;p&gt;$A$는 앞서 봤던 LMU에서와 같이 system의 &lt;strong&gt;implicit function&lt;/strong&gt; $x(t)$의 &lt;strong&gt;evolution&lt;/strong&gt;을 조정하는 matrix이며, $B, C, D$는 &lt;strong&gt;projection&lt;/strong&gt;에 사용된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{x}(t) =&amp;amp; Ax(t) + Bu(t) \newline
    y(t) =&amp;amp; Cx(t) + Du(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;만약 $\Delta t$를 &lt;u&gt;discrete step-size&lt;/u&gt;로 정한다면, LSSL은 정해진 갯수의 메모리와 연산으로 각 시간 축에 따라 state를 변화시키는 &lt;strong&gt;recurrent model&lt;/strong&gt;로 해석할 수 있으며, LTI system인 위의 두 수식은 결국 &lt;strong&gt;continous convolution&lt;/strong&gt;으로 표현될 수 있다. 고로 discrete-time version의 LTI system 또한 convolution으로 병렬화가 가능하다. 학습 속도가 빨라질 수 있다는 것이다. 마지막으로 LSSL은 LTI system의 모델링 자체가 differential equation이기 때문에 continous-time model의 모든 적용 가능한 상황을 그대로 모방할 수 있다.&lt;/p&gt;

&lt;p&gt;결국 이 논문에서 밝히고자 한 내용은 위의 LSSL이 고전적인 제어 이론으로부터 익히 알려져있는 사실과 같이 모든 형태의 1-D Convolution을 표현할 수 있을 뿐만 아니라, 적절한 step size인 $\Delta t$ 그리고 적절한 state matrix $A$를 가지고 RNN 및 ODE가 가지는 특성(특히 장점에 집중)을 그대로 가져올 수 있다는 것이다. $A$는 다시 말하지면 시스템의 변화를 주도하는 학습 행렬로 사용되는데, HiPPO와 같은 이전 연구에서 드러났던 것처럼 연속 시간에 대한 memory를 고려하면서 동시에 long dependency를 고려해야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;continuous-time-memorization&quot;&gt;Continuous-time memorization&lt;/h1&gt;

&lt;p&gt;Continuous time memorization 에 대한 근사화(approximation)는 HiPPO 그리고 LSSL 논문에서 공통적으로 가지는 이론적/기술적 배경에 해당된다.&lt;/p&gt;

&lt;p&gt;필연적으로 연속 시간 모델링을 그대로 적용할 수 없기 때문에 이를 이산 시간 모델로 근사화 혹은 다운 샘플링하는 과정을 거치게 된다.&lt;/p&gt;

&lt;p&gt;디퓨전 모델링에서도 확인할 수 있었던 것처럼 결국 연속 시간 미분 방정식의 $dt$를 얼마나 조정하냐에 따라 생성 성능이 달라졌기 때문에, 결국 연속 시간 모델링을 이산화할 때는 step size/time scale인 $\Delta t$를 조절하는 것이 중요하다.&lt;/p&gt;

&lt;p&gt;해당 섹션에서는 LSSL 모델링으로부터 여러 property에 대한 insight를 얻을 수 있는 &lt;strong&gt;근거&lt;/strong&gt;라고 볼 수 있는 개념들에 대해서 정리하도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;approximations-of-differential-equations&quot;&gt;Approximations of differential equations&lt;/h3&gt;

&lt;p&gt;모든 형태의 differential equation $\dot{x}(t) = f(t, x(t))$는 integral equation $x(t) = x(t_0) + \int_{t_0}^t f(s, x(s))ds$을 동치로 가지게 된다. 해당 integral solution은 함수 $x$의 근사치를 $f(s, x(s))$에 넣고 계속 연산을 하는 방식으로 풀어낼 수 있다. 예컨데 $x_0(t) = x(t_0)$라는 함수 초기 조건을 가지고 있다면,&lt;/p&gt;

&lt;p&gt;[
    x_{i+1} (t) = x_0 (t) + \int_{t_0}^t f(s, x_{i}(t))ds
]&lt;/p&gt;

&lt;p&gt;위와 같이 근사화할 수 있다. 이를 &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem&quot;&gt;Picard iteration&lt;/a&gt;&lt;/em&gt; 이라고 부른다.&lt;/p&gt;

&lt;h3 id=&quot;discretization&quot;&gt;Discretization&lt;/h3&gt;

&lt;p&gt;그리고 이산화 과정에서 함수를 직접 적분해낼 수 없기 때문에 discrete times $t_i$에 대해, $x(t_i)$를 쪼개서 얻어내야한다. Integral equation의 form을 closed form으로 정확히 계산할 수 있다면 단순히 downsampling하는 방법으로 각 $x(t_0), x(t_1), \cdots$ 를 얻어내거나, closed form으로 알지 못하더라도 &lt;em&gt;picard iteration&lt;/em&gt;을 각 구간별 integral equation인&lt;/p&gt;

&lt;p&gt;[
    x(t_{k+1}) = x(t_k) + \int_{t_k}^{t_{k+1}} f(s, x(s)) ds
]&lt;/p&gt;

&lt;p&gt;에 적용하여 각 $t_k$ 시점의 함숫값들을 샘플링할 수 있다. 다른 방법으로는 &lt;strong&gt;generalized bilinear transform (GBT)&lt;/strong&gt;가 있는데, 이는 현재 우리가 관심있는 Linear ODE에 적용될 수 있는 방법이다. 풀고자하는 Linear ODE의 형태가 다음과 같을때,&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{x}(t) =&amp;amp; Ax(t) + Bu(t) \newline
    y(t) =&amp;amp; Cx(t) + Du(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;GBT update는 다음의 수식으로 진행된다. 수식에서의 $\Delta t$는 step size를 의미한다.&lt;/p&gt;

&lt;p&gt;[
    x(t+\Delta t) = (I-\alpha \Delta t \cdot A)^{-1}(I+(1-\alpha)\Delta t \cdot A)x(t) +\Delta t(I-\alpha \Delta t \cdot A)^{-1}B \cdot u(t)
]&lt;/p&gt;

&lt;p&gt;수식이 조금 복잡해서 한번에 잘 이해가 되질 않지만 특별한 케이스를 보면 이해하기 어렵지 않다. $\alpha = 0$을 위 수식에 대입하면,&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    x(t+\Delta t) =&amp;amp; x(t) + \Delta t \cdot (Ax(t) + Bu(t)) \newline
    =&amp;amp; x(t) + \Delta t \cdot \dot{x}(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;위와 같이 표현되며 이는 가장 대표적인 방법인 &lt;em&gt;Euler method&lt;/em&gt;임을 알 수 있다. 결국 $\alpha$는 동일하게 함수를 구하는 방식에서 어느 위치에서의 미분값을 사용하냐에 따라 달려있다. $\alpha=1$이 되면 &lt;em&gt;backward Euler method&lt;/em&gt; 가 되는데, 이는 동일하게 함수를 예측할 때 특정 위치에서의 도함수에 기반한 first order approximation이라는 점은 같지만 특정 위치가 $t$ 가 아닌 $t + \Delta t$ 라는 점에서 차이가 있다.&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\Delta t A)^{-1}x(t) + \Delta t (I - \Delta t A)^{-1} B \cdot \dot{x}(t)
]&lt;/p&gt;

&lt;p&gt;따라서 $\alpha = \frac{1}{2}$를 사용하게 되면 서로 다른 두 위치의 도함수 평균을 쓰게 되므로, 만약 곡률이 큰 복잡도가 높은 함수가 솔루션을 구성하는 상황에서는 같은 $\Delta t$를 사용하더라도 보다 안정적인 함수 예측이 가능해진다. 이를 &lt;em&gt;bilinear&lt;/em&gt; 방법이라고 부른다.&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\Delta t / 2A)^{-1}(I+\Delta t / 2A) x(t) + \Delta t (I - \Delta t / 2A)^{-1} B\cdot\dot{x}(t)
]&lt;/p&gt;

&lt;p&gt;이렇게 &lt;em&gt;bilinear&lt;/em&gt; 방법에 사용되는 matrix A와 B를 $\bar{A}, \bar{B}$ 라고 했을 때, 이를 통해 위의 시스템을 discretize하게 되면 다음과 같은 discrete-time state-space model을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_t =&amp;amp; \bar{A}x_{t-1} + \bar{B}u_t \newline
y_t =&amp;amp; Cx_t + Du_t
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;timescale-factor&quot;&gt;Timescale factor&lt;/h3&gt;

&lt;p&gt;시퀀스 길이에 따른 dependency는 길이가 길어질수록 줄어든다. 예컨데 $\Delta t$ 만큼을 시간 간격으로 잡는다면 의존도는 그에 반비례하게 된다. 대부분의 ODE 기반 RNN 구조에서는 $\Delta t$를 고정값으로 사용하였지만, classical RNN의 gating 메커니즘은 이를 학습하는 것과 같은 효과를 지닌다. 그리고 CNN 관점에서의 $\Delta t$는 convolution kernel의 크기를 조절하는 형태로 해석이 가능하다. 즉, CNN이든 RNN이든 ODE 기반으로 해석한다면 모두 시간 간격인 $\Delta t$를 어떻게하면 최적화할 수 있을까에 대한 문제로 해석이 가능하다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;continuous-time-memory&quot;&gt;Continuous-time memory&lt;/h3&gt;

&lt;p&gt;입력되는 함수 $u(t)$와 고정된 probability measure(메트릭) $\omega(t)$가 있을 때, 함수의 기본꼴이 되는 $N$개의 basis가 있다고 가정해보자. 각 time step $t$마다 이전까지의 input들인 $u(\tau)\vert_{\tau &amp;lt; t}$ 는 $N$개의 basis의 조합으로 표현이 가능하고, 이는 곧 함수를 projection하여 획득한 coefficient vector $x(t) \in \mathbb{R}^{N}$ 이다. 이때 각 time step마다의 최적의 솔루션은 거리 메트릭 $\omega(t)$에 의존하게 된다. 이렇듯 함수 $u(t)$를 coefficient $x(t)$로 표현하는 과정이 앞서 소개했던 HiPPO (High-Order Polynomial Projection Operator)가 된다.&lt;/p&gt;

&lt;p&gt;HiPPO의 경우에는 두 가지 경우(해당 논문에서는 LegT, LagT라는 이름으로 제안된 메트릭)를 제안하였는데, 모든 time step에 같은 중요도를 매핑하는 uniform measure $\omega = \mathbb{I}{[0, 1]}$ 와, 가까운 time step에 보다 높은 중요도를 매핑하는 exponential-decaying measure $\omega(t) = \exp(-t)$ 가 있다. 논외긴 하지만 HiPPO에서는 정해진 sliding window 크기를 가지는 translated Legendre (LegT) 대신 long dependency 및 forgetting 문제를 해결하고자 scaled Legendre (LegS)를 사용하였다. 둘의 공통점은 window 안에서 균일한 measure weight을 가진다는 점이지만, LegS는 시간이 흐를수록 window 크기가 커진다는 차이점이 있다. 아무튼 중요한 점은 measure 종류에 따라 matrix $A$를 closed form으로 풀어낼 수 있으며, 이를 토대로 long dependency에 대한 모델링이 가능하다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/86837dba-a674-4944-b703-726028884b54&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;각 메트릭에 따른 matrix $A$를 정하는 과정은 HiPPO 논문의 Appendix를 참고하면 되는데, 이를 조금 간단하게 정리해보고자 한다. 관련 내용을 이해하는데 필요한 사전 지식이 너무 방대하여 완벽한 증명 과정을 담기에는 무리가 있지만 그럼에도 HiPPO 전반적인 내용을 이해해야 LSSL 모델링을 해석할 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;orthogonal-polynomials&quot;&gt;Orthogonal Polynomials&lt;/h3&gt;

&lt;p&gt;Orthogonal polynomials (서로 수직 관계에 있는 다항식)은 함수를 해석하는데 사용되는 기본적인 툴이다. 모든 measure $\mu$ 상에서 해당 OP에 대응되는 unique한 함수 시퀀스가 나오게 된다. 여기서 measure metric은 적분이 이루어지는 서브 공간으로 이해하면 된다. OP의 특징은, 서로 다른 OP들을 measure 상에서 적분했을때 0이 나와야한다는 것이다. 그리고 $i$번째 Polynomial은 차수가 $i$라는 constraints도 포함된다.&lt;/p&gt;

&lt;p&gt;[
\langle P_i, P_j \rangle_\mu = \int P_i(x) P_j(x) d\mu (x) = 0~~(i \neq j),~\deg (P_i) = i
]&lt;/p&gt;

&lt;p&gt;이러한 조건에서 $f$라는 이상적인 함수에 근사하는 최적의 솔루션은 다음과 같이 계산된다.&lt;/p&gt;

&lt;p&gt;[
\sum_{i=0}^{N-1} c_i P_i(x) / \parallel P_i \parallel_\mu^2,~\text{where }c_i = \langle f,P_i \rangle_\mu = \int f(x)P_i (x) d\mu(x)
]&lt;/p&gt;

&lt;p&gt;가장 대표적으로 유명한 OP에는 Fourier series basis를 생각해볼 수 있고, Jacobi, Laguerre 혹은 Hermite Polynomial도 이에 포함된다. 여기에서 소개할 OP는 Jacobi Polynomial에 속하는 르장드르 다항식이다.&lt;/p&gt;

&lt;h3 id=&quot;legendre-polynomials&quot;&gt;Legendre Polynomials&lt;/h3&gt;

&lt;p&gt;르장드르 다항식은 흔히 구면 좌표계에서 많이 사용한다. 공학 수학을 배울 때의 악몽이 떠오르는 기분이다. 암튼 orthogonal 관계는 익히 알려진대로 구간 $[-1, 1]$ 내에서 $L^2$ 내적을 취했을 때 $\frac{2}{2n+1}$ 만큼 스케일링된 크로네커 델타를 획득할 수 있다. 그리고 유명한 성질 중 하나가 $P_n(1) = 1, P_n(-1) = (-1)^n$ 라는 경계조건을 가진다는 것.&lt;/p&gt;

&lt;p&gt;여기서 일종의 선형성을 통해 다양한 time-scale 축에 대한 Polynomial 또한 구할 수 있다. 결국 르장드르 다항식이 성립하는 measure 공간 자체도 균일 확률 분포였기 때문에 가능한 일이다.&lt;/p&gt;

&lt;p&gt;원래의 orthogonality는 $[-1, 1]$에서 성립했고, 이를 $[0, t]$ 구간에서 성립하게 하기 위해 함수 구간을 맞춰주게 되면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
(2n+1)\int_0^t P_n \left( \frac{2x}{t} - 1 \right) P_m \left( \frac{2x}{t}-1 \right) \frac{1}{t} dx = \frac{2n+1}{2}\int P_n P_m \omega_\text{leg} dx
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;적분 구간만 맞춰줬는데 다시 크로네커 델타를 획득할 수 있다. 고로 measure가 스케일링된 경우 르장드르 다항식은 원래의 다항식을 스케일링 해주면 쉽게 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;[
(2n+1)^{1/2} P_n \left(\frac{2x}{t} - 1\right)
]&lt;/p&gt;

&lt;h3 id=&quot;translated-legendre&quot;&gt;Translated Legendre&lt;/h3&gt;

&lt;p&gt;Translated Legendre는 윈도우 크기가 $\theta$이고, 현재 지점이 $t$인 경우의 Legendre measure를 의미한다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\omega(t, x) =&amp;amp; \frac{1}{\theta} \mathbb{I}_{[t-\theta, t]} \newline
p_n(t, x) =&amp;amp; (2n+1)^{1/2}P_n\left(\frac{2(x-t)}{\theta} + 1\right) \newline
g_n(t, x) =&amp;amp; \lambda_n p_n (t, x)
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;그리고 원래 여기에서 tilting 개념이 등장하면서 굳이 OP를 쓰지 않을 때 사용하는 함수가 등장한다. 이를 $\chi$라고 하는데, 만약 $p_n(t, x)$ 대신 조합된 함수 형태인 $p_n(x)\chi(x)$를 쓴다고 가정한다면 각 time step에서 이번에는 $\omega/\chi^2$에 orthogonal해지게 된다 (OP 곱하기 OP 곱하기 $\chi^2$이 되므로). 만약 normalized된 measure와 orthonormal basis를 구한다치면,&lt;/p&gt;

&lt;p&gt;[
\zeta(t) = \int \frac{\omega}{\chi^2} = \int \frac{\omega^{(t)}(x)}{(\chi^{(t)}(x))^2}dx
]&lt;/p&gt;

&lt;p&gt;해당 함수가 곧 normalization constant가 된다. 그렇기에 normalized된 measure인 $\nu^{(t)}$는 $\frac{\omega^{(t)}(x)}{\zeta(t)\cdot(\chi^{(t)}(x))^2}$를 density로 가진다. 이렇게까지 하는 이유는 결국 tilted OP를 orthonormal하게 맞춰주기 위함이다. 위의 수식을 사용하여 orthogonality를 확인하면 르장드르에서의 orthogonality가 원래의 measure $\omega$에 대해 정규화가 됨을 알 수 있다. 하지만 이건 특수한 경우에 formulation을 위해 사용하게 되지만, 르장드르에 의한 projection에는 사용되지 않는다. 따라서 그냥 일반적인 수식을 생각해주면 된다. 앞서 추가로 언급했던 르장드르 다항식의 특성을 활용하면 마찬가지로 shifted and scaled Legendre에 대해,&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
g_n(t, t) =&amp;amp; \lambda_n (2n+1)^{1/2} \newline
g_n(t,t-\theta) =&amp;amp; \lambda_n (-1)^n (2n+1)^{1/2}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;위의 경계조건을 가진다.&lt;/p&gt;

&lt;h3 id=&quot;projection-and-coefficients&quot;&gt;Projection and Coefficients&lt;/h3&gt;

&lt;p&gt;$A$ 하나 유도하는데 너무 돌아가는 듯 하지만 HiPPO를 완전히 정복하기 위해선 필수적인 수식들이다. 앞서 tilting을 고려한 measure를 유도했었는데, 이를 사용하여 coefficient를 계산하기 위해 measure에 projection한 결과는 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
c_n(t) = \zeta(t)^{-1/2} \lambda_n \int fp_n^{(t)} \frac{\omega^{(t)}}{\chi^{(t)}}
]&lt;/p&gt;

&lt;p&gt;해당 수식을 토대로 end-to-end model을 구성하고, 해당 네트워크가 online prediction에 기반에서 이전의 함숫값 $f$ 그리고 현재의 함수를 제대로 대변하게 하기 위해서는 $c(t)$를 벡터로 표현해야하고, 이는 곧 coefficient의 벡터 형태로 얻고자 하는 목적에 부합한다.&lt;/p&gt;

&lt;p&gt;Coefficient는 항상 현재의 예측에 기반하여 업데이트되어야한다. 즉 coefficient는 고정되어있지 않고 지속적으로 변하는 함수로 고려해야하며, 이에 맞는 미분 방정식을 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\frac{d}{dt} c_n(t) &amp;amp;= \zeta(t)^{-1/2} \lambda_n \int f(x) \left(\frac{\partial}{\partial t}p_n (t, x) \right) \frac{\omega}{\chi} (t, x) dx \newline
&amp;amp;+\int f(x) \left( \zeta^{-1/2}\lambda_n p_n(t, x) \right)\left(\frac{\partial}{\partial t} \frac{\omega}{\chi} (t, x)\right) dx
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;coefficient-dynamics-with-translated-legendre&quot;&gt;Coefficient dynamics with Translated Legendre&lt;/h3&gt;

&lt;p&gt;르장드르 다항식의 projection을 구할 때 tilting을 무시한다고 했다. 그러면 위의 수식을 풀어낼 때 필요한 것은 OP의 편미분과 measure의 편미분이다. OP의 편미분은 자세한 과정은 생략하고 결과만 언급하자면 $n$번째 르장드르의 미분은 $n-1$번째의 르장드르까지의 linear combination으로 표현할 수 있다. 놀라운 르장드르의 세계.&lt;/p&gt;

&lt;p&gt;그래서 정말 다행이지만 $\lambda_n p_n(t, x)$의 미분은 수많은 $g$들로 간단하게 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\frac{\partial}{\partial t} g_n (t, x) = -\lambda_n (2n+1)^{1/2} \frac{2}{\theta} \left( \lambda_{n-1}^{-1}  (2n-1)^{1/2}g_{n-1} + \lambda_{n-3}^{-1} (2n-1)^{1/2} g_{n-3} + \cdots \right)
]&lt;/p&gt;

&lt;p&gt;그리고 measure에 대한 편미분은 rectangular function에 대한 미분과 같다.&lt;/p&gt;

&lt;p&gt;[
\frac{\partial}{\partial t} \omega (t, x) = \frac{1}{\theta}\delta_t - \frac{1}{\theta} \delta_{t-\theta}
]&lt;/p&gt;

&lt;p&gt;준비물이 모두 완료되었기 때문에 이를 통해 앞서 구했던 coefficient dynamics를 표현한 미분 방정식에 대입이 가능하다.&lt;/p&gt;

&lt;p&gt;[
\frac{d}{dt}c_n(t) = -\frac{\lambda_n}{\theta} (2n+1)^{1/2} \sum_{k=0}^{N-1} M_{nk} (2k+1)^{1/2} \frac{c_k(t)}{\lambda_k} + (2n+1)^{1/2} \frac{\lambda_n}{\theta} f(t)
]&lt;/p&gt;

&lt;p&gt;이며 이 때 $M_{nk}$는 $k$가 $n$보다 작거나 같으면 무조건 $1$이고 $k$가 $n$보다 크면 $(-1)^{n-k}$의 값을 가지는 value이다. 이제 임의로 정해줄 수 있는 $\lambda_n = (2n+1)^{1/2}(-1)^n$를 적용하면&lt;/p&gt;

&lt;p&gt;[
\frac{d}{dt} c(t) = -\frac{1}{\theta} Ac(t) + \frac{1}{\theta} B f(t)
]&lt;/p&gt;

&lt;p&gt;의 수식에서&lt;/p&gt;

&lt;p&gt;[
A_{nk} = (2n+1)\begin{cases}
(-1)^{n-k}&amp;amp; \text{if }k &amp;lt; n \newline
1 &amp;amp; \text{if }k \ge n
\end{cases},~~B_n = (2n+1)(-1)^n
]&lt;/p&gt;

&lt;p&gt;앞서 소개했던 LMU가 그대로 나오는 것을 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl-해석해보기&quot;&gt;LSSL 해석해보기&lt;/h1&gt;

&lt;p&gt;다시 LSSL로 돌아와서 Fixed state space representation $A, B, C, D$가 주어진 상황을 가정해보자. 간단하게도 LSSL은 input sequence를 output sequence로 매핑하는 과정이 된다. LSSL는 이러한 매핑 과정에서 파라미터 행렬 $A, B, C, D$ 그리고 discretize에 필수적인 $\Delta t$로 정의된다. 이제 이러한 LSSL이 대체 어떻게 RNN, CNN 그리고 Neural ODE의 모든 특징을 가질 수 있는지 해석해보도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;lssl-to-rnn&quot;&gt;LSSL to RNN&lt;/h3&gt;

&lt;p&gt;LSSL에서의 recurrent state는 각 time step$t$$x_{t-1}$에 해당한다. 현재 state $x_t$ 그리고 output $y_t$는 이산화된 LSSL formulation에 의해 계산된다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_t =&amp;amp; \bar{A}x_{t-1} + \bar{B}u_t \newline
y_t =&amp;amp; Cx_t + Du_t
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;따라서 RNN 구조와 같이 동작하는 것을 알 수 있다. 심지어 RNN 구조에서의 gated recurrence 도 만족한다. 예컨데 1차원의 gated recurrence 구조 $(1-\sigma (z))x_{t-1} + \sigma(z) u_t$는 backward-Euler method로 $\dot{x}(t) = -x(t) + u(t)$를 이산화한 것과 동일하다. $z$는 임의의 expression이 모두 가능한데, sigmoid function 특성과 앞서 소개한 GBT를 생각하면 $\Delta t = \exp (z)$로 표현했을때 gated recurrence가 $A = -1, B = 1$인 backward-Euler method임을 증명할 수 있다. 그런데 여기서 의문이 생길 수 있는 점은, Linear system에서 구축한 state layer가 과연 일반적인 deep RNN이 가지는 non-linearity 및 복잡도를 표현할 수 있는가에 대한 문제이다.&lt;/p&gt;

&lt;p&gt;앞서 단순히 $\dot{x}(t) = -x(t) + u(t)$의 이산화에 대해 언급했었는데, 이를 다르게 해석해서 &lt;em&gt;Picard iteration&lt;/em&gt;  을 사용한다고 생각하면, 결국 deep RNN은 학습 과정에서 &lt;em&gt;Picard iteration&lt;/em&gt; 을 거치면서 함수를 찾아간다고 생각할 수 있다. 즉, 만약 linear recurrence가 아닌 non-linear recurrence를 사용한다면 LSSL 또한 non-linearity를 학습할 수 있게 된다. 이를 통해 RNN 구조와 LSSL는 필요충분 관계에 놓여있다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;lssl-to-cnn&quot;&gt;LSSL to CNN&lt;/h3&gt;

&lt;p&gt;간단한 상황을 가정하기 위해 initial state를 $0$이라 가정해보자. 그렇게 되면 Linear state system을 풀어낸 output을&lt;/p&gt;

&lt;p&gt;[
y_k = C(\bar{A})^k\bar{B}u_0 + C(\bar{A})^{k-1}\bar{B}u_1 + \cdots + C\bar{A} \bar{B}u_{k-1} + \bar{B}u_k + Du_k
]&lt;/p&gt;

&lt;p&gt;이처럼 정리할 수 있으며, 이는 곧 discrete-time convolution으로 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
&amp;amp;y = \mathcal{K}_L (\bar{A}, \bar{B}, C) \ast u + Du \newline
&amp;amp;\mathcal{K}_L (\bar{A}, \bar{B}, C) = (CA^iB)_{i \in [L]} \in \mathbb{R}^L
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;따라서 LSSL은 output이 convolution에 의해 연산되는 모델로 해석 가능하며, 콘볼루션 연산은 FFT로 가속화가 가능하다.&lt;/p&gt;

&lt;p&gt;일반적인 continous state-space system의 관점에서 output $y$는 input $u$에 대해 시스템의 impulse response function $h$와의 콘볼루션 연산으로 표현된다.&lt;/p&gt;

&lt;p&gt;[
y(t) = \int h(\tau)u(t-\tau) d\tau
]&lt;/p&gt;

&lt;p&gt;이와는 조금 다르게, convolutional filter가 만약 rational functional degree ($N$)를 가지는 경우, 크기가 $N$인 state-space model로 필터를 나타낼 수 있다. 기존 연구들에서 밝혔던 점을 토대로 임의의 convolutional filter $h$는 유한한 degree 값을 가지는 rational function으로 표현이 가능하다. 앞서 봤던 HiPPO matrix의 케이스를 예로 들어보도록 하자. 필요한 사전지식을 정리할 때 Translated Legendre의 경우를 보게 되면, $A$는 특정 구간($\theta$) 내에서 동일한 확률 분포를 가지는 measure에서 정의되었다. 일반적인 LSSL에서 $dt$를 고정시켜서 생각했을 때, 첫번째 식인&lt;/p&gt;

&lt;p&gt;[
\dot{x}(t) = Ax(t) + Bu(t)
]&lt;/p&gt;

&lt;p&gt;은 history element를 기억하는 과정에 해당되고 두번째 식인&lt;/p&gt;

&lt;p&gt;[
y(t) = Cx(t) + Du(t)
]&lt;/p&gt;

&lt;p&gt;은 해당 윈도우 내에서 유의미한 feature를 뽑는 작업이다. 그렇기 때문에 LSSL은 결국 width가 학습 가능한 convolutional kernel filter를 학습하는 과정과 동치라고 생각할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;deep-linear-state-system-layers&quot;&gt;Deep Linear State-System Layers&lt;/h1&gt;

&lt;p&gt;일반적인 LSSL은 간단하게 요약하면 입력 시퀀스를 출력 시퀀스로 매핑하는 시스템이었다. 예컨데 길이가 $L$인 신호가 있다면, LSSL은 $\mathbb{R}^L \rightarrow \mathbb{R}^L$을 수행하는 하나의 vec to vec 함수 구조이며 이때 함수 자체는 parameterized 되어있다. 만약 LSSL을 $\psi$라고 한다면,&lt;/p&gt;

&lt;p&gt;[
\psi(\cdot \vert A, B, C, D, \Delta t),~A \in \mathbb{R}^{N \times N},~B \in \mathbb{R}^{N \times 1},~C \in \mathbb{R}^{1 \times N},~D \in \mathbb{R}^{1 \times 1}
]&lt;/p&gt;

&lt;p&gt;이처럼 표현할 수 있다. 앞서 언급했던 것처럼 단일 LSSL은 Recurrence, Convolution의 특징을 모두 가지고 있기 때문에 RNN과 CNN의 대표적인 레이어인 recurent unit이나 convolution kernel처럼 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;또한 입력 시퀀스가 transformer의 input처럼 $H$의 hidden dimension을 가지고 있다고 하면($L \times H$), LSSL은 $H$만큼의 LSSL을 독립적으로 학습하게 되고, Transformer의 multi-head 효과 또한 그대로 적용할 수 있다.&lt;/p&gt;

&lt;p&gt;말하고자 했던 것은 LSSL를 stacking하는 과정으로 기존 DNN 방법론과 같이 다양한 함수를 모사할 수 있으며 동시에  normalization, residual connection과 같은 방법론과 함께 모델링될 수 있다는 사실이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl과-continuous-time-memorization&quot;&gt;LSSL과 Continuous-time Memorization&lt;/h1&gt;

&lt;p&gt;LSSL이 기존 DNN 모델링의 특징을 살리면서 사용될 수 있다고 해서 무작정 사용할 수는 없는 노릇이고, LSSL이 장점을 보일 수 있어야 한다.&lt;/p&gt;

&lt;h3 id=&quot;long-dependency-into-lssls&quot;&gt;Long dependency into LSSLs&lt;/h3&gt;

&lt;p&gt;Discretized Linear system ODE에서, 시스템은 이산화된 parameter $\bar{A}$가 계속 곱해지며 발전해간다.&lt;/p&gt;

&lt;p&gt;[
x_t = \bar{A}x_{t-1} + \bar{B}u_t
]&lt;/p&gt;

&lt;p&gt;그 말은 gradient descent로 학습하게 되면vanishing gradient 문제를 피할 수 없다는 것이다. 이처럼 만약 $A$를 랜덤하게 초기화한 후 학습하는 형태를 사용하면, 기대하는 성능이 나오지 않을 것이라는 말이 된다.&lt;/p&gt;

&lt;p&gt;하지만 HiPPO와 같은 framework에서는 measure $\omega$에 따라 어떤 방식으로 이전 function을 기억할 지에 대한 문제를 언급했었다 (projection/coefficient화 과정을 통해). 그러나 HiPPO의 문제점이라고 한다면 이렇게 매뉴얼하게 정한 hippo matrix를 학습하지 못하고 그대로 사용해야한다는 점이다. 왜냐하면 HiPPO에서는 르장드르를 포함한 일부 measure에 대해서만 이를 풀어낼 수 있는 structured solution matrix $A$가 존재했고, 모든 일반적인measure에도 다른 형태의 $A$가 존재할 수 있다는 사실을 밝히지 못했기 때문이다.&lt;/p&gt;

&lt;p&gt;따라서 LSSL에서는 이를 arbitrary measure $\omega$로 확장시키고, 이때 Low-recurrence width $A$에 대한 미분 방정식을 찾을 수 있다고 증명하였다.&lt;/p&gt;

&lt;h3 id=&quot;efficient-algorithms-for-lssls&quot;&gt;Efficient Algorithms for LSSLs&lt;/h3&gt;

&lt;p&gt;그러나 A와 $\Delta t$가 상당히 중요한 parameter임이 드러났음에도 불구하고, naive LSSL에서는 학습하기 어렵다는 문제가 발생한다. LSSL은 MVM(Matrix Vector Multiplication) 그리고 Krylov function을 연산할 때 (각각 convolution/recurrence에 해당) 전자의 경우에는 matrix inversion이 필요하다는 어려움과,&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\alpha \Delta t \cdot A)^{-1}(I+(1-\alpha)\Delta t \cdot A)x(t) +\Delta t(I-\alpha \Delta t \cdot A)^{-1}B \cdot u(t)
]&lt;/p&gt;

&lt;p&gt;후자는 $\bar{A}$를 feautre의 길이인 $L$만큼 곱해야 한다는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}_L (\bar{A}, \bar{B}, C) = (CB, CAB, \ldots, CA^{L-1}B)
]&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 $A$를 학습할 때의 효율성을 증대하기 위한 조건이 하나 더 발생한다. 모든 기존의 fixed LSSL의 $A$는 &lt;em&gt;3-quasiseparable&lt;/em&gt;함이 증명되었다. 만약 학습되는 $A$ 또한 &lt;em&gt;quasiseparable&lt;/em&gt; 특성을 유지할 경우, MVM과 krylov function 연산이 보다 적은 연산량으로 처리될 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;evaluations-and-demonstrations&quot;&gt;Evaluations and Demonstrations&lt;/h1&gt;

&lt;p&gt;실제로 특정 조건을 가지는 $A$를 학습할 수 있으면, 이는 이전 HiPPO system $A$보다 더 좋은 성능을 보임을 확인하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/8eecb1e1-bac6-4e12-bbde-2a5f655a4a65&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 길이가 긴 음성 신호의 classification 성능을 통해 long time dependency 또한 입증하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/451dfa18-8b16-4813-97a2-b55b5ffa5b84&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 기존 SoTA에 필적하는 성능을 보이기까지 학습 epoch가 훨씬 적어질 수 있음을 보여주었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/LayerwiseTTA/assets/79881119/1babd539-fdea-42ec-8790-7e9ff95d7526&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Mamba modeling의 가장 기초가 되는 LSSL을 살펴보았으며, LSSL의 이해에는 HiPPO의 이해가 필수적이기 때문에 해당 논문도 함께 다루었다. 앞으로 몇개의 포스팅을 통해 Mamba를 리뷰하게 될지는 모르겠지만 State Modeling에 대해서는 아무도 제대로 정리를 안해놓을 것 같아서..&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Feb 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/lssl</link>
        <guid isPermaLink="true">http://localhost:4000/blog/lssl</guid>
        
        <category>Mamba</category>
        
        <category>LSSL</category>
        
        <category>HiPPO</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>DINOv2(Learning Robust Visual Features without Supervision) 논문 리뷰</title>
        <description>&lt;h1 id=&quot;supervised-학습의-한계점&quot;&gt;Supervised 학습의 한계점&lt;/h1&gt;

&lt;p&gt;이전 게시글 중 &lt;a href=&quot;https://junia3.github.io/blog/dino&quot;&gt;DINO&lt;/a&gt;에서는  Self-supervised learning은 NLP 뿐만 아니라 CV에서도 적절한 전략을 잘 사용한다면 기존 ViT/CNN 구조에서 발견하지 못한 &lt;strong&gt;유의미한 visual feature를 획득할 수 있음&lt;/strong&gt;이 증명되었다는 점을 소개했었다. Supervised learning은 label이 존재하는 형태의 학습에서 손실 함수로 적용되는 objective value가 명확하다는 점, 그렇기 때문에 학습되는 encoder 및 decoder의 hypothesis를 정확하게 align할 수 있다는 장점이 있었으나 특정 task에 최적화된 파라미터는 label space가 조금만 달라지거나 input image의 domain이 조금만 달라지더라도 optimal point에서 크게 벗어날 수 있기 때문에 일반화된 성능을 보여주기 힘들다는 명확한 한계점이 존재했다. 특히 classification과 같이 이미지를 전반적으로 해석하는 task에서는 큰 문제가 없지만 segmentation과 같이 입력 이미지와 동일한 resolution에서 픽셀별 prediction이 진행되는 high-level task에서는 더 큰 문제를 불러오게 된다. NLP가 상대적으로 성능 수렴을 빠르게 달성하고 거대 모델의 property를 찾거나, tuning 방법과 관련된 연구가 진행된 바탕에는 바로 supervised learning에서 벗어났다는 사실이 존재한다. 즉 task 마다 직접 생성해주는 ground truth는 high level로 올라갈수록 cost가 높아진다는 superficial한 단점 말고도 궁극적으로 얻고자 하는 robust한 visual feature를 얻는 과정에 악영향을 준다는 문제가 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-learning&quot;&gt;Self-supervised Learning&lt;/h1&gt;

&lt;p&gt;DINO 첫번째 논문을 간단하게 요약하면 EMA 구조를 가져가면서 batch size에 따라 overfitting/underfitting되지 않도록 무관한 안정적인 학습을 위해 model output의 entropy를 조절하는 centering/sharpening 작업을 도입했었다. DINO에서는 ‘ ViT를 위한 SSL, 그리고 이를 통해 획득할 수 있었던 visual feature의 특징에 대해 서술했다.’라고 한다면 &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;DINOv2&lt;/a&gt;는 ‘이러한 SSL 구조를 어떻게 확장시킬지 데이터/모델링 관점에서 서술했다.’고 요약할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/510383ce-688a-497e-97b3-30e8f68a27d9&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSL이 가지는 장점을 보여주는 하나의 qualitative 예시는 위의 그림에서 확인할 수 있다. 첫번째 칼럼에서 ‘새’와 ‘비행기’는 low level에서 비슷한 semantic 특징을 가지지만 엄연히 다른 도메인에 속한다. 그럼에도 불구하고 각 이미지의 pixel 사이의 PCA를 수행하고 가장 큰 값을 가지는 3개의 component를 visualize하면 비행기의 날개 부분이 새의 날개 부분에 매칭되거나 몸통 부분은 몸통 부분에, 꼬리는 꼬리 부분에 매칭되는 것을 확인할 수 있다(색을 보면). 마찬가지로 세번째 칼럼에서 하나의 말이 존재하는 이미지, 여러 말이 존재하는 이미지와 같이 이미지 인스턴스 내에 특정 물체를 나타내는 feature가 많이 존재하는 상황에서도 이러한 correspondence가 잘 유지된다던지, sketch(drawing)과 같이 input에 대한 natural shift가 발생한 상황에서도 PCA 결과가 합리적인 것을 볼 수 있다. 학습된 모델 스스로가 같은 종류의 instance를 포함하는 여러 도메인 사진에 대해서도 correspondence를 잘 인지할 수 있고, 이는 곧 추가적인 fine-tuning 작업 없이도 학습된 visual feature를 다양한 downstream task/dataset에 활용할 수 있음을 보여준다. 이를 흔히 표현하는 방식으로는 &lt;strong&gt;“Out of Box model”&lt;/strong&gt;라고 부른다. 한국말로 &lt;strong&gt;“우물 안 개구리 형태의 모델에서 벗어남”&lt;/strong&gt;이라고 하고 싶다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dataset-filteringcurating&quot;&gt;Dataset filtering/Curating&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/89d7e6fd-360e-4522-b40c-d451393c0006&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대용량의 데이터에 대해 SSL을 수행하기에 앞서 paper에서는 &lt;strong&gt;data curation(데이터 정제) 작업의 중요성&lt;/strong&gt;을 언급한다. 특정 dataset을 semantic하게 분석했을 때 bias가 존재한다면 다양한 이미지에 대한 visual feature를 뽑을 때 bias가 포함될 수 있기 때문이다. 이러한 ‘문맥상’의 정규화에 대한 중요성은 이미 SSL이 만연하게 적용된 NLP에서는 어느 정도 당연하게 인식하고 있는 사실이며, computer vision에서도 비슷한 맥락의 효과를 얻고자 한다면 data curation이 필수적이라는 것이다. 사실 생각해보면 supervised learning에서도 이와 같은 data curation이 필요하기는 하지만 label space 상에서 카테고리 간의 비율만 얼추 맞으면 학습 수렴에 큰 문제가 없었기 때문에 치명적으로 작용하는 문제가 아니었으나, self-supervised/unsupervised learning에서는 모델은 오로지 ‘이미지’에만 의존한 학습을 하기 때문이라고 볼 수 있다. 데이터를 처리한 과정은 다음과 같다. 위의 framework figure를 참고하면서 보면 이해하기가 편하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Curated dataset은 다음과 같이 다양한 데이터셋을 사용한다. Uncurated dataset으로는 Crawling이 가능한 웹사이트에서 img 태그에 포함된 url source를 통해 가져오게 되며, 이렇게 획득한 소스 이미지들을 PCA hash deduplication(중복 제거), NSFW filtering(성적인/폭력적인 이미지 필터링) 그리고 초상권 문제가 있기 때문에 사람 얼굴을 blurring하는 등의 작업을 추가로 진행한다. 이렇게 수집한 이미지는 대략 1.2B 이미지이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/406d47ee-6fff-4290-8b9c-907ddde4f36e&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Uncurated dataset 자체적으로 진행한 중복 제거 및 이런저런 필터링을 제외하고도, 기존 curated dataset을 기준으로 curation을 진행하기 전 이미 존재하는 데이터셋과 중복되는 이미지를 없애는 작업을 시작한다. 굳이 dataset에 이미 존재하는 이미지를 다시 retrieval하는 과정을 진행할 필요는 없기 때문이다.&lt;/li&gt;
  &lt;li&gt;Self-supervised image retrieval
중복 제거가 완료된 uncurated dataset을 기준으로 이미지 retrieval은 curated dataset과 잘 align되는 샘플들을 추출하는 과정이다. 각 이미지에 대한 임베딩을 ImageNet-22k에 사전 학습된 ViT-H/16 네트워크로 추출하고, 이미지 벡터 간의 코사인 유사도를 통해 벡터 간 거리를 계산하게 된다. 만약 retrieval의 구심점이 되는 이미지가 충분하다면 query를 기준으로 $N$개의 가장 가까운 이미지들을 찾은 뒤 이를 그대로 데이터셋에 넣고(위의 표에서 sample에 해당), 충분하지 않다면 cluster로부터 샘플링하는 방법을 채택한다. Cluster 방식에서는 uncurated data source를 $100,000$개의 분리된 cluster로 구성한 뒤 retrived image가 포함된 cluster에서 $10,000$개의 이미지를 가져온다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-pre-training&quot;&gt;Self-supervised pre-training&lt;/h1&gt;

&lt;p&gt;데이터셋 정제와 더불어 학습법도 DINO에 비해 일부 추가된 점이 있는데, 각 요소들에 대해 간단히 요약하면 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;image-level-objective&quot;&gt;Image-level objective&lt;/h3&gt;

&lt;p&gt;DINO 원래 논문에서 사용하기로는 student/teacher network에 각각 local/global feature를 구분해서 넣고, 나오는 output에 대해 consistency loss를 cross entropy term으로 적용했었다. Loss에 대한 최적화는 student에만 적용하고 teacher는 EMA로 파라미터 업데이트하는 것까지 동일하게 사용하였다.&lt;/p&gt;

&lt;h3 id=&quot;patch-level-objective&quot;&gt;Patch-level objective&lt;/h3&gt;

&lt;p&gt;Student model로 들어가는 일부 patch를 랜덤하게 마스킹하고,  각 mask patch 위치의 feature 간의 cross-entropy loss를 추가하였다. Mask에 의한 augmentation 효과가 더해졌다고 보면 된다.&lt;/p&gt;

&lt;h3 id=&quot;untying-head-weights-between-both-objectives&quot;&gt;Untying head weights between both objectives&lt;/h3&gt;

&lt;p&gt;image/patch loss를 적용할 때 같은 head(classifier)를 사용하면 image-level loss는 overfitting되고 patch-level loss는 underfitting되는 문제가 발생하였고, 각 loss가 적용되는 헤드의 분리를 통해 이 문제를 해결할 수 있었다고 한다.&lt;/p&gt;

&lt;h3 id=&quot;sinkhorn-knopp-centering&quot;&gt;Sinkhorn-Knopp centering&lt;/h3&gt;

&lt;p&gt;DINO에서 teacher softmax-centering하는 방식을 SWaV로 바꾸게 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/7835bdd0-2001-4ed2-9065-11e1ad4948a9&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SWaV 논문에서는 prototype $C$가 일종의 code book 역할이 되는 $Q$로 향하는 경로로서 학습이 진행된다.  서로 다르게 augmentation된 이미지는 각자 prototype에 의해 코드북으로 매핑이 진행되고, 각자 본인의 코드북을 예측하는 것이 아니라 다르게 augmentation된 이미지에 의해 매핑된 코드북을 예측한다. 이로써 일반적인 contrative learning을 대체할 수 있다는 논리 전개가 가능하다. Collapse (모든 latent인 $z$가 동일한 code $q$로 매핑되는 케이스)를 막기 위해 code book $Q$는 각 배치 단위에서 구성되는 모든 샘플들을 각각의 코드북에 균등하게 배분하는 과정을 거친다. 즉 $B$만큼의 배치 사이즈로 $K$개의 코드북에 매핑될 때, 각 iteration 마다 코드북 하나는 &lt;strong&gt;최소한&lt;/strong&gt; $B/K$ 만큼 선택될 수 있어야한다는 조건이 필요하다.  해당 조건 내에서 최적화 문제를 풀어내는 과정을 쭉 요약하는 것이  SWaV 에 대한 내용이다. 이 논문에서 제안된 방법은 centering을 위한  빌드업이었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/e4a0b875-7a62-4c01-afc8-96c7a0323fd8&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/81314508-0064-4b66-a3b7-30b3126f31a2&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Centering은 단일 헤드의 학습 불안정성을 해소하기 위한 일종의 앙상블 장치였다. 과거의 prediction 정보를 accumulation 함에 따라 이전 input들의 정보가 이후 input의 prediction을 보다 bias되지 않게 해줄 수 있었다. 이러한 방법 대신 representation에 head를 $m$개씩 달아두고, 해당 예측 정보들에 대한 SWaV에 weight를 주어 앙상블하는 방법을 제안한 것이 바로 &lt;a href=&quot;https://arxiv.org/abs/2211.09981&quot;&gt;Weighted Ensemble Self-supervised learning&lt;/a&gt;이다. 엔트로피(예측의 확실성 지표)에 따른 weight가 가장 좋은 성능을 보였고, centering을 SWaV 방식에 여러 head로부터의 앙상블로 대체한 것이 DINO의 representation 성능을 증가시키는 것을 확인할 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;koleo-regularizer&quot;&gt;KoLeo regularizer&lt;/h3&gt;

&lt;p&gt;배치 내에서 각 샘플들이 embedding되는 point 간의 간격을 동일하게 유지하고자 하는 정규화 term이다. 예컨데 embedding point $n$개 모두에 대해 가장 가까운 다른 point와의 거리를 log 분포로 나타내면, 이 분포가 균등하면 균등할수록 엔트로피는 커질 것이고 regularizer는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt; 가 되게끔 역수를 취해 사용한다. 물론 embedding point 간의 거리 자체가 확률 분포를 표방하는 값으로 간주되므로 정규화 작업 전 normalization을 통해  scale을 조정해준다. 즉 거리를 균등하게 함으로써 필요한 정보량을 최소로 하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;[
    \mathcal{L}_{\text{koleo}}= -\frac{1}{n}\sum_{i=1}^n \log (d_{n, i}),~~d_{n, i} = \min_{j \neq i} \parallel x_i -x_j \parallel
]&lt;/p&gt;

&lt;h3 id=&quot;adapting-the-resolution&quot;&gt;Adapting the resolution&lt;/h3&gt;

&lt;p&gt;보다 높은 해상도의 이미지를 사용했을 때 segmentation/detection과 같은 pixel level task에서의 성능이 올라간다. 이는 small object의 경우 low resolution에서 백본에 연산을 돌리면 작은 물체의 feature가 일종의 노이즈처럼 사라지는 현상이 발생되기 때문이다. 그렇다고 해서 무작정 고차원의 이미지를 가지고 모델을 학습시키는 건 메모리나 시간이 투머치로 소모적이기 때문에 권장되지는 않는다.&lt;/p&gt;

&lt;p&gt;따라서 DINOv2에서는 pretraining 마지막 일부만 이미지의 해상도를 $518 \times 518$로 증가시켜서 학습시키는 전략을 사용했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;efficient-training-details&quot;&gt;Efficient Training Details&lt;/h1&gt;

&lt;p&gt;DINO-v2는 일종의 테크니컬 리포트다. 사실 앞부분만 보아도 데이터 정제 과정이나 학습에 사용한 프레임워크를 방법론으로 제시했다기보단 기존의 연구로부터 이어지는 여러 insight 및 approach를 사용해서 좋은 모델을 만들어보겠다는 노력이 보이기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;utilizing-faster-transformer--flash-attention&quot;&gt;Utilizing faster transformer : Flash attention&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/208995fa-f961-4600-adfd-bcaf7f41c89a&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash attention&lt;/a&gt;에서 제안한 방법을 사용하면 하드웨어에 최적화해서 적용되기 때문에 더 빠르고 효율적인 연산이 가능하다. 이 논문 저자의 경우 논문에 나온 FlashAttention을 직접 구현하여 사용하였다. 사실 아직 본인은 FlashAttention 논문 자체는 이해하지 못하고 있다. 하드웨어 요소에 대한 이해가 부족한데, 어떻게 하면 이쪽으로 지식 및 기술 스택을 쌓을 수 있으려나…??&lt;/p&gt;

&lt;h3 id=&quot;nested-tensors-in-self-attention&quot;&gt;Nested tensors in self-attention&lt;/h3&gt;

&lt;p&gt;이전의 implementation에서는 서로 다른 patch token 수를 가지게 되는 global crop/local crop이 서로 따로 forward passing 및 backward passing 과정을 거쳤었다. 그러나 새롭게 구현된 버전에서는 이를 동시에 수행함으로써 그만큼의 연산량을 줄일 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;efficient-stochastic-depth&quot;&gt;Efficient stochastic depth&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.09382&quot;&gt;Stochastic depth&lt;/a&gt;는 네크워크 깊이에 따른 layer dropout을 수행하여 레이어 간의 의존성 문제를 해결하고 학습 시 각 layer의 feature map을 빠르게 최적화하는 것이 주된 contribution이었다. DINO-v2에서는 이를 다르게 수행하는데, 레이어를 skip한다는 개념이 아니라 batch 단위로 랜덤하게 들어오는 샘플들을 각 레이어에서 drop out rate $d$에 따라 $(1-d)$만큼의 batch만 block에 통과시키는 전략을 취한다. 어차피 unsupervised setting이기 때문에 batch 순서에 따른 label space의 영향을 무시할 수 있으며, 학습마다 네트워크의 모든 block에 대해 stochastic하게 drop-out을 해줄 경우 오히려 학습 시간이 늘어날 수 있기 때문에 이를 최소화한 전략으로 보인다.&lt;/p&gt;

&lt;h3 id=&quot;fully-shared-data-parallel-fsdp&quot;&gt;Fully-shared data parallel (FSDP)&lt;/h3&gt;

&lt;p&gt;AdamW로 EMA 구조를 가지는 모델을 최적화할 때, model로 하여금 4개의 replica가 필요하다. Student, teacher와 이에 추가로 Adam/AdamW 최적화에 사용되는 first momentum 그리고 second momentum이 필요하기 때문이다. 보다 큰 모델을 사용할 때 memory footprint가 급격히 증가할 수 밖에 없는데, 이를 해소하기 위해 data parallel을 사용하였다. 그리고 이렇게 replica를 gpu에 분리를 할 때 GPU memory가 분리된다는 점에서 또다른 장점이 생기는데, 이는 weight 저장 자체는 float32로 하고 최적화 시 gpu 간 통신에서는 float16으로 부동 소숫점 절반을 날려버려도 학습 성능의 큰 저하 없이 메모리를 줄일 수 있다는 것이다. 원래대로라면 GPU 갯수가 증가하더라도 메모리 총합은 같은게 DDP의 특징이었는데, FSDP를 사용하면 communication 단의 메모리를 절반으로 줄여버리니까, 결론적으로는 보다 많은 GPU를 분리해서 사용할수록 학습 전체 메모리는 줄어드는 장점이 생긴다.&lt;/p&gt;

&lt;h3 id=&quot;model-distillation&quot;&gt;Model distillation&lt;/h3&gt;

&lt;p&gt;큰 모델이 가지고 있는representation을 효과적으로 작은 모델에 넘겨줄 때, 작은 모델을 scratch부터 학습시키는 것보다는 큰 모델의 prediction에 align하는 distillation 학습법이 효과적인 것은 어느 정도 알려진 사실이다. 따라서 DINO-v2에서도 결론적으로 작은 모델에 넘겨주는 방식을 distillation으로 했는데, 이때 기존 학습 framework인 EMA는 그대로 가되, 약간의 차이가 발생한다. 우선 self-distillation 구조가 아니므로 teacher는 학습된 large model을 frozen한 채로, 작은 모델을 student로 잡아 spare EMA를 수행한다. 또한 앞서 설명했던 stochastic depth, masking 같은 방법론은 제외한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/fbe455c6-63d4-4474-b444-f358faa1f33e&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;DINO-v2는 베이스라인 논문인  &lt;a href=&quot;https://arxiv.org/pdf/2111.07832.pdf&quot;&gt;iBOT&lt;/a&gt;(image BERT 논문)을 기준으로 짜잘한 방법들이 추가로 사용되었는데, 그래서 제안한 방법들이 얼마나 효과적인지를 task를 고정한 채로 ablation을 진행한 표이다. 이외에 여러 실험 결과들에 대한 내용이 페이퍼에 있는데, 대부분 성능이 올라갔다는 점을 드러내고 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;

&lt;p&gt;DINO-v2 논문을 보면 주인공이 모든 무술을 연마하여 마스터해버리는 무협 영화(?)장르가 생각난다. 메타의 이 연구는 단순히 scaling-up하는데 목적을 두지 않고 보다 효율적인 학습법과 그러면서도 어떻게 좋은 representation을 얻을 수 있는지 다양한 방법들을 적용해보고 실험해본 결과물로 보인다. 어느새 학계에서 성능 좋은 베이스라인 모델을 만드는 것은 불가능에 가까운 게 아니라 불가능이 되어버렸다. 이제는 논문을 쓰는 과정에서 집중해야 할 곳들은 이런 SSL 자체보다는 학습된 representation을 어떻게 활용하고, mapping하고 혹은 tuning할 것인가에 있어 보인다.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Dec 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/dino2</link>
        <guid isPermaLink="true">http://localhost:4000/blog/dino2</guid>
        
        <category>SSL</category>
        
        <category>ViT</category>
        
        <category>DINOv2</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Github 프로필 리드미 작성하는 법 및 꾸미기 (feat. github readme snake)</title>
        <description>&lt;h1 id=&quot;깃허브를-꾸며야-하는-이유&quot;&gt;깃허브를 꾸며야 하는 이유&lt;/h1&gt;
&lt;p&gt;깃허브를 꾸며야만 하는 이유에 대해 설명하기에 앞서, 조금은 개인적인 이야기를 덧붙이고자 한다. 엔지니어라면 가장 중요한 역량은 이론에 기반한 공학적 지식과 더불어 이를 실제 산업 환경에서 적용할 수 있는 능력 그리고 새로운 기술 전반에 대한 거부감이 없어야한다는 점 등이 있을 것이다. 본인도 이렇게 흔히 생각할 수 있는 현실적 역량들이 최우선이라 생각했고, 그렇기 때문에 스스로를 PR하는 방식에 있어서 &lt;strong&gt;프레젠테이션&lt;/strong&gt;이나 &lt;strong&gt;멋드러진 포트폴리오&lt;/strong&gt;는 없다고 생각해왔다.&lt;br /&gt;
이러한 생각은 여러 발표 경험, 그리고 새로운 사람들을 만나면서 자극을 받으면서 달라졌다. 애써 좋게 보이려는 노력 하나가 작게는 내 주변 사람들의 시선부터 시작해서 크게는 내가 잘 모르는 사람들에게까지, 그리고 더 중요하게는 내 자신의 발전을 위해서도 중요하다는 사실이었다.&lt;br /&gt;
포트폴리오는 &lt;u&gt;나를 증명하는 수단&lt;/u&gt;이며 나를 모르는 사람들에게는 내 &lt;u&gt;얼굴로 비춰진다.&lt;/u&gt; 예컨데 내가 팔자에도 없는 프론트 지식을 익혀가면서까지 이 블로그를 열심히 꾸민 이유도, 이렇게까지 글을 열심히 써내는 이유도 마찬가지다. 나는 남들과 같은 그런저런 비슷한 깃허브 블로그는 만들고 싶지 않았고, 내 포트폴리오를 날림식의 공부처럼 남겨놓고 싶지는 않았다. 개발자, 혹은 컴퓨터를 전공으로 하는 사람들에게는 깃허브는 일종의 &lt;u&gt;포트폴리오 및 블로그, Social Network Service&lt;/u&gt;이며 commit 하나를 얼마나 신중하게 작성하는가에 따라 미래의 나 혹은 나를 제외한 누군가가 과거의 나에게 도움을 받거나 과거의 나에게 증명받는 과정이 명확해진다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/354580ce-f735-4cf0-aaf9-ff8204f96afb&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;u&gt;깃허브 프로&lt;/u&gt;필은 여느 SNS 계정과 비슷하게 사진, 사용자 이름 그리고 홈페이지 등으로 구성되지만 보다 중요하게 여겨지는 요소는 실제로 그동안 얼만큼 여러 project에 contribution을 했었고, 이러한 contribution을 github platform 상에 잘 정리했는가이다. 하지만 솔직히 말하자면 잔디를 심는 과정은 그냥 적당한 레포지토리에 작은 코드라도 이것저것 매일같이 올리거나 엄청난 프로젝트를 하지 않더라도 할 수 있다. 즉 어느 정도 &lt;u&gt;'깃허브'&lt;/u&gt;를 타인 평가의 지표로 잘 사용하던 사람은 단순히 깃허브라는 플랫폼에 코드만 많이 올린다고 장땡은 아니라는 사실을 이미 알고 있을 것이다. 개인적으로 깃허브를 여전히 제대로 활용하고 있지 못하다고 생각하고, 그렇기 때문에 깃허브를 꾸며야하는 이유에 대해 작성하면서도 인지부조화가 일어나지만 적어도 앞으로 점차 중요성을 인지하고 꾸준히 발전해나가고자 마음을 다잡기 위해 이 글을 작성해보려한다. 그리고 내가 깃허브 프로필을 꾸민 방식, 테마에 대해 간략하게 정리해보고자 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;프로필-만들기&quot;&gt;프로필 만들기&lt;/h1&gt;
&lt;p&gt;프로필을 만드는 법은 간단하게 자신의 깃허브 ID와 동일한 이름의 레포지토리를 만들고, 여기에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readme&lt;/code&gt; 파일을 만들면 된다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/34668121-b9d3-471a-98c7-8a6bd89cdf3b&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;자신의 ID를 모르는 경우는 거의 없겠지만 혹시라도 기억이 잘 안나면 본인 계정 프로필 들어간 다음에 상단의 ‘https://github.com/’ 뒤에 붙어있는 걸로 확인하거나 직접 프로필 상의 ID를 확인하면 된다. 내 ID는 &lt;a href=&quot;https://github.com/junia3&quot;&gt;‘junia3’&lt;/a&gt;이므로 앞으로 글을 작성할 때 이 ID를 기준으로 작성할 예정이다. 아무튼 내 github ID는 junia3이므로 프로필용 repository를 만들 때 이 이름을 그대로 사용하면 된다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/467f8db9-4d66-42ba-aa0f-9c150cb2eeb7&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;본인은 이미 만들어져있는데, 만약 없다면 오른쪽 상단의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new&lt;/code&gt;를 눌러서 새로 만들어주면 된다. 들어가서 repository 이름을 그렇게 지정하면&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/6650600a-0e76-452a-b348-35e31643b17f&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;아직 만들기도 전인데 열심히 캐릭터가 설레발치면서 좋아라한다. 암튼 이렇게 나오면 제대로 입력했다는 뜻임. 당연히 리드미 파일을 포함할거니까&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/2d4c601e-5b97-47b0-995e-7efa038ee71e&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대강 이렇게 해두고 &lt;u&gt;create repository&lt;/u&gt;하면 된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;프로필-꾸미기&quot;&gt;프로필 꾸미기&lt;/h1&gt;

&lt;p&gt;프로필을 구성할 수 있는 요소는 정말 다양한데, 그냥 내가 적용한 요소들에 대해서 간단하게 소개하겠다. 현재 본인 깃허브 프로필은 다음과 같이 구성되어있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/8512b62e-a516-4694-8837-091306f33ed7&quot; width=&quot;500&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/937d65b7-05f0-4e59-8ac4-010aef306368&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;본인은 전체적인 톤앤매너를 맞춰주는 걸 선호하고 좋아하기 때문에 깃허브 다크모드를 기준으로 색상을 맞춰보았다. 각 요소들을 디자인함에 앞서 사용법과 관련하여 디자인 세부 프레임워크를 나열하면 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;프로필-상하단에-headerfooter-배너-추가하기&quot;&gt;프로필 상/하단에 header/footer 배너 추가하기&lt;/h3&gt;
&lt;p&gt;프로필을 보면 상단과 하단에 그라데이션으로 색상 톤을 맞춰준 배너가 존재한다. 이는 흔히 많이들 사용하는 &lt;a href=&quot;https://github.com/kyechan99/capsule-render&quot;&gt;capsule-render&lt;/a&gt;를 사용하였고, 들어가면 사용법에 대해서는 디테일하게 나와있다. 예컨데 내가 사용한 배너는 상단은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;waving&lt;/code&gt;, 하단은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rect&lt;/code&gt;이다. 배너에 직접 텍스트를 넣을 수도 있다.&lt;br /&gt;
본인이 사용한 상/하단 코드는 다음과 같다. 위에서부터 차례대로 상단과 하단 배너에 대한 코드로, 본인은 리드미 전체를 중앙정렬된 div 태그로 감싸주었고 배너는 해당 태그 내부의 양 끝단에 위치한다.&lt;/p&gt;
&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://capsule-render.vercel.app/api?type=waving&amp;amp;color=0:E34C26,10:DA5B0B,30:C6538C,75:3572A5,100:A371F7&amp;amp;height=100&amp;amp;section=header&amp;amp;text=&amp;amp;fontSize=0&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;100%&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://capsule-render.vercel.app/api?type=rect&amp;amp;color=0:E34C26,10:DA5B0B,30:C6538C,75:3572A5,100:A371F7&amp;amp;height=40&amp;amp;section=footer&amp;amp;text=&amp;amp;fontSize=0&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;100%&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;컬리의 경우 gradient를 지정해줄 수도, 자동으로 하거나 단색 지정 등 여러 옵션이 있는데 본인은 조금 특별하게 깃허브 페이지 스타일 자체가 가진 색상과 어우러지길 원했기 때문에 개발자 도구(f12)를 사용하여 프론트를 구성하는 색상을 알아낸 뒤 이를 사용했다. 그라데이션 위치는 $0 \sim 100$ 까지를 배너의 가장 좌측부터 우측이라고 생각하면 되고, footer는 그 반대로 생각하면 된다.&lt;br /&gt;
만약 0, 25, 50, 75, 100에 직접 색상을 지정한 gradient를 구현하려면 위의 코드에서,&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;?color=0:color1,25:color2,50:color3,75:color4,100:color5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 부분을 바꿔주면 된다. color1 ~ color5는 직접 hexcode로 뽑아서(&lt;a href=&quot;https://www.color-hex.com/&quot;&gt;참고 링크&lt;/a&gt;) 사용하는걸 추천한다.&lt;/p&gt;

&lt;h3 id=&quot;cvresume-추가하기&quot;&gt;CV(Resume) 추가하기&lt;/h3&gt;

&lt;p&gt;이 부분은 진짜 별거없는게 그냥 markdown 문법을 써서 간단하게 언급할 내용만 작성해놓았다. 직접 코드를 확인하는게 편한데, (&lt;a href=&quot;https://github.com/junia3/junia3&quot;&gt;참고링크&lt;/a&gt;). Markdown을 다 작성한 다음에 숨기고자 하는 부분을 detail 코드로 감추면 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/9b3a1db2-2a45-4cf4-9526-c16fb97cbdfa&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;details&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;summary&amp;gt;&lt;/span&gt;About Me (접혔을 때 화살표 옆에 뜨는 텍스트)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/summary&amp;gt;&lt;/span&gt;
어쩌구저쩌구 (감추고자 하는 내용)
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/summary&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cv에-들어가는-instagram-gmail-github-blog-등등-여러-배너-추가하는-방법&quot;&gt;CV에 들어가는 instagram, gmail, github blog 등등 여러 배너 추가하는 방법&lt;/h3&gt;
&lt;p&gt;또한 본인의 프로필에서 CV를 펼쳐보면 연락 수단이나 additional skill란에 배너가 추가되어있는데, 이를 적용하는 법은 다음과 같다. 우선 &lt;a href=&quot;https://shields.io/&quot;&gt;shields.io 홈페이지&lt;/a&gt;에 들어간다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Get Started&lt;/code&gt;를 누르면 다음과 같이 static badge를 커스텀해서 해당 뱃지의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;html&lt;/code&gt; 임베딩 코드를 빼내는 사이트가 나온다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/aafc87c3-6915-4bad-8d0f-b8116ae8b300&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 코드를 가져왔을때, 배너에 로고가 들어가게끔 해주고 싶다면(아래 그림과 같이 instagram, gmail 등등 뱃지에 맞는 로고) 우선 &lt;a href=&quot;https://simpleicons.org/&quot;&gt;심플 로고&lt;/a&gt; 사이트에 들어간 뒤에,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/da36a212-ddeb-443b-88a1-b07ed6f586fc&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;찾는 로고 이름을 검색어에서 찾으면 엥간한 애들은 다 나온다(프로그래밍 언어도 다 나옴)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/248ecb72-9f57-45f2-98a2-c670079636c4&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;여기서 로고에 나와있는 hexcode 색상(#E4405F)이랑 그 바로 위에 있는 로고 이름(Instagram)을 기억한 채로 다시 배너 코드로 돌아와서,&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://img.shields.io/badge/Instagram-hexcode색상?style=plastic&amp;amp;logo=로고이름&amp;amp;logoColor=로고색상&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이런 식으로 작성하게되면 배너는 hexcode 색상, 로고이름에 해당되는 로고가 로고색상에 맞게 들어간다. 여기에 추가로 실제 인스타그램 링크와 연결한다던지, 그리고 여러 배너를 옆으로 붙이는 방식은 다음과 같이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;span&lt;/code&gt; 태그와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; 태그를 함께 적절하게 활용해주면 된다. 실제로 본인이 인스타그램 배지에 사용한 코드는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://www.instagram.com/6unoyunr/&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://img.shields.io/badge/Instagram-ff69b4?style=plastic&amp;amp;logo=Instagram&amp;amp;logoColor=white&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이를 전반적으로 쭉 사용하면 다음과 같이 꾸밀 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/9119ed6c-f1ca-4d7c-90be-ba2ca9279378&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;깃허브-status-추가하기&quot;&gt;깃허브 status 추가하기&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/869b0c81-5b9d-4c75-8586-cb30ba0ee5a3&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;깃허브 프로필에 내가 자주 사용하는 언어, 깃허브 &lt;u&gt;총 commit/star 개수&lt;/u&gt;나 &lt;u&gt;날짜별 contribution graph&lt;/u&gt;를 보여주는 곳이다. 해당 UI는 모두 &lt;a href=&quot;https://github.com/anuraghazra/github-readme-stats&quot;&gt;github-readme-stats&lt;/a&gt;에서 구할 수 있는데, 본인은 해당 UI들에 대한 코드 중 일부 스타일만 수정하여 그대로 사용하였다. 사실 이 부분은 그렇게 별다른 설명이 필요하지 않을 정도로 복붙이라서 코드만 올리면 다음과 같다. 내 코드에서 username이라고 되어있는 부분이나 bg_color/icon_color/title_color을 수정해주면 되고, 개인적으로 특정 레포에서 투머치로 사용된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C#&lt;/code&gt; 언어가 most used language로 뜨는게 싫어서 해당 repository를 exclude해주었다. 그리고 높이랑 너비 맞춰주는게 예뻐서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;width&lt;/code&gt; 부분은 노가다 뛰면서 맞췄다.&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github.com/anuraghazra/github-readme-stats&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github-readme-stats.vercel.app/api/top-langs/?username=junia3&amp;amp;layout=donut&amp;amp;show_icons=true&amp;amp;theme=material-palenight&amp;amp;hide_border=true&amp;amp;bg_color=20232a&amp;amp;icon_color=58A6FF&amp;amp;text_color=fff&amp;amp;title_color=58A6FF&amp;amp;count_private=true&amp;amp;exclude_repo=Face-Transfer-Application&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;38%&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;    
&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github.com/anuraghazra/github-readme-stats&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github-readme-stats.vercel.app/api?username=junia3&amp;amp;show_icons=true&amp;amp;theme=material-palenight&amp;amp;hide_border=true&amp;amp;bg_color=20232a&amp;amp;icon_color=58A6FF&amp;amp;text_color=fff&amp;amp;title_color=58A6FF&amp;amp;count_private=true&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;56%&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github.com/ashutosh00710/github-readme-activity-graph&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github-readme-activity-graph.vercel.app/graph?username=junia3&amp;amp;theme=react-dark&amp;amp;bg_color=20232a&amp;amp;hide_border=true&amp;amp;line=58A6FF&amp;amp;color=58A6FF&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;94%/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;github-잔디-먹는-뱀-만들기&quot;&gt;Github 잔디 먹는 뱀 만들기&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/25faad36-1317-4d4e-89a0-bfdcf87cfae3&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;사실 이 부분이 생각보다(?) 제일 애를 먹었는데, 애초에 생긴것부터 리드미에 이게 왜있지 싶은 느낌이 크지 않은가… 우선 원래 사용법 링크는 &lt;a href=&quot;https://github.com/Platane/snk&quot;&gt;여기&lt;/a&gt; 아니면 &lt;a href=&quot;https://dev.to/mishmanners/how-to-enable-github-actions-on-your-profile-readme-for-a-contribution-graph-4l66&quot;&gt;여기&lt;/a&gt;인데, 둘다 무시하고 걍 혼자했다. 이유는 모르겠는데 계속 오류나고 안되는 것 같아서 걍 맨땅에 헤딩하면서 익혀보았다. 원래 사용법 올라와있는 곳에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gif&lt;/code&gt;를 임베딩하는 형태로 자꾸 제시하는데, 실제로 yml 생성해보고 코드 붙여넣으면 그렇지가 않았다. 우선 깃허브 프로필 repository에 들어간다. 그런 뒤 상단에 보이는 메뉴에서 “Actions”를 누른다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/54f3cd64-7d6c-4f9a-aed0-ffac82eb50bf&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그런 뒤 &lt;u&gt;&quot;set up a workflow yourself&quot;&lt;/u&gt;를 누른다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/8642420e-e335-4383-a067-8fdcf2b7c082&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;누르게 되면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.github/workflows/main.yml&lt;/code&gt;을 생성하는 화면이 나온다. 여기에 다음과 같은 코드를 작성한 뒤에..&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;generate animation&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;cron&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*/24&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;*&quot;&lt;/span&gt; 
  &lt;span class=&quot;na&quot;&gt;workflow_dispatch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;branches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;master&lt;/span&gt;
    
&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;generate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;permissions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
      &lt;span class=&quot;na&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;write&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;timeout-minutes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
    
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# generates a snake game from a github user (&amp;lt;github_user_name&amp;gt;) contributions graph, output a svg animation at &amp;lt;svg_out_path&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;generate github-contribution-grid-snake.svg&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Platane/snk/svg-only@v3&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;github_user_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;여기다가 github ID 적기 !!!&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;dist/github-snake.svg&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;dist/github-snake-dark.svg?palette=github-dark&lt;/span&gt;

      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;push github-contribution-grid-snake.svg to the output branch&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;crazy-max/ghaction-github-pages@v3.1.0&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;target_branch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;output&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;build_dir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dist&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;여기다가는 토큰 적기&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;코드 중간에 잘 보면 &lt;u&gt;&quot;여기다가 github ID 적기 !!!&quot;&lt;/u&gt;라고 써있다. 빼먹지 말고 자기 깃허브 아이디 적기..
그리고 &lt;u&gt;토큰을 적는 곳&lt;/u&gt;이 있는데 직접 토큰을 구해서 적어도 되고 ${{ secrets.GITHUB_TOKEN }}를 적으면 된다. 완료되었다면 commit을 한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/a21568e7-9e46-4a59-a2b5-6d259d6b67bd&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;커밋하고 나면 main branch로 넘어간 뒤, 다시 Actions 탭으로 들어간다. 그렇게 되면 방금 생성한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;generate animation&lt;/code&gt; 탭이 생겨있는 것을 볼 수 있다. 이걸 눌러보자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/6a8759bb-a895-45f9-b415-b07bcb9f180e&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;본인은 이미 workflow를 돌리고 있어서 뜨는 중인데 아마 commite하자마자 들어가면 아무것도 안떠있을 것이다. 그럼 다음과 같이 우측에 있는 Run workflow를 눌러본다. 누른 직후에는 바로 workflow가 안뜨는데, 조금만 기다리면 노란색 build 중인 워크플로우가 뜨게 되고, 이것도 좀 더 인내심을 가지고 쭉 기다리면 빌드가 완료되었다는 표시(파란색 체크)가 나오게 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/530c34bc-050a-4a9c-810f-5e0b6b1600e7&quot; width=&quot;300&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/9c459e99-6ec3-4752-892c-e3a9f9584d8b&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;github_user_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;여기다가 github ID 적기 !!!&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;dist/github-snake.svg&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;dist/github-snake-dark.svg?palette=github-dark&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 아까 작성했던 코드에서 outputs를 보면 두 개가 있는데, 이 중 위에 있는 녀석(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dist/github-snake.svg&lt;/code&gt;)은 light theme에 적용될 snake이고 밑에 있는 녀석(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dist/github-snake-dark.svg?palette=github-dark&lt;/code&gt;)은 dark theme에 적용될 snake이다. 본인은 참고로 다크 테마여서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?palette=github-dark&lt;/code&gt;를 적용했다. 이 친구를 깃허브 리드미에 넣고 싶다면 readme.md에서 원하는 위치에&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;img&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github.com/깃허브ID/깃허브ID/blob/output/github-snake-dark.svg&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;100%&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;를 넣으면 되고 마찬가지로 깃허브ID를 넣어서 불러올 svg 파일 위치를 소스 url로 명시해주면 된다. 근데 이렇게만 하면 그냥 일반적인 테마의 contribution map이랑 보라색 뱀이 나오는데 본인은 테마 색을 지정해주면서 새로 만들었다. 본인처럼 보라색 contribution map과 파란색 뱀이 쓰고싶은 분들은 앞서 작성했던 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.yml&lt;/code&gt; 코드 중 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dist/github-snake-dark.svg?palette=github-dark&lt;/code&gt; 요부분을&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;s&quot;&gt;dist/github-snake-dark.svg?color_snake=#58A6FF&amp;amp;color_dots=#EEEEEE,#E1BEE7,#BA68C8,#8E24AA,#4A148C?palette=github-dark&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이걸로 대체해주면 된다. color_snake나 color_dots를 매뉴얼하게 바꿔줄수도 있음. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;color_dots&lt;/code&gt;에는 총 5가지의 색상이 들어가야하고 좌측이 contribution 가장 낮은 정도부터 해서 우측이 가장 contribution 높은 색상이라고 보면 된다. 아래처럼 깃허브 잔디 순서처럼 생각하면 됨.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/4e5d1a2f-1b57-4455-b7c7-5ce9f2d55318&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;물론 메인 브랜치에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.yml&lt;/code&gt; 코드를 바꿨다면 마찬가지로 master 브랜치에서의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.yml&lt;/code&gt; 코드에도 동일하게 적용해야 제대로된 output이 나오고, 가장 중요한 건 Action에 들어가서 앞서 했던 내용을 그대로 반복하면 된다. 그냥 코드만 바꾸면 아마 안바뀌는 걸로 알고있다. 귀찮으면 그냥 뱀 쓰는것도 추천..&lt;/p&gt;

&lt;h3 id=&quot;마지막-부분에-hit-counter-넣기&quot;&gt;마지막 부분에 hit counter 넣기&lt;/h3&gt;
&lt;p&gt;이 부분은 hit count를 수행할 각자의 주소를 &lt;a href=&quot;https://hits.seeyoufarm.com/&quot;&gt;해당 사이트&lt;/a&gt;에 넣고 색상이나 로고 등을 지정해주면 된다.&lt;/p&gt;
</description>
        <pubDate>Mon, 13 Nov 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/githubreadme</link>
        <guid isPermaLink="true">http://localhost:4000/blog/githubreadme</guid>
        
        <category>Github</category>
        
        <category>Readme</category>
        
        <category>Profile</category>
        
        <category>Portfolio</category>
        
        
        <category>development</category>
        
      </item>
    
      <item>
        <title>DINO(Emerging Properties in Self-Supervised Vision Transformers) 논문 리뷰</title>
        <description>&lt;h1 id=&quot;들어가며-&quot;&gt;들어가며 …&lt;/h1&gt;

&lt;p&gt;제목에서 알 수 있듯이 이 논문은 &lt;strong&gt;Vision Transformer&lt;/strong&gt;가 자기 학습을 통해 &lt;strong&gt;습득할 수 있는 능력이나 특성&lt;/strong&gt;에 대해 논의한다. ViT의 프레임워크가 제안된 배경에는 자연어 분야의 Transformer 구조가 존재하는데, 이미 GPT나 BERT와 같은 후속 연구를 기반으로 NLP에서는 Large Dataset의 self-supervised learning이 downstream task에서 보다 풍부한 semantic information을 제공한다는 사실이 증명된 바 있다. 이와는 다르게 ViT의 학습 구조를 보게 되면 언어 모델과 같이 대용량의 이미지 데이터셋을 사용하여 사전 학습하는 과정이 이후의 downstream task에도 도움이 된다는 사실은 증명이 되었으나, 여전히 &lt;u&gt;supervised learning 구조&lt;/u&gt;에서 벗어나지 못한 것을 알 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;발견한-특성들&quot;&gt;발견한 특성들&lt;/h1&gt;

&lt;p&gt;논문 구성은 간단하게도 아이디어를 develop하는 과정(self-supervised learning 방법론에 대한 approach)에 대한 motivation으로 시작하게 되고, 해당 방법론으로부터 온 &lt;strong&gt;효과&lt;/strong&gt;를 언급하면서 이를 증명할 &lt;strong&gt;여러 실험 결과들&lt;/strong&gt;을 보여주게 된다.  논문에서 발견한 ViT의 self-supervised learning 특성을 요약하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;자기 학습을 통해 획득한 ViT의 feature는 이미지의 semantic segmentation 정보를 가지게 되고, 이는 지도 학습으로 학습된 ViT나 convnet에서도 발견되지 않은 특성이다.  실제로 아래 그림과 같이 attention 정보를 통해 네트워크가 각 이미지 단위로 포커싱하고있는 영역이 곧 이미지 상에서 object의 semantic한 정보 그 자체라는 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/1d391dce-5b69-4f2c-86a6-a42717b885d8&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;또한 이렇게 획득된 feature는 유사도에 기반한 $k$&lt;em&gt;-NN&lt;/em&gt; classifier로 활용될 수 있고 small ViT로도 ImageNet(recognition task)에서 좋은 정확도를 보임을 확인하였다.&lt;/li&gt;
  &lt;li&gt;마지막으로 여러 셋팅에서의 실험 및 충분한 ablation을 통해 ViT의 자기 학습 과정에서 효과적으로 쓰일 수 있는 방법론들을 직접 실험들을 통해 규명했으며, momentum encoder, multi-crop training 그리고 smaller patch(more number of patches)가 중요하게 쓰인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-learning-frameworks&quot;&gt;Self-supervised learning Frameworks&lt;/h1&gt;

&lt;p&gt;대표적인 label이 없는 환경에서의 unsupervised(self-supervised) learning 접근법으로는 &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1911.05722&quot;&gt;MoCo&lt;/a&gt; 그리고 &lt;a href=&quot;https://arxiv.org/abs/2006.07733&quot;&gt;BYOL&lt;/a&gt;가 있었다. 갑자기 이 얘기를 꺼낸 이유는 이 페이퍼에서 논하고자 했던 property가 곧 ViT의 self-supervised learning으로부터 나오기 때문에, 제안된 structure의 근거를 알기 위해서는 이전 논문들의 참고가 필수적이기 때문이다. DINO 논문에서는  SimCLR, MoCo 그리고 BYOL 중 BYOL에서 inspiration을 얻었다고 한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/051e06fa-82c9-4410-9e21-953f40652631&quot; width=&quot;500&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/6e7e1653-06d9-454c-8a65-8e4efca985c8&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SimCLR의 구조는 첫번째 그림과 같다. Input $x$가 주어지면 이를 정해진 random augmentation을 적용한 각각의 샘플 $\tilde{x}_i$ 그리고 $\tilde{x}_j$로 만들게 되고, 이를 뉴럴 네트워크 $f(\cdot)$에 통과시킨 output $h_i$, $h_j$ 를 하나의 representation/embedding이라고 했을 때 이를 Linear operation($g(\cdot)$)으로 mapping한 최종 latent인 $z_i$ 그리고 $z_j$를 contrastive하게 학습하는 방법을 사용하였다.  Moco도 큰 틀에서는 contrastive learning과 두 개의 branch를 사용한다는 점에서 SimCLR와 거의 동일하지만, 차이점이라고 한다면 SimCLR은 배치 내에서 동일한 인코더를 기준으로 representation 학습을 진행하지만 MoCo는 학습이 되지 않고 EMA 방식으로 업데이트되는 momentum encoder가 사용된다는 점이다. 쿼리에 사용되는 배치는 매 학습마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enqueue&lt;/code&gt; 및 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dequeue&lt;/code&gt; 를 통해 최신 mini-batch가 지속적으로 업데이트되며, positive logit은 동일 배치의 샘플에 대해, negative logit은 이전 queue의 샘플에 대해 연산을 진행하게 된다. Querying에 사용되는 encoder를 점진적으로 학습하는 방법을 적용했다는 점에서 차이가 생긴다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;byol-paper에-대한-짧은-논문-리뷰&quot;&gt;BYOL paper에 대한 짧은 논문 리뷰&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/d20e37ea-3c2b-47e8-8016-be9e1f295316&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;BYOL은 momentum encoder의 장점을 가져오면서 학습의 전반적 형태는 SimCLR와 같은 방식을 가져왔다. 그러나 알고리즘 측면에서 큰 차이가 있는데, 이는 바로 BYOL은 contrastive learning을 하지 않는다는 점, 즉 negative pair가 필요하지 않다는 것이다.&lt;/p&gt;

&lt;p&gt;BYOL에서 가장 크게 주목할 점은 어떻게 negative pair와 같이 collapse를 방지할 만한 장치가 없이도 안정적인 학습이 가능한가에 대한 부분이다. 바로 이 부분에서 대체 왜 online/target 두 브랜치가 서로 assymetric(비대칭)하게 구성되었는가를 확인할 수 있다. 예컨데 predictor $q_\theta(\cdot)$는 projection 목적이 되는 $g_\theta$와 동일한 신경망 구조를 가진다.  단순하게 생각했을때 prediction은 하나의 classifier라고 생각할 수 있지만 그렇지는 않고 projection network와 같은 dimension의 output을 내보낸다. 하지만 바로 이 projector 부분이 학습되면서 optimal point에 놓여있는 것이 가장 주요한 학습 키포인트로 작용한다.&lt;/p&gt;

&lt;p&gt;Projector가 수렴했다는 것은 곧 projector가 어느 정도 optimal한 영역에 있다고 볼 수 있고 이를 $q^\ast_\theta$라고  한다면 online branch의  input이 되는 $z_\theta$에 대해 $q^\ast_\theta(z_\theta) = \mathbb{E}[z^\prime_\xi \vert z_\theta]$로 표현이 가능하다.  수식 상에서 조건부 expectation은 $z_\theta$에 대한 함수가 되며, 조건부 확률 분포와 동일한 의미를 가진다. 즉 우리가 흔히 optimal하게 학습된 neural network를 특정 도메인의 데이터셋 ${X, Y}$에 대해 parameterized posterior $p_\theta(Y \vert X)$로 표현하는 것처럼 projector가 수렴했다는 가정 하에 $z^\prime_\xi$와의 수식으로 해석할 수 있다. 결국 이 가정을 통해 수식을 다시 전개하게 되면 simplified BYOL loss(원래 BYOL에서는 view를 교차하는 형태로 symmetric한 cost function을 구성하는 것과 더불어 latent의 정규화 작업이 추가됨)은 다음과 같이 표현 가능하며&lt;/p&gt;

&lt;p&gt;[
\mathcal{L}_\text{BYOL} = \mathbb{E}\left(\parallel \mathbb{E}(z^\prime_\xi\vert z_\theta)-z_\xi^\prime\parallel_2^2\right)
]&lt;/p&gt;

&lt;p&gt;결국 학습 파라미터(online branch) $\theta$에 대한 gradient는 다음과 같이 expected variance의 gradient로 수렴하게 된다.&lt;/p&gt;

&lt;p&gt;[
\nabla_\theta \mathcal{L}_\text{BYOL}= \nabla_\theta\mathbb{E}\left(\sum_i \text{Var}(z^\prime_{\xi, i} \vert z_\theta) \right)
]&lt;/p&gt;

&lt;p&gt;이러한 가정은 optimal projector가 수렴했다는 전제에서 성립하게 되는데, 이를 통해 BYOL loss는 수렴된 projector를 변화시키지 않고 online network를 업데이트할 수 있다. 위의 수식은 파라미터 및 projection을 다변수로 가지는 최적화 함수를 &lt;strong&gt;Lagrangian으로 표현했을 때&lt;/strong&gt;의 envelop theorem 그리고 optimality condition에 기반한다.&lt;/p&gt;

&lt;p&gt;BYOL에서는 이렇게 업데이트되는 $\theta$에 대해 online branch와 target branch $\xi$가 동시에 감소하는 방향은 loss surface $\mathcal{L}$에는 정의될 수 없다는 것이다. Target branch에서의 projection $z^\prime_\xi$와 online branch에서의 $z_\theta$에 대한 Variance로 loss 최적화 식을 만들었었고 이게 의미하는 바는 projector가 어느 정도 수렴한 상황에서 가장 말단의 posterior는 고정된 상태라고 보는 것이다. 임의의 random variable에 대해 조건부 분산은 조건 변수가 추가될수록 이전 분산의 lower bound가 된다. 만약 BYOL을 통한 최적화 과정이 collapse를 일으킨다면 online network의 projection인 $z_\theta$는 더이상 무작위로 분포한 확률 랜덤 변수가 아닌 constant $c$로 고정될 수 있고, 이는 parameter space에서기존 업데이트 과정이 lower bound가 됨을 명시할 수 있는 근거가 된다.&lt;/p&gt;

&lt;p&gt;[
\text{Var}(z^\prime_\xi \vert z_\theta) \le  \text{Var}(z^\prime_\xi \vert c)
]&lt;/p&gt;

&lt;p&gt;즉 collapse가 일어날 수 있는 환경이 parameter surface에서 보다 큰 값을 가지기 때문에 더 불안정(unstable), collapse가 발생하지 않는다.&lt;/p&gt;

&lt;p&gt;만약 반대라면 어떻게 될까? 이 논문에서는 $\xi$를 loss function을 기준으로 업데이트하지 않고 EMA를 사용하여 점진적 과부하를 걸었는데, 이는 같은 위의 수식으로 그 이유를 찾을 수 있다. Target network에 collapse가 발생한다면 이번에는 $z^\prime_\xi = c$ 인 deterministic constant가 되고, 이번에는 조건부 변수가 아닌 메인 변수에 해당되므로 분산이 0이 된다.&lt;/p&gt;

&lt;p&gt;[
\text{Var}(c \vert z_\theta) = 0 \le \text{Var}(z^\prime_\xi \vert z_\theta)
]&lt;/p&gt;

&lt;p&gt;즉 $\xi$에 대해서 학습하게 되면 무조건 collapse가 발생하게 된다는 것을 볼 수 있다. BYOL는 이러한 이론적 배경에 근거하여 negative pair를 굳이 구하지 않더라도 similarity loss를 기반으로 점진적으로 latent를 bootstrapping (과거의 online parameter가 미래의 online parameter의 학습에 도움이 되는 과정)하는 방법을 제시하였고, 이는 batch size로부터의 자유 및 자기 학습 방법의 지평을 보다 넓힐 수 있는 계기가 되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/b455d805-e79e-402e-a153-aef0aea6801b&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dino-approach&quot;&gt;DINO approach&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/781c6f72-b2a2-477e-a246-04373d81194c&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;DINO는 ‘knowledge distillation with no labels’의 줄임말로, 말 그대로 ViT 학습에 SSL 프레임 워크를 제안한 형태가 된다. 이 방법 역시 student/teacher(혹은 online/target)의 두 브랜치 간의 학습이 진행되는데, 안정적인 pseudo label을 만들어내는 teacher은 loss term에 대한 파라미터 최적화가 발생하지 않고consistency를 통해 지속적으로 update되는 student parameter를 지수 평균 이동(exponential moving average)으로 가져온다.&lt;/p&gt;

&lt;h3 id=&quot;knowledge-distillation&quot;&gt;Knowledge Distillation&lt;/h3&gt;

&lt;p&gt;논문에서 접근한 SSL은 다음과 같다. Student model과 teacher model은 학습되는 중간에는 데이터가 매핑되는 함수로 작용하고, 이는 곧 probability mapper로 해석 가능하다. 만약 student network($g$)의 파라미터를 $\theta_s$라 한다면 입력 신호 $x$에 대한 output logit $g_{\theta_s}(x)$를 구할 수 있다. 그리고 이 logit에 softmax function을 적용하면 확률로의 직접 매핑이 가능하다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt;가 적용되는 dimension은 특징자(feature embedding) 축이라고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;[
P_s(x)^{(i)} = \frac{\exp(g_{\theta_s}(x)^{(i)}/\tau_s)}{\sum_{k=1}^K\exp(g_{\theta_s}(x)^{(k)}/\tau_s)}
]&lt;/p&gt;

&lt;p&gt;여기서 temperature $\tau_s$가 사용되는데, 이는 다양한 논문들에서 probability distribution의 분포를 결정하는 하이퍼파라미터 혹은 학습 가능한 파라미터로 많이 사용된다. 이 논문에서의 temperature parameter의 목적은 student network에 의한 probability의 sharpness 조절 역할을 하게 된다. 마찬가지로 teacher network에 대해서도 다음과 같은 formulation이 가능하다.&lt;/p&gt;

&lt;p&gt;[
P_t(x)^{(i)} = \frac{\exp(g_{\theta_t}(x)^{(i)}/\tau_t)}{\sum_{k=1}^K\exp(g_{\theta_t}(x)^{(k)}/\tau_t)}
]&lt;/p&gt;

&lt;p&gt;Knowledge distillation에서 학습은 teacher의 output을 일종의 ground truth로 가정한 student output과의 consistency loss이다. 즉 cross entropy에서 one-hot label을 teacher network의 output으로 바꿨다고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;[
\underset{\theta_s}{\min} H(P_t(x),~P_s(x)) = \min_{\theta_s} {-P_t(x) \log P_s(x)}
]&lt;/p&gt;

&lt;p&gt;크로스 엔트로피가 의미하는 것이 정보이론에서 “하나의 확률분포”가 “또다른 확률분포”가 가지는 정보와 얼마나 가까운지에 따른 거리 metric이기 때문에 결국 학습 목적은 학생으로 하여금 선생의 지식을 잘 모방하도록 하는 것이 된다. 하지만 단순히 이 방법론으로 마무리되는 알고리즘은 아니고, DINO는 효과적인 학습을 위해 다양한 방법들을 추가하게 된다. 예컨데 위의 수식은 앞서 보여준 framework와는 다르게 augmentation에 대한 내용이 없지만, 저자는 바로 이 수식 전개 직후 단순 distillation을 사용함에 따라 생기는 문제점들을 언급한다.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h3&gt;

&lt;p&gt;Transformer 구조가 가지는 가장 큰 문제점 중 하나가 local-to-global correspondence가 적다는 것이다. Transformer는 attention을 기반으로 단번에 global information을 인지하기 때문에 convolution network에 비해 가지는 장점도 있겠지만, local information을 포착하기 전에 모든 attention map들이 global feature에서 수렴해버린다면 CNN이 가지는 계층적 구조에 의한 correspondency(feature간 상관관계에서 얻을 수 있는 high to low level 효과)를 SSL에서 기대할 수 없다는 문제가 있다.&lt;/p&gt;

&lt;p&gt;따라서 이를 해결하는 방법으로 augmentation의 비대칭을 사용하였다. 이 프로세스를 요약하면 다음과 같다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Teacher가 local/global에 대한 consistency를 가지고 어느 정도 수렴했다는 가정 하에, teacher는 이미지의 global한 형태를 보고 ‘그럴 듯한’ 예측을 한다.&lt;/li&gt;
  &lt;li&gt;위의 가정이 있다면 teacher network에는 계속 global한 image 정보만 주면 된다.&lt;/li&gt;
  &lt;li&gt;Teacher는 student의 파라미터로부터 EMA된다. 즉, teacher의 바람직한 수렴을 위해서는 student가 앞서 말했던 local/global에 대한 consistency 정보를 학습할 수 있는 환경이 되어야한다.&lt;/li&gt;
  &lt;li&gt;따라서 student에는 local image 정보를 같이 준다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;길게 설명했지만 풀어쓰자면 teacher network에는 이미지에 큰 범위에서의 augmentation이 들어간 global view $x_1^g$, $x_2^g$만 예측에 사용되고, student network에는 multi-crop strategy와 같은 이미지의 작은 범위까지의 augmentation이 적용된 local/global  정보가 예측에 사용된다.&lt;/p&gt;

&lt;p&gt;[
\underset{\theta_s}{\min}\sum_{x \in {x_1^g,~x_2^g}} ~~\sum_{x^\prime\in V,~x^\prime \neq x} H(P_t(x), P_s(x^\prime))
]&lt;/p&gt;

&lt;p&gt;논문에서는 보통의 방식과 같이  multi-crop image들을 생성했는데, 2개의 global views는 원본 이미지 대비 $50\%$보다 큰 크기만큼 잘라서 쓰고 여러 local view는 반대로 $50\%$보다 작은 크기만큼 잘라서 쓴다.&lt;/p&gt;

&lt;h3 id=&quot;avoiding-collapse&quot;&gt;Avoiding collapse&lt;/h3&gt;

&lt;p&gt;Self-supervised learning의 문제점은 representation 학습에 대한 ground truth가 없기 때문에 collapse가 발생할 수 있다는 것이다. 사실상 우리가 많이 알고 있는 contrastive learning이든, clustering 방식이든, predictor를 다는 BYOL과 같은 구조라던지 Batch Normalization을 도입하는 등등의 approach는 공통적으로 collapse를 막는 역할을 하게 된다. 물론 DINO 또한 normalization 구조라던지 앞서 언급한 여러 방법론으로 stabilization을 수행할 수 있었지만, 이 논문에서는 momentum teacher network의 output을 centering 및 sharpening하는 구조를 통해 이러한 효과를 얻을 수 있다고 한다.  Sharpening/Centering에 대한 내용은 조금 알아보기 쉽게 나타내면 Sharpening은 temperature 조절을 통해 softmax 예측값 분포를 보다 명확하게 드러내는 것이고 centering은 teacher output에 center value $c$를 bias term으로 더해주어 예측값 사이의 차이를 조절해주게 된다. 즉 sharpening과 centering은 효과만 보게 되면 서로 반대의 역할을 수행한다. 여기서 드는 의문점은, 굳이 sharpening을 통해 prediction의 entropy minimization을 수행할 목적이었다면 왜 다시 centering이라는 방법으로 다시금 prediction을 재조정하는 과정을 거치는지에 대한 부분이다. 이 부분에 대해서 나름대로 이해한 것은 다음과 같다.&lt;/p&gt;

&lt;p&gt;우선 centering에 사용될 bias term $c$는 다음 식을 통해 exponentially update가 된다. EMA 방식으로 teacher parameter가 업데이트되는 것과 동일하다.&lt;/p&gt;

&lt;p&gt;[
c \leftarrow mc + (1-m)\frac{1}{B}\sum_{i=1}^B g_{\theta_t}(x_i)
]&lt;/p&gt;

&lt;p&gt;식을 자세히 보면 center $c$에는 결국 학습 시 사용되는 batch size랑은 무관하게, 기존 input에 대한 model의 output(prediction) 정보를 평균으로 저장하게 된다. 이제 centering에 대한 맥락은 얼추 이해했고, 다시 sharpening으로 돌아가보도록 하자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/33257496-03e5-4908-8fb8-847766f63a92&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;예컨데 고양이라는 이미지에 대해 모델이 낸 prediction을 sharpening하는 작업을 하게 되면 뭉뚱그려진 예측값을 어느 정도 명확하게 하면서 feature map을 선명하게 만들어 준다는 장점이 있지만, 만약 배치 단위로 들어오는 특정 input이 모델로 하여금 지속적으로 collapse가 발생하게 한다면, 이는 불난 집에 부채질하는 격이 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/ae4354d5-1b3c-48d5-b665-69e09d4403c7&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;원래 목적이라 함은 다른 input에 대해 특징을 잘 잡아내는 feature를 뽑고자 sharpening을 도입했는데, contrastive learning과 같은 제어장치가 없다면 모델은 그냥 단순히 네트워크 예측 자체의 entropy를 낮추는 방향으로 끊임없이 학습이 될 것이기 때문에 이미지의 종류에 상관없이 단일의 feature를 뽑게 되고, 이러한 문제를 trivial solution이 발생한다고 한다. 결국 기존 SSL approach를 사용하지 않고는 이를 근본적으로 해결하기 어렵다는 문제가 발생한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/fe9e1b73-0155-47b2-8d01-09b3d8e55e9f&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그렇기 때문에 만약 centering term이 있다면 이를 단순히 model prediction에 더해주는 것만으로도 이전 배치들의 정보를 가져올 수 있으며, batch size의 크기에 robust한 학습 효과를 보여주는 것이다. 예컨데 contrastive learning에서는 positive sample과 negative sample 쌍을 얻기 위해 최대한 많은 배치 수가 필요했고, 그 이유는 모델이 학습할 때 metric learning을 적은 단위의 배치 내에서 진행하는 것보다는 큰 배치 내에서 진행하는 것이 전체 데이터셋의 확률 분포를 잘 나타낼 수 있기 때문이었다. 하지만 위와 같이 output을 뽑아서 배치 단위의 prediction을 저장하고, 이를 이후의 output을 sharpning할 때 smoothing에 사용하는 것만으로도 배치 사이즈를 키우지 않고 이러한 효과를 볼 수 있다는 것이 바로 sharpning/centering이 가지는 장점이다. 사실 까놓고 말하자면 단순하게 이전 prediction을 일종의 prototype으로 저장해놓고 쓴다는 느낌인데, 저자들은 이 방법론이 실제로 학습에 미치는 영향을 보여주기 위해 실험을 진행하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/ad2abdce-e626-4466-a81b-eb8bec041dc1&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Sharpening의 효과는 entropy를 0으로 만든다. 그리고 centering의 효과는 smoothing을 통해 어떤 input이 들어오든 entropy를 유지시킨다. 둘 중 하나만 사용하면 epoch에 따라 representation overfitting/underfitting이 발생하는모습을 잘 확인할 수 있다. 무엇보다 이를 잘 보여주는 실험이 KL divergence에 있지 않을까 싶다.&lt;/p&gt;

&lt;p&gt;Teacher/student 구조를 쓰면서 얻고 싶은 장점은 EMA 방식으로 기존 representation 정보를 차곡차곡 모아가는 teacher network의 prediction을 student가 따라가면서 서로 간의 학습에 bootstrapping이 일어날 수 있다는 것인데 만약 representation이 collapse가 된다면 이러한 효과를 볼 수 없을 것이고, 결국 bootstrapping이 없다는 것은 학습이 진행되면서 student/teacher prediction 차이가 없어진다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h1&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;SSL/Unsupervised Learning의 경우 학습된 feature를 증명하는 과정이 여러 가지로 분류된다.
우선 downstream task에 맞게 head를 다는 과정이 필요하고, 이 head를 어떻게 써먹냐에 따라 linear classifier/fine-tuning/k-NN classifier로 분류된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear classifier : 학습된 backbone을 frozen한 채로 linear classifier만 학습해서 representation의 효과를 보고자 하는 것&lt;/li&gt;
  &lt;li&gt;Fine tuning : 학습된 backbone을 head에 붙인 채로 fine tuning하여 representation의 효과를 보고자 하는 것&lt;/li&gt;
  &lt;li&gt;k-NN classifier : classifier 같은 부수적인 요소 없이 단순히 embedding으로 retrieval해서 representation/metric learning 자체 효과를 보고자 하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/e782636e-65de-4b28-8881-ee18adef47e1&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/120dd910-ba77-4247-8773-60d6057f2728&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;뭐 성능 자체와 관련해서는 상당히 좋게 나온 것을 확인할 수 있고, ViT baseline의 다른 SSL 방식과 비교했을 때도 유의미하게 높은 classification 성능을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;vit-attention-map&quot;&gt;ViT Attention map&lt;/h3&gt;

&lt;p&gt;하지만 classification 보다는 DINO의 가장 큰 특징은 ViT의 attention map을 보면 잘 드러나는데, 바로 local feature에 attention을 집중할 수 있다(localization)는 것이다. 이는 기존 ViT 방식으로는 얻을 수 없는 feature map에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/021ba60f-0505-44ae-90e1-5ddf186db625&quot; width=&quot;350&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/d8418382-6a65-45fe-823a-36234304ac82&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/b33c0e0a-6a45-4a91-861f-b3a32fd4252a&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Segmentation처럼 high-level image task의 경우 모델의 예측이 정교해야하기 때문에 상대적으로 classification task에 비해 SSL이 달성할 수 있는 성능 수치가 그리 높지 않았다. 그럼에도 불구하고 DINO의 attention map을 보면 알 수 있듯이 이 페이퍼에서 제안한 학습 방법은 ViT의 input에 대한 attention을 효과적으로 localization하는 것에 성공하였고, 정량적으로도 그 수치를 증명하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;

&lt;p&gt;DINO는 BYOL을 비롯한 기존 SSL 방식에서 motivation된 self-distillation 구조와 더불어 collapse를 방지하고 학습 안정화를 위해 sharpening/centering을 도입하여 ViT를 효과적으로 학습하였다. 그런데 이렇게 안정적으로 ViT를 라벨 없이 SSL로만 학습하고 보니 이게 무슨 일이람. ViT의 attention map이 localization되는 중요한 변화를 확인할 수 있었다. 이러한 점이 시사하는 바는 상당히 크다.&lt;/p&gt;

&lt;p&gt;지금까지는 supervised learning이 절대적인 학습법이었으며, 사실 SSL이 학습 안정화를 토대로 가끔 supervised learning의 성능을 넘는 경우도 있긴 했지만은 모든 task에 정통으로 사용될 수 있는 방법은 아니었으며 linear probing이나 fine tuning 시에 미리 학습된 representation의 효과를 강하게 보여주었지 실질적으로 SSL로 학습된 representation이 가능성을 보여주는 경우는 많지 않았다. 하지만 애초에 구조상 inductive bias가 없어 localization이 힘든 transformer baseline인 ViT를 SSL하였더니 attention map이 segmentation 효과를 보여주었고, 이는 NLP가 아닌 Computer Vision 분야에서도 classification 뿐만 아니라 여러 task에 SSL이 우월한 성능을 보여줄 수 있음을 증명하였다.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Nov 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/dino</link>
        <guid isPermaLink="true">http://localhost:4000/blog/dino</guid>
        
        <category>SSL</category>
        
        <category>ViT</category>
        
        <category>DINO</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>(LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기</title>
        <description>&lt;h1 id=&quot;gpu에서-동작하는-챗봇-구현하기&quot;&gt;GPU에서 동작하는 챗봇 구현하기&lt;/h1&gt;
&lt;p&gt;이전 글에서 Llama-2-cpp를 사용하여 CPU로도 동작하는 챗봇을 구현했었다. 이번에는 리소스가 있다는 가정 하에, 보다 빠르게 입력된 prompt에 대한 답변을 처리할 수 있는 GPU를 활용한 챗봇을 만들어보기로 하였다. 마찬가지로 Llama-2를 사용하였으며, Llama-2의 경우에는 meta에 신청서만 제출하면 네트워크를 다운받을 수 있는 링크가 주어진다. 우선 &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama-2 github&lt;/a&gt;에 들어간다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/75d65b67-a6b9-4f1d-ab53-b41de523a97d&quot; width=&quot;800&quot; /&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/77f26735-339b-479d-8326-f1dc5288ca48&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;다운받고자 한다면 accept를 받아야한다는 소리가 나온다. 경험상 양식을 채우고 이메일 수신을 기다렸을때 빠르면 5분 안에 바로 승인이 났다. 그리고 링크를 받게되면 이메일로 오게 되는데, 이때의 링크를 잘 저장해두자. 참고로 링크의 유효기간은 24시간이기 때문에, 승인 받고나서 ‘나중에 다운받아야지’하면 안된다. 당일에 모델을 다운받지 않으면 바로 해당 링크는 무용지물.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/ff901970-2c54-4a5e-8560-a357d20bd190&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;빨갛게 가려놓은 부분이 바로 링크다. 만약 본인이 이메일로 해당 내용을 회신받지 못했다면 넉넉하게 기다렸다가 다시 승인 요청을 보내보는 것을 추천한다. 아무튼 다운받는 법은 정말로 간단하다. 우선 Llama 깃허브를 클론한 뒤에 다운로드 스크립트를 켠다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/facebookresearch/llama.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;llama
pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
bash download.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;제대로 진행했다면 다음과 같은 문구가 뜨는데,&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/17e9b043-2491-497a-9714-5f149b05f041&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;바로 여기에 아까 받은 이메일의 링크를 복붙해서 넣으면 된다. 이후 다운로드 받는 과정은 output을 보면서 차근차근 따라하면 된다. 본인은 가능한 용량인 7B, 13B만 다운받았다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;llama-2-챗봇-모델과-discord-bot-코드-연결하기&quot;&gt;Llama-2 챗봇 모델과 discord bot 코드 연결하기&lt;/h1&gt;
&lt;p&gt;우선 알아야할 점은 Llama-cpp와 Llama-2 GPU 버전은 모델이 동작하는 형태가 다르다. Llama-2 챗봇은 다음과 같은 input prompt 구조를 가져야한다. 예컨데 1명이 챗봇을 사용하는 상황을 가정하겠다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prompt_for_llama2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;챗봇이 어떠한 방식으로 대답했으면 좋겠는가 작성&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Query #1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;엄밀하게 따지면 prompt는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;List[Dialog]&lt;/code&gt; 형태이지만 간단하게는 리스트 안에 리스트가 내장된 구조를 생각하면 편하다. 가장 외곽의 리스트는 $n$개의 병렬적인 대화를 수용한다고 생각하면 되고, 우리는 실제로 단일 dialog에 하나의 유저에 대한 대화를 지속적으로 이어나갈 것이기 때문에 위와 같이 내부의 리스트에만 계속 query를 쌓아가는 구조가 된다. 간단한 코드 구조는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;llama&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Llama&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fire&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;chat_example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Initialize Llama for text generation
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Llama&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ckpt_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;llama-2-7b-chat/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tokenizer_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tokenizer.model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dialog_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Inference on Llama2 Model
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat_completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialog_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;max_gen_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;top_p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Extract only answer(Bot reply)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;generation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat_example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;대화-쌓아가기&quot;&gt;대화 쌓아가기&lt;/h1&gt;

&lt;p&gt;기존에 했던 방식과 마찬가지로(이전 글인 ‘&lt;a href=&quot;https://junia3.github.io/blog/chatbot&quot;&gt;CPU에서 돌아가는 나만의 디스코드 챗봇 만들기&lt;/a&gt;’ 참고) 챗봇은 이전 대화 내용을 어느 정도 고려하여 맥락을 맞출 필요가 있다. 이를 위해 대화를 쌓는 방식을 다음과 같이 지정해주었다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# Initialize global variables
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialog_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Define a function to make user dialog
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_usr_dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Define a function to make AI dialog
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_ai_dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;assistant&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;answer_for_chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_usr_dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialog_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialog_prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat_completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dialog_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;max_gen_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;top_p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;generation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_ai_dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_chat_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;I could not generate message 🥲 ...&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;쌓는 방식을 요약하면 다음과 같다. 챗봇이 대답해야하는 형태 (본인의 챗봇의 컨셉은 고양이이므로, 이를 system이라는 역할로 알려줌)를 prompt로 고정해둔다. 그리고 유저가 질문하는 내용을 llama에서 요구하는 프롬프트 형태로 바꿔주는 함수와, assistant의 대답을 llama에서 요구하는 프롬프트 형태로 바꿔주는 함수 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_usr_dialog&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_ai_dialog&lt;/code&gt;를 각각 설정해준다. 쿼리가 들어오게 되면 이를 유저의 질문 형태로 바꿔 기존 로그에 추가하고, 모델에 들어가는 input에는 챗봇의 컨셉 + 질의 응답 로그룰 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;List&lt;/code&gt;로 wrapping하는 절차를 거친다.&lt;/p&gt;

&lt;p&gt;또한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_chat_logs&lt;/code&gt;라는 integer value를 통해 대화 로그의 메모리 관리를 하게 되는데, 대화 내용이 너무 길어지게 되면 모델 inference 시간이 증가하므로 이를 방지하기 위함이다. CPU에서의 방식과 다른 점이 있다면, input으로 들어가는 질의 응답의 경우 list의 요소가 유저/AI가 반복되어 들어가기 때문에 짝수 갯수만큼을 지워줘야하고, 이를 단순히 리스트의 indexing(windowing)으로 구현하였다. 대화가 반복될수록 이전 대화는 지워지고, 새로운 대화 내용이 로그에 남아 input에 사용된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;distributed-parallel-for-chatbot&quot;&gt;Distributed parallel for ChatBot&lt;/h1&gt;

&lt;p&gt;GPU 연산의 경우 랩 사용량이 꽤 되므로 이를 보조하기 위한 GPU 병렬 처리 시스템이 기본이다. Llama-2에서는 이를 자동으로 구현하였으며, 다음과 같이 본인의 컴퓨터 스펙에 따라 GPU 사용을 결정해주면 된다.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; torchrun &lt;span class=&quot;nt&quot;&gt;--nproc_per_node&lt;/span&gt; 1 chatbot.py &lt;span class=&quot;c&quot;&gt;# GPU 1개 사용&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; torchrun &lt;span class=&quot;nt&quot;&gt;--nproc_per_node&lt;/span&gt; 2 chatbot.py &lt;span class=&quot;c&quot;&gt;# GPU 2개 사용&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; torchrun &lt;span class=&quot;nt&quot;&gt;--nproc_per_node&lt;/span&gt; 3 chatbot.py &lt;span class=&quot;c&quot;&gt;# GPU 3개 사용&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; torchrun &lt;span class=&quot;nt&quot;&gt;--nproc_per_node&lt;/span&gt; 4 chatbot.py &lt;span class=&quot;c&quot;&gt;# GPU 4개 사용&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;참고로 기본 셋팅의 경우 GPU는 아이디 오름차순으로 쓰이게 된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;디스코드-챗봇-시스템-코드와-연결하기&quot;&gt;디스코드 챗봇 시스템 코드와 연결하기&lt;/h1&gt;
&lt;p&gt;기존 디스코드 챗봇 시스템은 병렬 GPU에 대한 고려가 없었기 때문에 이를 무시할 수 있었지만, Llama 코드가 들어간 이상 병렬 처리가 가능하게끔 코드를 일부 손봐야한다. 구현 과정은 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Discord bot command to chat with Llama
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer_for_chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;만약 커맨드 상에서 chatting을 요청하고 이에 대한 query를 전송하면, fire 함수를 통해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;answer_for_chat&lt;/code&gt; 함수를 불러오게 된다. 해당 함수는 위에서 소개한 함수 코드와 완전히 동일하며, query에 대한 Llama 모델의 인퍼런스를 담당한다. 모델을 불러오는 것은 코드 최초 실행 시 단 ‘한번만’ 수행한다. 최초 실행 시에 광역변수 추가 및 초기화 과정에 대해 다음 코드를 추가하였다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Initialize Llama for text generation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Llama&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ckpt_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;llama-2-7b-chat/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenizer_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tokenizer.model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Initialize global variables
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialog_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dialogs_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_chat_logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결과-확인해보기&quot;&gt;결과 확인해보기&lt;/h1&gt;

&lt;p&gt;본인은 A6000 서버를 사용하여 테스트하였고, 각 GPU의 RAM 용량은 49기가바이트이다. 단일 GPU를 사용하면 다음과 같이 돌아가게 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/d7da1cb6-ceab-4d50-981b-b0f28f025361&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대략 15~16기가 정도의 용량만 있다면 7B 모델을 넉넉하게 수용할 수 있는 것을 확인하였다. CPU와 비교했을때 긴 대답을 요구하는 질의에 대해서도 확연히 올라간 채팅 성능을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/91578953-0e09-4e85-b239-7d128f0845d0&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;13B 모델 또한 수용이 가능하여 확인해보았는데, 13B 모델의 경우에는 무슨 일인지 단일 GPU로는 돌아가지 않고 무조건 2개 이상의 GPU로 돌려야했다. 참고하면 좋을 것 같다.&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Sep 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/chatbotgpu</link>
        <guid isPermaLink="true">http://localhost:4000/blog/chatbotgpu</guid>
        
        <category>Automatic system</category>
        
        <category>Chatting</category>
        
        <category>LLM</category>
        
        <category>NLP</category>
        
        
        <category>development</category>
        
      </item>
    
      <item>
        <title>OpenAI의 새로운 이미지 생성 모델, DALLE-3의 등장</title>
        <description>&lt;p&gt;이미지 생성 기술인 DALLE보다는 ChatGPT로 일반 대중들에게 보다 빠르게 다가갈 수 있었던 OpenAI가 자체 이미지 generative AI 모델인 Dalle의 세번째 버전을 공개하였다. &lt;a href=&quot;https://stability.ai/stable-diffusion&quot;&gt;Stable diffusion&lt;/a&gt;과 같이 프롬프트 기반의 image 생성 AI라는 점에서 큰 차이는 없지만 눈에 띄는 변화는 바로 &lt;strong&gt;ChatGPT와의 연동성&lt;/strong&gt;이라고 볼 수 있다. 언어 모델을 통해 &lt;u&gt;프롬프트 엔지니어링으로부터 자유로워지는 것&lt;/u&gt;은 사용자로 하여금 한정적인 리소스 공간 상에서 최대한의 효율을 뽑아낼 수 있다는 이점이 생기는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dalle-기존의-모델&quot;&gt;DALLE 기존의 모델&lt;/h1&gt;
&lt;p&gt;DALLE를 논문으로 봐왔던 사람이면 알겠지만 가장 첫번째 버전이었던 &lt;a href=&quot;https://arxiv.org/abs/2102.12092&quot;&gt;Zero-Shot Text-to-Image Generation&lt;/a&gt;는 트랜스포머 기반의 토큰 형태로의 학습을 위해 discrete VAE를 활용한다. 예컨데 트랜스포머에서 image와 text의 연관 학습을 위해서는 텍스트를 인코딩하는 것 뿐만 아니라 이미지를 패치 단위로 분리하거나 autoencoder와 같은 approach를 통해 인코딩해서 사용한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/ba8f180b-5275-4b6c-b90c-a0dfa333bcf0&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;물론 트랜스포머 기반이라는 점, dVAE를 통해 이미지를 축소한 뒤 이를 재복원하는 작업을 해보면 알 수 있듯 $32 \times 32$의 이미지 토큰이 이미 충분한 이미지 정보를 내포한다는 점에서 대용량의 학습을 통해 zero-shot image generation 성능을 보장할 수 있었으나, 가장 큰 문제는 필연적으로 인코딩 과정에서 트랜스포머 인코더의 modality로 사용되는 이미지 퀄리티를 손해봐야한다는 점과 prompt와의 연관성이 그리 높지 않은 이미지(논리적으로 부합하지 않는 샘플링)가 나타난다는 점이다. 이는 Autoregressive한 모델링에서 주로 발생하는 문제라고 할 수 있다.
따라서 OpenAI에서 두번째 DALLE인 &lt;a href=&quot;https://arxiv.org/abs/2204.06125&quot;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt; 논문을 제안할 때는 이러한 기존 프레임워크에서 벗어나고자 diffusion prior를 가져왔다. 기존 본인들이 제안한 기술로는 생성 성능의 한계점이 명확할 것이기 때문이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/4ef7487e-a39f-4a3e-942f-3f1c18c0fab4&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;학습 과정을 보면 GLIDE paper로부터 많은 영감을 받은 것을 확인할 수 있는데 이에 대한 내용은 본인 블로그의 &lt;a href=&quot;https://junia3.github.io/blog/glide&quot;&gt;GLIDE 리뷰글&lt;/a&gt; 그리고 &lt;a href=&quot;https://junia3.github.io/blog/diffusionpapers&quot;&gt;Contintioned diffusion models&lt;/a&gt;에 대한 글을 보면 도움이 될 것이다. 논문만 보면 텍스트 프롬프트를 주었을 때 충분한 성능을 보여주는 것 같지만, 실은 그렇지 않고 이미지 상에서 프롬프트가 제시한 디테일을 잘 살리지 못하는 경우가 많고, 이는 곧 프롬프트 엔지니어링의 관점이나 학습 과정에서의 noisy한 context의 문제로 돌아가게 된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dalle-3의-문맥-파악-능력&quot;&gt;DALLE-3의 문맥 파악 능력&lt;/h1&gt;

&lt;p&gt;DALLE-3가 가지는 가장 큰 특징은 본인들은 ChatGPT와의 연결을 통해 사용자로 하여금 DALLE-3를 최대한의 효율로 사용하기 위한 프롬프트를 자동으로 &lt;u&gt;유틸라이즈한다는 것&lt;/u&gt;이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/2f795178-ca7a-41fa-b4d0-e84c53e9aa0d&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그래서 그런지 OpenAI의 DALLE-3 홍보 영상을 보면 이러한 흐름이 아주 잘 드러나는데, 고슴도치 캐릭터를 그리는 와중에 &lt;u&gt;사용자의 목적에 맞게&lt;/u&gt; (딸 아이의 묘사를 잘 드러내는 캐릭터 생성 $\rightarrow$ 딸 아이가 고슴도치에게 이름을 지어줌 $\rightarrow$ 고슴도치의 성격(property)를 보여줄 수 있는 그림을 생성해줌) 이를 &lt;strong&gt;지속적으로 수정해가는 과정&lt;/strong&gt;을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/d9d2161e-635d-4d41-84a6-868cdf93d9c2&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그저 ChatGPT API에게 그림에 대한 부탁만 하면 이에 맞게 그림을 그려준다는 것이다. 사실 아직 모델링에 대한 내용은 알지 못하므로 이미지 생성 AI 기술 자체가 발전했다는 사실은 발견하기 힘들다. 그저 프롬프트 제안을 통해 사용자가 매뉴얼하게 넣어주지 못하는 엔지니어링을 대신해준다는 것. 프롬프트 엔지니어라는 직업조차도 이제는 무색해지지 않을까?&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;새로운-이미지-생성-ai-접근성은&quot;&gt;새로운 이미지 생성 AI, 접근성은?&lt;/h1&gt;

&lt;p&gt;DALLE는 Stable diffusion으로 유명한 Stability AI나 Midjourney와 같은 유명한 생성 모델 회사보다도 먼저 기술 도입을 하였고, DALLE 2번째 버전이 나왔을 당시에는 당시에는 DALLE 때의 여러 비판들을 수용하기 위해 waitlist를 작성하여 bias되는 이미지나 샘플의 생성을 막고 이를 제어하고자 했다. 그리고나서 비로소 작년 9월 DALLE는 드디어 waitlist를 떼고 대중들 앞에 나설 수 있게 되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/junia3/junia3.github.io/assets/79881119/f26979d1-c164-4801-b93c-e0af392df99a&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그리고 올해 10월, DALLE의 세번째 버전은 드디어 ChatGPT Plus 및 Enterprise(모두 유료 버전)에 한하여 공개된다. 그런 뒤 API 서비스가 공개될 예정이다. OpenAI는 DALLE 3의 출시에 대해 단계적으로 계획 중이지만 아직 &lt;strong&gt;무료 공개 버전이 언제 출시될지&lt;/strong&gt;는 확약하지 않았다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;새로운-이미지-생성-ai-안정성이나-저작권-문제는-없나&quot;&gt;새로운 이미지 생성 AI, 안정성이나 저작권 문제는 없나?&lt;/h1&gt;

&lt;p&gt;OpenAI가 주장한 바에 따르면 DALLE의 새로운 모델은 &lt;u&gt;부정적인 이미지 샘플 생성을 막기 위한 여러 작업을 도입&lt;/u&gt;했다고 한다. 예컨데 모델링 관점에서 red teamers(외부의 적)과 같은 관계를 만들어 system의 취약점을 파고드는 형태로 adversarial하게 기술을 업그레이드 하였으며, input classifier(분류기)를 통해 language model로 하여금 위험하거나 폭력적인 prompt를 이미지 생성 과정에서 배제할 수 있게끔 하였다. 또한 생성 모델의 문제점 중 하나인 public figure 또한 생성하지 않을 수 있게 막았다고 주장하는 바다. 그러나 회사의 정책 연구원인 샌디니 아가르왈(Sandhini Agarwal)은 안전 조치에 대한 높은 신뢰를 가지고 있다고 언급했으나, 모델은 &lt;u&gt;계속해서 개선 중이며 완벽하지 않다&lt;/u&gt;고 설명한다.&lt;/p&gt;

&lt;p&gt;그리고 사실 생성 AI라고 한다면 저작권과 관련된 문제가 가장 이슈가 되는데, OpenAI 대표는 DALL-E 3은 &lt;u&gt;특정 아티스트의 스타일로 이미지를 생성하지 않도록 학습&lt;/u&gt;되었다고 밝힌다. 이는 프롬프트에 따라 특정 아티스트의 스타일로 예술을 모방할 수 있는 기존 DALLE와는 큰 차이가 있다.&lt;/p&gt;

&lt;p&gt;OpenAI는 가능한 저작권 및 독창성과 관련된 소송을 피하기 위해 이후의 DALLE framework에서는 아티스트가 자신의 예술작품을 선택적으로 제외할 수 있도록 할 것으로 보인다. 예술가가 자신의 소유권이 있는 샘플을 제출하고 양식을 통해 삭제를 요청할 수 있으며, 해당 예술가의 이미지와 스타일과 유사한 샘플링을 차단할 수 있게 된다. 이는 예술가들이 OpenAI의 경쟁사인 Stability AI와 Midjourney, 그리고 아트 웹사이트인 DeviantArt를 고소하여, 본인들의 저작권이 달려있는 작품들을 학습에 활용했다는 점을 밝혔고 이는 곧 앞으로의 이미지 생성 모델이 여러 윤리적이나 법적 책임을 따라야한다는 점을 시사한다.&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Sep 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/dalle3</link>
        <guid isPermaLink="true">http://localhost:4000/blog/dalle3</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Generative model</category>
        
        
        <category>deep learning</category>
        
      </item>
    
  </channel>
</rss>