<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
   <!-- 피드경로 명시 -->
  <channel>
    <title> Welcome to my blog  </title>
    <description>Personal Tech Blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000https://6unoyunr.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 13 Mar 2025 16:52:37 +0900</pubDate>
    <lastBuildDate>Thu, 13 Mar 2025 16:52:37 +0900</lastBuildDate>
    <generator>Jekyll v4.3.1</generator>
    
      <item>
        <title>딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</title>
        <description>&lt;h1 id=&quot;a-little-bit-personal&quot;&gt;A Little Bit Personal&lt;/h1&gt;

&lt;h3 id=&quot;들어가기전-극히-개인적인-주저리&quot;&gt;들어가기전 극히 개인적인 주저리&lt;/h3&gt;

&lt;p&gt;나는 아직도 공부를 한다. 대부분의 개발자는 커리어를 쌓으며 끊임없는 공부가 필요한데, 왜냐하면 지금까지 익혀온 기술 스택이 더이상 트렌디하지 않아지는 경우가 많기 때문이다. 특히나 AI의 경우에는 빠른 변화를 보이는데, 그래서 그런지 실제로 논문을 쓰다가 미친 일반화 성능을 보이는 파운데이션 모델이 나와버리면 해당 task가 아예 날아가버리는 경우도 발생하기 시작했다. 이렇듯 AI 연구자의 숙명은 어쩔 수 없이 기술의 최전선에서 그 누구보다 빠르게 현황을 파악해야만 살아남는다는 사실이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img width=&quot;372&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/11605769-25eb-4fa3-b992-9baaeab13250&quot; /&gt;
    &lt;img width=&quot;278&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/75c20494-6528-4f4e-a942-0663c14cc13b&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;본인도 학부 그리고 대학원 시절을 거치며 개인적으로 새로운 인공지능 논문들이 나올 때마다 거의 즉각 읽는 습관을 들였다. 처음에는 일주일에 한 편 읽는 것도 어려웠는데 이제는 완전 디테일하게 읽지는 못해도 하루에 논문 두세편 정도는 읽어낼 수 있게 되었다. 인공지능을 연구하는 사람이기도 하고 나에게 있어 논문은 일종의 뉴스, 신문같은 존재이며, 급변하는 기술 환경에 보다 빠르게 적응할 수 있도록 도움을 주는 방법이다. 딥시크라는 모델도 테크니컬 레포트로 처음 접하여 알고는 있었지만 R1 모델의 등장이 이렇게나 파격적인 행보를 보일 줄은 몰랐다.&lt;/p&gt;

&lt;p&gt;나름 챗지피티 이후로도 인공지능 모델은 끊임없이 학계나 산업계에서 연구되었으나, 그 모든 연구들이 딱히 크게 이슈화되는 경우는 거의 없었다. 여기서의 이슈화는 사람마다 기준이 다르겠지만 적어도 본인은 지금의 scaling law를 깨부술 무언가를 원했던 것일수도 있다. 물론 다양한 연구들이 진행되었기 때문에 현존하는 수많은 closed/opened source AI의 성능 향상에 큰 영향을 끼친 것은 사실이다.  그러나 실제 서비스에 활용되는 모델을 기준으로 대부분의 연구는 덩치 키우기에 집중했다. 거대한 자본이 수많은 리소스와 컴퓨팅 파워, 리소스 그리고 Human Resource(AI Engineer)을 끌어모았고,  매우 빠른 연구 및 개발이 시작되었다. Explainability와 scalability의 줄다리기가 어느새 scalability의 승리로 마무리되는 양상이 되어버렸다. 그로 인해 인공지능 연구 방향이 바뀌기도 했으며, 기존에는 생각하지도 못했던 task가 새롭게 제안되는 일도 생겼다.&lt;/p&gt;

&lt;p&gt;사실 테크니컬 레포트를 읽어본 후기로는 딥시크는 &lt;u&gt;기술적으로 그렇게까지 독보적이거나 유니크한 모델이라고 볼 수 있을까?&lt;/u&gt;였다. 그럼에도 대단하다고 여긴 것은 현재의 기술 시장을 흔들 정도의 파급력을 가져왔다는 사실이고, 기존에 소스코드를 공개하지 않았던 OpenAI의 기술력을 따라잡기 위해 정말 많은 노력을 했다는 사실이다. 적어도 적당히 타협해서 오픈 소스 튜닝하던 대부분의 방법들보단 훨씬 유의미한 결과를 낸 것은 사실이다.&lt;/p&gt;

&lt;p&gt;아무튼 이제 글을 시작해보고자 한다. 이번 글은 테크니컬 라이팅이라기 보다는 개인적인 고찰, 혹은 일기장 정도 될 것 같다.&lt;/p&gt;

&lt;h1 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h1&gt;

&lt;h3 id=&quot;강화학습에-대한-고찰&quot;&gt;강화학습에 대한 고찰&lt;/h3&gt;

&lt;p&gt;우리는 로봇에게 어떤 일을 수행하게 시키고 싶다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/d7573063-5b10-4a1b-87a0-6079f8686023&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;지금은 딥시크, 즉 LLM 모델에 대해 얘기하고 있으니 LLM 모델을 기준으로 말하면 &lt;u&gt;‘대답 잘 하게 만들기’&lt;/u&gt; 쯤 되겠다. 옛말에 미운놈 떡 하나 준다는 말이 있는데 인공지능의 세상은 그리 호락호락하지 않다.&lt;/p&gt;

&lt;p&gt;앞으로 우리는 &lt;u&gt;예쁜놈한테 떡 하나를 더 줄거다&lt;/u&gt;. 이러한 개념을 “Reward(보상)”이라고 한다. 적절히 잘 학습된 모델을 가지고 이런저런 질의응답 Environment(환경)에서 대답을 잘하면 칭찬해주는 프로세스를 반복해서 &lt;u&gt;더욱 대답을 잘하는 모델을 만들고자 하는 것&lt;/u&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;policy-based-method의-발전&quot;&gt;Policy based method의 발전&lt;/h3&gt;

&lt;p&gt;기존의 강화학습 방식이었던 Value based method는 인공지능 모델이 취할 모든 행동에 대해 가치평가하는 모델을 학습시킨다. 그래서 인공지능 모델이 현재 상태에서 어떤 Action(동작)을 취할지에 따른 Value(가치)를 판단하고, 가치에 따라 greedy 알고리즘으로 다음 동작을 결정하는 방식을 학습하게 된다. 이때 각 상황에서 어떤 동작을 취할지에 대한 기준이 바로 Policy(정책)이다. 이때 &lt;u&gt;가치를 판단해주는 모델을 딥러닝으로 학습&lt;/u&gt;시키자는 관점이 바로 &lt;u&gt;DQN(Deep Q-Network)&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;이때 Value function에 해당되는 Q-Network의 개념이 나온 이유는 여러 동작에 대한 Reward가 가지는 불안정성을 어느 정도 &lt;u&gt;action-value function Q&lt;/u&gt;가 해소해줄 수 있다는 관점이었다 (아래는 알고리즘 코드).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/af43d1ab-d9f0-4e7e-ba2f-4c3e91b52d92&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그러나 사실 생각해보면 결국 가치 판단을 위한 모델을 학습하는 것의 최종 목적은 &lt;u&gt;‘각 동작의 가치를 잘 판단하자’&lt;/u&gt;가 아니라 &lt;u&gt;‘가장 높은 가치를 지니는 동작을 취하게 하자’&lt;/u&gt; 라는 것이다. 그렇다면, 그냥 모델이 취할 모든 행동에 대한 가치 판단 없이 바로 모델의 현재 상태에서 바람직한 다음 동작을 예측하는 Policy, 그 자체를 학습하는 것이 낫지 않겠는가에 대한 근본적 의문이 생긴다. 심지어 동작이 속한 공간을 Discrete하게 만들 필요도 없다. 정책 모델이 내뱉은 확률 분포에 근거해서 다음 동작을 예측하고, 만약 이러한 흐름이 좋은 결과를 가져왔을때 보상을 주어 Policy 모델을 업데이트한다. 이러한 학습법에 대한 이론이 바로 &lt;u&gt;‘Policy Gradient’&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;강화학습에서는 다양한 시뮬레이션  환경을 가정한다. 데이터셋 전체를 objective function에 empirical하게 근사시키는 deep learning의 개념과 유사하게, 강화학습에서는 각 시뮬레이션 단계를 ‘Episode’라고 부르고 하나의 에피소드 내에서 시간의 흐름에 따라 강화학습을 구성하는 각 모델의 입출력이 나오게 된다. 결국 강화학습의 가장 큰 목적은 다양한 에피소드로부터 모델의 다양한 의사결정방식을 받고, 해당 모델이 &lt;u&gt;가장 이상적인 의사결정방식을 취했을 때&lt;/u&gt; 모든 에피소드로부터의 리워드는 가장 큰 기댓값을 가지게 된다. 흔히 정책은 $\theta$라는 매개변수를 가지는 함수 $\pi_\theta(\cdot)$로 주로 표현하게 된다. 이 정책 함수는 각 state $s$에서 수행할 수 있는 모든 액션 $a$에 대한 확률 분포를 추출할 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 우리는 특정 매개변수를 가지는 정책(Policy)모델이 특정 액션을 수행했을때 관측 가능한 입/출력 결과에 대해 최대의 reward를 얻을 수 있는 방향으로 모델을 학습하고자 하며, 이 방향이 곧 &lt;u&gt;policy gradient&lt;/u&gt;에 해당된다.&lt;/p&gt;

&lt;p&gt;사실 policy의 gradient를 계산하는 과정은 순탄치 않다. 그 이유는 reward function에 대한 gradient는 정책 함수($\pi_\theta$)가 어떤 액션을 수행하였는가 뿐만 아니라, 실제로 마르코프 프로세스에서 정책 함수에 의하여 불가피하게 조정된 현재 state $s$에 대한 확률 분포 또한 고려되어야하기 때문이다. 즉, 정책 함수에 추가로, 정책 함수로 하여금 지속적으로 변해온 state $s$의 확률 분포에도 $\theta$가 기여한 바가 있기 때문에 gradient를 직접 구하기 어렵다는 문제가 발생한다. 그러나 이를 단순히 무시할 수 있다는 이론이 바로 &lt;u&gt;policy gradient theorem&lt;/u&gt;이고 한 문장으로 다음과 같이 정리할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\text{보상 함수의 gradient는 정책 함수의 gradient에 비례한다.}
]
엄밀한 증명은 아래에 간단하게 요약해보겠다.&lt;/p&gt;

&lt;p&gt;[
\nabla_{\theta} J(\theta) = \nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \pi_{\theta}(a|s) &lt;br /&gt;
\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \pi_{\theta}(a|s)
]&lt;/p&gt;

&lt;p&gt;보상함수는 각 state에 머물 확률과, 그 확률에서의 모든 액션 수행에 대한 Value의 총합(혹은 기댓값)을 의미한다. 위의 식에서 &lt;u&gt;뒷쪽에 있는 state value function&lt;/u&gt;에 대한 gradient를 구하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \nabla_{\theta} V^{\pi}(s) &amp;amp;= \nabla_{\theta} \left( \sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s, a) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} Q^{\pi}(s, a) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \nabla_{\theta} \sum_{s’, r} P(s’, r | s, a) (r + V^{\pi}(s’)) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s’, r} P(s’, r | s, a) \nabla_{\theta} V^{\pi}(s’) \right) \newline    &amp;amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s, a) + \pi_{\theta}(a|s) \sum_{s’} P(s’ | s, a) \nabla_{\theta} V^{\pi}(s’) \right)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;우선, 전체 식에 대한 gradient는 product rule에 따라 분배된다. 이때 뒤쪽에 있는 $Q^\pi$는 &lt;u&gt;현재 가치와 미래 가치의 합&lt;/u&gt;으로 표현 가능하다.  이때 뒤에 발생하는 $P$는 Markov decision process(MDP)에 따른 확률 분포로 생각하면 되고, 이전 state가 $s$일때 모든 다음 state에 대한 확률을 정의할 수 있다. 이는 어떠한 정책 함수에 따른 결과가 아니기 때문에 $\theta$라는 변수와 독립이다. 따라서 뒤쪽 term에 있는 gradient는 시그마의 안쪽으로 들어갈 수 있게 되며, 최종적으로는 $P(s^\prime, r \vert s, a)$를 $r$에 대해 marginalize하면서 수식이 완성되는 구조다.이 수식에서 주목할 점은, &lt;u&gt;다음 state value function의 gradient가 현재 state value function에 recursive하게 들어간다는 사실&lt;/u&gt;이다. 그렇다면 뒤쪽에 들어있는 $\nabla_\theta V^\pi (s^\prime)$ 또한 다음 state인 $\nabla_\theta V^\pi (s^{\prime\prime})$의 recursive한 수식으로 표현된다.  이렇게 계속 unrolling(recursive하게 $s^\infty$까지 전개)한다고 생각해보자.&lt;/p&gt;

&lt;p&gt;아래의 식에서 $\rho$는 policy function $\pi$에 의해 $k$번의 step 이후 특정 state로 바뀔 확률을 간소화하여 표현한 식이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
    \nabla_{\theta} V^{\pi}(s) &amp;amp;= \phi(s) + \sum_{a} \pi_{\theta}(a|s) \sum_{s’} P(s’|s,a) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \sum_{a} \pi_{\theta}(a|s) P(s’|s,a) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \nabla_{\theta} V^{\pi}(s’) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s’) Q^{\pi}(s’, a) + \pi_{\theta}(a|s’) \sum_{s’’} P(s’‘|s’, a) \nabla_{\theta} V^{\pi}(s’’) \right) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \left( \phi(s’) + \sum_{s’’} \rho^{\pi}(s’ \to s’’, 1) \nabla_{\theta} V^{\pi}(s’’) \right) \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^{\pi}(s \to s’’, 2) \nabla_{\theta} V^{\pi}(s’’) \quad \newline
    &amp;amp;= \phi(s) + \sum_{s’} \rho^{\pi}(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^{\pi}(s \to s’’, 2) \phi(s’’) + \sum_{s’’’} \rho^{\pi}(s \to s’’’, 3) \nabla_{\theta} V^{\pi}(s’’’) \newline
    &amp;amp;= \dots \newline
    &amp;amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \to x, k) \phi(x)
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;우리는 현재 state에서 미래 state $x \in \mathcal{S}$로 가는 모든 MDP를 전개했다. 이제 전개된 식에 state의 초기 상태를 대입하고 여러 전개 과정을 거치면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \nabla_{\theta} J(\theta) &amp;amp;= \nabla_{\theta} V^{\pi}(s_0) \newline    &amp;amp;= \sum_s \sum_{k=0}^{\infty} \rho^{\pi}(s_0 \to s, k) \phi(s) \newline    &amp;amp;= \sum_s \eta(s) \phi(s) \newline    &amp;amp;= \left( \sum_s \eta(s) \right) \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &amp;amp;\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) \newline    &amp;amp;= \sum_s d^{\pi}(s) \sum_a \nabla_{\theta} \pi_{\theta} (a | s) Q^{\pi}(s, a)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이를 통해 &lt;u&gt;정책 함수의 gradient의 방향&lt;/u&gt;이 곧 &lt;u&gt;보상 함수의 gradient의 방향&lt;/u&gt;과 일치한다는 점을 알아낸 것이다.&lt;/p&gt;

&lt;h3 id=&quot;trpo-to-ppo&quot;&gt;TRPO to PPO&lt;/h3&gt;

&lt;p&gt;앞서 본 내용은 policy gradient에 대한 기본적인 내용이었다. 그러나 이 이후 사실 LLM에 적용되기까지 강화학습 분야에서 policy gradient에 대한 다양한 접근법이 있었으며, 지금부터 살펴볼 내용이 현존하는 딥시크-R1의 아이디어 근간이라고 할 수 있다. TRPO와 PPO 중 &lt;a href=&quot;https://arxiv.org/pdf/1707.06347&quot;&gt;PPO&lt;/a&gt;가 조금 더 중요한데(&lt;strong&gt;OpenAI에서 쓴 논문&lt;/strong&gt;), TRPO를 꼭 짚고 넘어가야하는 이유는 &lt;u&gt;TRPO&lt;/u&gt;(&lt;strong&gt;피터 아벨 연구소 논문&lt;/strong&gt;)가 가장 기본 틀이 되기 때문이라고 생각했기 때문이다. TRPO에는 수학적으로 어려운 내용이 포함된다. 가장 간단하게 설명하자면 TRPO는 정책 모델을 학습할 때 “신뢰 가능한 구간 내에서 업데이트한다”라는 개념이다. 그렇다면 대체 왜 신뢰 가능한 구간이 중요한 것일까?&lt;/p&gt;

&lt;p&gt;Policy gradient은 다음과 같이 진행된다. 초기 상태는 확률 분포 속에서 샘플링되었다고 가정하자. 예를 들어 우리가 걷기 시작하거나 뛰기 시작할때 항상 같은 자세에서 시작하지는 않는 것과 같다.  우리는 시시각각 지금 행동에 기반하여 다음 행동을 결정하고 이를 수행한다. 이러한 과정을 우리는 자연스럽게 처리하지만 로봇에서 시키는 경우를 생각해봐야 한다.&lt;/p&gt;

&lt;p&gt;로봇이 지금 현재 어떤 행동을 수행했을때, 그 행동이 가져오는 가치 그리고 리워드를 수치화하고 이를 누적해서 더해간다. 그리고 그 행동에 기인하는 Policy가 지속적으로 좋은 방향으로 학습된다면, 이론상 업데이트되는 정책에 따른 누적된 리워드는 계속 우상향할 것이다.&lt;/p&gt;

&lt;p&gt;Optimal solution에 수렴할 때까지 지속 학습을 진행한다고 가정해보자. 모든 state에서 평가된 가치가 0보다 크거나 같다면 결국 모든 미래가치가 0이 되어 더이상 발전할 수 없는 상황이 올 때의 Policy가  최적의 해가 될 것이다. 하지만 업데이트된 policy($\tilde{\pi}$)는 모든 다음 state, action($s, a$)에 따른 advantage($Q^\pi$)를 0보다 같거나 크게 만든다는 보장이 없다. 이를 다소 단순화하기 위한 방법으로 학습하고자 하는 Objective는 다음과 같이 표현된다.&lt;/p&gt;

&lt;p&gt;[
L(\tilde{\pi}) = \eta (\pi) + \sum_s d^\pi (s) \sum_a \tilde{\pi}(a \vert s) Q^\pi(s, a).
]&lt;/p&gt;

&lt;p&gt;이제는 state에 대한 density 변화($d^{\tilde{\pi}}$)는 무시할 수 있다. 좌측 함수와 우측 함수의 초기값(파라미터 : $\theta_0$)이 동일하기 때문에 Locally 아주 작은 변화량 $\Delta \pi$에 대해서 gradient를 같은 방향으로 유지할 수 있지만, step size를 키울 수 없다는 문제가 생긴다. 만약 gradient가 이상적으로 생기지 않았다면, step size를 잘못 지정해주면 학습이 불안정해질 수 있다. 실제로 많은 강화학습에서 가장 큰 문제가 이러한 학습 불안정성에서 기인하며, 이를 해결하기 위해 수많은 방법론이 제안되었다.&lt;/p&gt;

&lt;p&gt;이를 해결하는 방법은 새로운 policy를 기존의 policy와 $\alpha$ mixing하게 되면 lower bound를 가진다. 근데 이 $\alpha$값도 매 task마다 새롭게 정의하기 힘드니, $\pi$와 $\tilde{\pi}$ 간의 거리 메트릭으로 정하자는 것 ($\alpha =D_{\text{metric}}(\pi \vert \tilde{\pi})$ )이 솔루션이 된다. 해당 논문에서는 KL divergence를 확률 분포상의 거리 메트릭 기준으로 진행하였다. 이때의 가장 큰 문제점은 $\alpha$에 대한 constraints를 가지는 $L$의 최대화 수식(Objective)에서, 최적의 $\theta$값을 찾기 위해 Approximation을 진행하고, 이를 위해 &lt;u&gt;Hessian(2차 미분)을 수행해야한다는 점&lt;/u&gt;이다. 이를 우회해서 풀 수 있는 방법으로 &lt;u&gt;Fisher Information Matrix&lt;/u&gt;가 있는데, Gradient의 공분산을 Empirical하게 평균내면 Hessian에 근사할 수 있는 방법이다. TRPO에서는 $k=10$ 정도에서 타협을 했고, 이를 통해 Policy gradient의 안정적인 강화학습을 제안했던 논문이다.&lt;/p&gt;

&lt;p&gt;PPO에서는 TRPO의 이런 타협점을 개선할 수 있는 방향을 제시한다. TRPO에서 constraint인 KL divergence에 적용되던 2차 미분(이를 surrogate objective로 표현한다)대신 1차 미분에 근사할 수 있는 방법을 찾고자 했다. PPO에서 적용한 방법은 기존의 surrogate objective에서 old/new policy가 크게 벗어나는 지점을 비율로 조절하게 된다($1-\epsilon$, $1+\epsilon$). 이러한 개념은 TRPO에서의 constraints의 $2^{nd}$ order differentiation에서 $\pi \simeq \tilde{\pi}$ 인 지점을 찾고자 하는 주목적을 Hessian에서 단순 clipping으로 간소화시켰다고 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;사실 PPO/TRPO에 쓰이는 Q-function은 Advantage function $A$로 사용하는데, 이에 대한 보다 자세한 내용은 다음 섹터인 GRPO에서 언급하며 시작하도록 하겠다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/8f1076b1-f46f-41b4-ba02-76701a30c5f8&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;grpo&quot;&gt;GRPO&lt;/h3&gt;

&lt;p&gt;이전에 올렸던 글 중 챗지피티는 어떤 식으로 학습되었을까에 대해 간단하게 소개했던 내용 중, RLHF와 PPO에 대해 간단하게 넘어갔던 부분이 있다. 딥시크는 PPO가 아닌 GRPO라는 자체적으로 연구한 방법을 통해 강화학습을 수행하였고, 강화학습을 통해 학습된 가장 고성능의 모델을 여러 작은 모델에 distillation하는 과정을 수행하였다. GRPO는 &lt;a href=&quot;https://arxiv.org/pdf/2402.03300&quot;&gt;“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”&lt;/a&gt;라는 논문에서 제안된 방법인 듯하다. 강화학습 논문이라기보다 &lt;u&gt;Deepseek에서 Language model의 수학적 추론 능력을 강화하기 위한 방법론&lt;/u&gt;으로 제안하였다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{J}_{PPO}(\theta) = \mathbb{E} \left( q \sim P(Q), o \sim \pi_{\theta_{\text{old}}} (O | q) \right) \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left( \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\theta_{\text{old}}} (o_t | q, o_{&amp;lt;t})} A_t, \text{clip} \left( \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\theta_{\text{old}}} (o_t | q, o_{&amp;lt;t})}, 1 - \epsilon, 1 + \epsilon \right) A_t \right).
]&lt;/p&gt;

&lt;p&gt;LLM에 위와 같은 PPO가 적용되며 다음과 같은 문제점이 생긴다. 우선 갑자기 등장한 수식에 맛있게 먹었던 점심이 다시 올라올 것 같기 때문에 천천히 보고 넘어가면 다음과 같다. 앞서 주구장창 언급했기 때문에 policy model에 대한 notation은 어느 정도 유추가 된다. 업데이트되기 전 파라미터가 $\theta_\text{old}$이고 업데이트된 파라미터는 $\theta$이다. $q, o$는 question dataset으로부터 샘플링된 질문, old policy로부터의 output을 의미한다(성능 비교의 베이스라인이 되는 LLM이 내뱉는 답변). 그리고 PPO에서의 clipping을 통해 학습 안정화하는 과정 또한 동일하다. 앞서 단순하게 넘어갔던 Advantage가 여기서 등장하는데, &lt;u&gt;기존의 Advantage function estimator들이 가지고 있는 문제점&lt;/u&gt;(특히 state의 변화 갯수 $k$에 따른 bias/variance trade-off, $k$가  크면 variance가 커지고, $k$를 줄이게 되면 bias가 커지는 문제)를 보다 일반화하는 방법으로 소개된 Generalized Advantage Estimator(GAE)을 짚고자 한다. 우선 시작하기 전, estimator에 대한 정의부터 알아야한다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \hat{A}_t^{(1)} &amp;amp;= r_t + \gamma V(s_{t+1}) - V(s_t) \newline    \hat{A}_t^{(2)} &amp;amp;= r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t) \newline    &amp;amp;\quad \vdots = \vdots \newline    \hat{A}_t^{(\infty)} &amp;amp;= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots - V(s_t)\end{aligned}
]&lt;/p&gt;

&lt;p&gt;현재의 policy가 앞으로의 state 변화에 끼칠 영향력을 생각한다. Value를 판단할 function $V$ 또한 학습 가능한 하나의 뉴럴 네트워크이다. 이렇듯 policy 모델과 함께 value 평가 모델이 함께 학습되는 구조를 &lt;u&gt;actor-critic model&lt;/u&gt;이라 부른다.그런데 이때, time step+1인 시점과 time step+$k$인 시점의 가치는 서로 다르다. 당연하게도 만약 discounting value $\gamma$가 적용되지 않는다면 estimator가 보는 시점이 길어지면 길어질수록 그만큼 보다 가까운 다음 state에 대한 미래 가치를 상실하게 되고, 결국 &lt;u&gt;policy 및 value에 대한 공정 평가가 어렵기 때문&lt;/u&gt;이다. 하지만 이러한 Advantage Function에 문제가 있다.&lt;/p&gt;

&lt;p&gt;작은 $k$값을 갖는 추정량 $A_t^{(k)}$는 분산이 낮지만 편향이 크고, 큰 $k$값을 갖는 경우 편향이 낮지만 분산이 크다는 점이다. 이는 &lt;u&gt;항의 개수를 보면 직관화가 가능&lt;/u&gt;하다. 합산해야 할 항이 적을 경우(state의 변화가 많지 않기 때문에) 분산이 낮아지지만, 상대적으로 $r_k$에 대한 정확한 정보를 활용하지 않기 때문에 편향이 상대적으로 커진다. 합산해야 할 양이 큰 경우는 반대로 생각하면 된다. 또한, $V(s_t)$ 가 추정 클래스 내에서는 상수로 생각될 수 있기 때문에, 추정량 간 차이는 오직  $k-step$ return에서만 발생하게 된다(길게 표현했지만 trade-off가 존재한다는 사실).&lt;/p&gt;

&lt;p&gt;그렇기 때문에 GAE는 &lt;u&gt;추정량 집합 전체에 대해 싸그리 어셈블하는 전략을 사용&lt;/u&gt;하였다. 각 estimator에 존재하는 trade-off를 &lt;u&gt;$\lambda$로 조절하겠다는 생각&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}    \hat{A}_t^{GAE(\gamma, \lambda)}    &amp;amp;= (1 - \lambda) \left( \hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V + \lambda (\delta_t^V + \gamma \delta_{t+1}^V) + \lambda^2 (\delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V) + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V (1 + \lambda + \lambda^2 + \cdots) + \gamma \delta_{t+1}^V (\lambda + \lambda^2 + \cdots) + \cdots \right) \newline    &amp;amp;= (1 - \lambda) \left( \delta_t^V \frac{1}{1 - \lambda} + \gamma \delta_{t+1}^V \frac{\lambda}{1 - \lambda} + \cdots \right) \newline    &amp;amp;= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V\end{aligned}
]&lt;/p&gt;

&lt;p&gt;바로 이 개념이 GAE이고, PPO 또한 이를 적용한 모델이라고 볼 수 있다. 여기서의 문제점을 드러내면 다음과 같다. Value model은 Policy model이 학습됨에 따라 내뱉는 &lt;u&gt;output에 대한 성능 평가를 진행&lt;/u&gt;하는데, 이때 Policy model과 별개로 학습이 진행되어있어야한다는 점이다.  PPO에서는 reward model에 과적합되는 문제를 직접적으로 피하기 위해 &lt;u&gt;KL penalty&lt;/u&gt;를 도입하게 된다. Reference model은 주로 초기 언어 모델이나 SFT(Supervised Fine-tuned) 모델로 사용하게 되는데, 학습되는 정책 모델이 내뱉는 output이 SFT 모델과 지나치게 달라지지 않도록 각 토큰 단위에서 제약을 가하게 되어 생성되는 텍스트의 &lt;u&gt;각 토큰이 기준 모델에서 기대되는 분포에서 크게 벗어나지 않도록&lt;/u&gt; 한다.&lt;/p&gt;

&lt;p&gt;[
r_t = r_{\varphi} (q, o_{\leq t}) - \beta \log \frac{\pi_{\theta} (o_t | q, o_{&amp;lt;t})}{\pi_{\text{ref}} (o_t | q, o_{&amp;lt;t})}
]&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/75729eb9-95d7-48ed-ae11-9967c26881fb&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그런데 실제 GRPO 논문의 그림을 보면 살짝 이해하기 어려운 부분이 있을텐데, 바로 KL divergence의 방향성이다. 그런데 실질적으로 KL penalty가 적용되는 구조는 동일하다. PPO 기반으로 동작하는 것은 거의 유사하지만, 바뀐 점은 다음과 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b742ff69-ada0-4368-8f4c-52a5a5641833&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO와 달리 GRPO는 더이상 Value Model을 사용하지 않는다. 그렇기 때문에 메모리를 절약할 수 있다.&lt;/li&gt;
  &lt;li&gt;그러나 Value Model을 사용할 수 없다는 것은 GAE를 사용할 수 없다는 뜻이고, 결국 기존에 연산되던 advantage function $A$를 새롭게 정의해야한다.&lt;/li&gt;
  &lt;li&gt;두 정책 모델의 TR 내에서 강화학습을 진행하는 구조는 동일하다. 그러나 이번에는 동일 질문($q$)에 대해 여러 output $o_{1 \ldots G}$를 답변으로 추출한다.&lt;/li&gt;
  &lt;li&gt;그리고 이 각각의 답변에 대해 예측된 이득의 평균을 정책 강화 학습의 Objective로 사용할 것이다.&lt;/li&gt;
  &lt;li&gt;학습된 reward 모델로부터 각각의 output을 평가한다.&lt;/li&gt;
  &lt;li&gt;Reward model의 결과 ${r_1, r_2, \cdots, r_G}$를 그룹 전체의 reward로 normalize해서 사용한다. 이는 특정 문맥 $q/o$에 대한 bias/variance(GAE에서 문제가 된 부분)를 줄이기 위한 노력이다.&lt;/li&gt;
  &lt;li&gt;KL divergence는 기존의 식이 아닌 Unbiased Estimator를 사용하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;요약하자면, Value function을 없앴기 때문에 &lt;u&gt;불가능한 GAE 대신 Reward의 grouped output을 정규화&lt;/u&gt;하여 trade-off 효과를 상쇄하였으며, KL divergence 식이 바뀐 정도가 되겠다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left( q \sim P(Q), {o_i}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}} (O | q) \right) &lt;br /&gt;
\frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta} (o_i | q)}{\pi_{\theta_{\text{old}}} (o_i | q)} A_i,
\text{clip} \left( \frac{\pi_{\theta} (o_i | q)}{\pi_{\theta_{\text{old}}} (o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} (\pi{\theta} | \pi_{\text{ref}}) \right)
]&lt;/p&gt;

&lt;p&gt;여기서 쓰인 KL divergence 식은:&lt;/p&gt;

&lt;p&gt;[
\mathbb{D}_{KL} (\pi_{\theta} | \pi_{\text{ref}}) = 
    \frac{\pi_{\text{ref}} (o_i | q)}{\pi_{\theta} (o_i | q)} - \log \frac{\pi_{\text{ref}} (o_i | q)}{\pi_{\theta} (o_i | q)} - 1.
]&lt;/p&gt;

&lt;p&gt;이며 각각의 리워드는 다음과 같이 정규화하여 연산된다:&lt;/p&gt;

&lt;p&gt;[
A_i = \frac{r_i - \operatorname{mean} ({r_1, r_2, \dots, r_G})}{\operatorname{std} ({r_1, r_2, \dots, r_G})}.
]&lt;/p&gt;

&lt;h1 id=&quot;methods--approaches&quot;&gt;Methods / Approaches&lt;/h1&gt;

&lt;h3 id=&quot;deepseek-r1-zero의-학습법&quot;&gt;Deepseek-R1 Zero의 학습법&lt;/h3&gt;

&lt;p&gt;딥시크는 &lt;u&gt;DeepSeek-V3-Base를 베이스 모델&lt;/u&gt;로 활용, &lt;u&gt;GRPO를 가장 메인인 강화 학습법&lt;/u&gt;으로 적용한 연구이다. 그 중 가장 메인이 되는 학습법에는 크게 리워드 모델링과 학습 템플릿이 있다. 이외의 방법론은 직접 학습을 통해 demonstration 과정을 거쳤다. 딥시크의 가장 큰 주장은, LLM 베이스 모델을 강화학습 만으로도 o1 만큼의 코딩/수학적 능력까지 키울 수 있다는 사실로, 강화학습이 LLM 학습에 큰 도움이 된다는 사실을 입증한다. Deepseek-R1은 Ollama에서 경량화 버전을 바로 사용해볼 수 있는데, 이때의 output을 확인해보면 실제 답변을 하기 전 ‘생각하는 부분’이 추가된 것을 알 수 있다. 개인적으로 맥북 프로 (M3)를 사용중인데, 속도 면에서 deepseek-r1:14b 모델 정도의 distillation 버전 정도면 편하게 돌릴 수 있는 것 같다. Deepseek-R1:14B 버전에게 LLM system에 대해 질문한 예시는 다음과 같다. 물론 이렇게 사용한 모델은 R1-Zero는 아니다. 그냥 예시를 보여주기 위해 사용하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/7b855d6f-a6a5-4012-ac93-da700caed26c&quot; width=&quot;962&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;딥시크는 보는 바와 같이 추론 과정을 &amp;lt;think&amp;gt; ~ &amp;lt;/think&amp;gt; 태그로 넣고, 그 이후에 실제 답변을 내놓는 것을 볼 수 있다(근데 distill 버전을 쓰다보면 가끔 생각을 멈추기도 함). 이러한 답변 특성은 &lt;u&gt;학습 템플릿 구성&lt;/u&gt;에 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ec5f5886-63d1-4c95-b87e-a55f09830888&quot; width=&quot;962&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 o1이고 o3고 어떤 방식으로 추론해서 정답을 내놓는지는 모르겠지만, 이 &lt;u&gt;일련의 과정을 강화학습으로 자동 학습하게 하는 것이 주된 아이디어인 듯&lt;/u&gt;하다. 이에 따라 R1-Zero의 Reward modeling 또한 두 분류로 나뉘게 되었다. 리워드 모델을 따로 학습하여 사용할 수도 있지만, Deepseek-R1-Zero 경우 Rule-based method (blackbox가 아닌 평가가 예측 가능한 경우 사용)를 적용하였다. 주요 보상 유형은 두 가지로 구성된다. 개발하는 과정에서 결과 기반 또는 과정 기반 신경망 보상 모델은 적용하지 않았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;정확성 보상:&lt;/strong&gt; 정확성 보상 모델은 응답이 올바른지 평가한다. 예를 들어, 결정적인 결과를 가진 수학 문제의 경우, 모델이 특정한 형식으로 정답을 제시해야 하며, 이를 통해 신뢰할 수 있는 규칙 기반 검증이 가능하다. 마찬가지로, LeetCode (코딩) 문제의 경우, 사전에 정의된 테스트 케이스를 기반으로 컴파일러를 활용하여 피드백을 생성할 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;형식 보상:&lt;/strong&gt; 정확성 보상 모델 외에도, 형식 보상 모델을 적용하여 모델이 사고 과정을 &lt;think&gt; 및 &lt;/think&gt; 태그 사이에 넣도록 강제한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;흥미로운-점&quot;&gt;흥미로운 점&lt;/h3&gt;

&lt;p&gt;학습 도중 발견한 흥미로운 내용으로 &lt;u&gt;‘아하 모먼트’&lt;/u&gt;를 언급한다. 처음엔 잘못 본 줄 알았는데 정말 아하 모먼트였다. 학습 중간의 정책 모델에서 아하 모멘트가 등장하고 난 후, 사고하는 시간에 보다 시간을 할애하고(스스로 생성한 think가 보다 풍부해진다는 뜻 같음) 본인의 초반 접근법(아하 포인트 이전의 의식의 흐름)을 자체 평가하고 개선하는 과정이 추가되었다는 것이다 (아래 예시).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/bd7247df-f5e1-4ec0-8edb-b596f840e25c&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/5417bbb1-8163-45ed-b200-62a097b1f131&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;답변을 내놓는 LLM은 하나의 의사 결정을 하는 정책 모델이다. 이 정책 모델을 학습하기 위해 기준선인 reward를 세우고 템플릿에 맞춰 자체적으로 발전할 수 있게 했더니 o1의 성능을 넘었다는 연구가 되었다.&lt;/p&gt;

&lt;h3 id=&quot;deepseek-r1의-학습법&quot;&gt;Deepseek-R1의 학습법&lt;/h3&gt;

&lt;p&gt;흥미롭지만 일단 여기까지는 &lt;u&gt;수학문제나 코딩에 대한 부분&lt;/u&gt;이다. LLM은 보다 다양한 task에 적용되어야하는데, 다음 step으로 어떤 방법을 썼을까?
Deepseek-R1-Zero로부터 알아낸 결과는 템플릿과 강화 학습의 조합으로 충분히 좋은 추론 능력을 키울 수 있다는 점이었다. 그렇다면 다음과 같은 의문이 들게 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;강화학습 과정은 초반 학습이 불안정하다는 단점이 있다. 이때 만약 적은 양의 high-quality data(사전 학습 데이터)를 통해 수렴 속도를 줄일 수 있는가? (R1-Zero는 오로지 강화학습만으로 학습함)&lt;/li&gt;
  &lt;li&gt;수학이랑 코딩 능력 말고 일반화 능력이 강화됨과 동시에, 앞서 했던 것과 같이 CoT 과정이 보다 사용자 친화적이면서 명확해질 수 있는 방법이 있는가? (R1-Zero는 수학 문제랑 코딩만 함)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 해소하기 위해 Cold Start 방식을 사용하였다. RL을 통해 DeepSeek-V3-Base 모델을 처음부터 학습하는 것이 아닌, 어느 정도 길이의 CoT를 포함한 question/answer 쌍을 답변 데이터로 구성하고 이를 사용하여 fine-tuning한 모델을 시작 포인트로 삼는다. 해당 데이터는 기존 모델의 성능을 해치지 않고 사후의 RL 학습에 도움이 되어야하기 때문에 &lt;u&gt;최대한 깔끔하게 curating하는 과정&lt;/u&gt;을 거치며, 모집한 샘플은 대략 $8\times 10^5$개다.&lt;/p&gt;

&lt;p&gt;그러나 Cold Start로 미세 조정한 모델을 RL로 학습했을때 language mixing 문제가 발생하였는데, 이는 추론 과정이나 답변 과정에 하나의 언어만 포함되지 않고 여러 언어가 포함되는 경우(한글, 중국어, 영어 혼용 등등) 가 발생한다는 사실이다.&lt;/p&gt;

&lt;p&gt;실제로 DeepSeek 사용자들의 리뷰를 봤을 때 &lt;u&gt;한국어를 사용할 경우 종종 경량화 모델에서 언어가 섞여서 출몰한다는 경험담&lt;/u&gt;이 있었는데, 아마 이 문제를 완벽하게 해결하지는 못했나보다. 아무튼 이를 그나마 좀 해소하기 위해 선택한 방법은 RL training 과정에 추가 reward로 language consistency를 준다는 것이다. 해당 리워드 때문에 성능이 좀 저하는 되지만 그래도 일단 LLM이라면 알아듣게는 말하는게 선호도가 더 높다는 것이 DeepSeek의 판단.&lt;/p&gt;

&lt;h3 id=&quot;distilled-model&quot;&gt;Distilled Model&lt;/h3&gt;

&lt;p&gt;Distillation 과정을 통해 원본 모델에 비해 상대적으로 경량화된 버전을 내놓았다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/fd5e338c-0fd6-45a2-aae0-26433e959091&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;앞서 Cold Start 때문에 모아놨던 샘플을 똑같이 Llama, Qwen 미세 조정에 적용하고, 이후에는 RL 학습을 하지 않고 SFT 방식을 적용했다. 이유는 &lt;u&gt;아래와 같이 RL로 학습하는 것보다, R1 모델로부터 증여받는 것이 훨씬 이득이었기 때문&lt;/u&gt;이다. &lt;strong&gt;이래서 금수저 집안이 좋은가보다ㅠㅠ&lt;/strong&gt;. 그렇다해서 RL 성능이 너무 낮은 것은 또 아니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/884ae95d-3954-4928-818f-686ceb180165&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;근데 사실 LLM 써보면 느끼는 점인데 추론 문제에 대한 정량평가는 수치일 뿐 실제 사용했을때 유저 평가 지표가 좀 더 중요할 듯 하다.&lt;/p&gt;

&lt;h1 id=&quot;내-생각&quot;&gt;내 생각&lt;/h1&gt;
&lt;p&gt;DeepSeek는 오픈소스이다. OpenAI는 클로즈소스이다. 이게 가장 큰 장점이라고 할 순 있겠다. 그런데 메모리 사양이 딸려서 원본 모델을 직접 올려볼 수는 없지만 정말 필요하다면 서버를 확보하고 사용할 수는 있을 것 같다.&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Feb 2025 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/deepseekr1</link>
        <guid isPermaLink="true">http://localhost:4000/blog/deepseekr1</guid>
        
        <category>Deepseek</category>
        
        <category>Reinforcement Learning</category>
        
        <category>LLM</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</title>
        <description>&lt;h3 id=&quot;시작하기-전에-&quot;&gt;시작하기 전에 …&lt;/h3&gt;

&lt;p&gt;이전 글들을 올린 후 꽤나 많은 시간이 지났다. 처음으로 올렸던 글인 &lt;a href=&quot;https://6unoyunr.github.io/blog/lssl&quot;&gt;LSSL(Linear State-Space Layer)&lt;/a&gt;에서는 연속 시퀀스 데이터셋에 대해 딥러닝 모델이 효과적으로 latent space를 정의할 수 있는 구조의 발달 양상을 살펴보았다. 그 중 가장 주요한 키포인트가 되는 HiPPO 논문의 경우 임의의 길이를 가지는 시퀀스의 hidden state를 모델링할 수 있는 근거로 자리잡았고, 이후 LSSL은 레이어 개념으로 확장시켜 HiPPO 행렬의 학습을 통해 성능을 높일 수 있다는 가능성을 보여주게 된다. 그리고 학습이 진행됨에 따라 떨어질 수 있는 학습 안정성 및 수치 엄밀성을 확보하기 위해, 이에 추가로 이후 scalable(데이터 및 모델 확장 가능성)을 높이기 위해 제안된 &lt;a href=&quot;https://6unoyunr.github.io/blog/s4&quot;&gt;S4모델&lt;/a&gt;을 두번째 글로 다루었다. 이제 이러한 기존 SSM based modeling을 근거로 하여, 트랜스포머 모델의 불가피한 연산량 증대를 개선하고자 한 Mamba 모델을 소개한다.&lt;/p&gt;

&lt;h3 id=&quot;트랜스포머-모델의-큰-문제&quot;&gt;트랜스포머 모델의 큰 문제&lt;/h3&gt;

&lt;p&gt;최근 가장 많이 다루게 되는 딥러닝 모델, 혹은 foundation model은 아마 대부분 알고 있겠지만 Transformer에 해당된다. 기계 번역 분야에서 등장한 transformer 구조는 이후 Computer vision, NLP, Audio 등등 모달리티(데이터 형태)의 종류에 무관하게 활발하게 활용되었으며, 가장 중요한 특성인 model scalability(모델 크기와 데이터 크기가 증가함에 따라 성능도 같이 향상됨) 특성이 현존하는 모델링 중 가장 확연하게 드러난 모델이라고 할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/af6e2cbf-55b6-48c7-945f-41337bca9e23&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그러나 트랜스포머 모델에는 가장 큰 문제점이 있다. 그것은 바로 한정된 길이의 시퀀스를 (보통은 트랜스포머 모델은 토크나이저를 통해 문장을 토큰 단위로 분해하고, 이를 임베딩화하여 사용한다) 받아들이며 이를 병렬 연산(Attention)하기 위해 그만큼의 메모리를 소모하게 되고, 이는 결국 동시에 처리 가능한 데이터의 길이가 어느 정도 한정될 수 밖에 없는 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e40c97ca-5414-440b-9535-3bce74e32ec0&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;만약 짧은 문장 하나를 트랜스포머에 넣게 되면, 적당히 토크나이징하여 연산을 수행하면 되지만 논문과 같이 긴 줄글의 경우 한정된 연산량 때문에 이를 분리해서 넣게 된다. 가장 큰 문제는 트랜스포머 모델은 논문 제목에서도 볼 수 있듯이(Attention is All you Need) RNN모델과 같이 hidden state를 따로 학습하지 않다 보니, 모델에 들어간 시퀀스 내에서 모든 것을 수행하게끔 되어있다. 즉, 논문 전체를 쿼리로 하여 특정 질문에 대한 주요 부분들을 추출하거나 하는 과정에서는 충분한 연산량이 보장되지 않으면 좋은 성능을 보이지 않게 된다. 이러한 한계점은 곧 트랜스포머 모델의 어텐션 연산을 위한 효율화 작업으로 이어지게 된다.&lt;/p&gt;

&lt;h3 id=&quot;그럼-왜-트랜스포머-모델을-쓰는-건데&quot;&gt;그럼 왜 트랜스포머 모델을 쓰는 건데?&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/becd8521-f6fb-42e8-8eb7-c6ab4720d523&quot; width=&quot;550&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그럼 대체 왜 꼭 “트랜스포머”여야만 하는가??라고 한다면, 그간 연산 효율화를 위해 단순 어텐션을 벗어난 모델링인 &lt;strong&gt;linear attention,&lt;/strong&gt; &lt;strong&gt;gated convolution, RNN,&lt;/strong&gt;  &lt;strong&gt;structured state space models (SSMs)&lt;/strong&gt; 모두 연구되었지만 Language와 같은 데이터에 대해 트랜스포머의 어텐션보다 높은 성능을 보이지 못했다는 단순한 사실이다. 트랜스포머가 보여준 가능성에 대해 수많은 연구가 진행되었기 때문에 하드웨어 친화적인 알고리즘이나 다양한 학습법 등등 많은 연구가 진행되어 이미 상당히 높은 발전을 이루어낸 트랜스포머 시장에서 적당한 연산량으로 애매한 성능을 보이는 다른 모델이 주목받기가 힘든 상황이다. 특히나 최근 long range dependency/reasoning 에 집중했던 SSM의 경우에는 텍스트와 같이 오히려 정보 집약적인 task에 대해서 높은 성능을 보이지 못했다.&lt;/p&gt;

&lt;h3 id=&quot;mamba-너로-정했다&quot;&gt;Mamba, 너로 정했다&lt;/h3&gt;

&lt;p&gt;맘바는 SSM에 집중한다. SSM은 기본적으로 특정 시스템을 모델링하기 위한 구조로, 이 방법에 attention, RNN에서 사용되는 “gate” 개념을 “selection”으로 가져간 것이다. Attention은 사실 대단한 알고리즘은 아니고, 현재 토큰에 대해 참고가 가능한 시퀀스 내에서 집중할 부분과 무시할 부분을 구분하고, 이를 예측에 활용하는 것이다. 결국 SSM에서도 특정 정보만 선택적으로 활용하는 방법을 사용해볼 수 있는 것이다. 그러나 selective SSM은 일반적인 SSM에 비해 효율적 콘볼루션 연산 등 연산 효율화를 위한 장치를 전혀 사용하지 못하게 된다. 고로 가장 기본적인 연산 방식인 recurrent를 기본적으로 사용하게 되는데, 이때 하드웨어 친화적인 알고리즘을 고안하여 연산 비효율성을 보완하게 되는 것이다. 즉 어텐션과 MLP 없이, selective SSM과 이를 효율적으로 연산할 수 있는 방법을 더하여 Mamba를 만들어낸 것이다. 그래서 사실 맘바 논문의 가장 주요한 키포인트는 SSM에 있다기 보다는 FlashAttention과 비슷한 맥락인 GPU 활용력에 있는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;selection-메커니즘&quot;&gt;Selection 메커니즘&lt;/h3&gt;

&lt;p&gt;이전의 SSM에서 다뤄지지 않은 내용은 “input”에 대해 선택적 알고리즘이 없다는 사실이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ee7a07d6-b890-4feb-9024-ceeedce2292e&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSM 모델링을 보게 되면 길이에 무관한 모델링을 할 수 있다는 장점이 있지만, 이를 다르게 해석하면 임의의 시점에 들어오는 입력은 모두 동일하게 모델링된 state를 보기 때문에 관찰 시점 이전의 input 혹은 이후의 input 중 일부 무시해야할 내용을 구분할 수 없다. SSM의 가장 큰 장점은 아무리 긴 길이의 입력이 들어오더라도, 모든 정보를 함축적으로 모델링할 수 있다는 사실이다. 그러나 위와 같은 예시를 보면 어텐션과의 차이점이 크게 드러난다. 예컨데 어떤 텍스트의 초반에 “고양이를 5년 전부터 키워왔다.”라는 정보가 있고, 중간에 그와 무관한 이야기인 “시골집에서 살던 내용”이 포함되어있고, 텍스트 후반부에 고양이 이름이 나와있는 경우를 생각해보자.&lt;/p&gt;

&lt;p&gt;해당 쿼리에 대해 “OOO는 몇살 정도로 예상되는가?”라는 질문을 받는다고 가정하면 어텐션 모델은 우선적으로 해당 질문과 가장 큰 연관성을 지니는 “고양이 이름은 OOO이다.” 라는 내용과 “대략 5년 전부터 고양이를 키웠다.”라는 내용을 참고하겠지만, SSM은 중간에 있는 시골집에 살던 내용까지 전부 참고하여 정답을 내놓게될 것이다. 물론 SSM이 잘못된 대답을 내놓지 않고 정답을 내놓을 수 있지만, 결국 말하고자 하는 것은 이처럼 정보가 집약적인 데이터(특정 부분에 집중해야 제대로 된 QnA가 가능한 데이터 구조)에 대해서는 어텐션 만큼 효율적이고 정확한 방법이 없다는 것이다. 따라서 맘바에서는 기존 SSM 구조에 추가로 입력 신호에 대해 SSM 파라미터(이전의 정보들)를 다변화하는 구조를 통해 Selection 매커니즘을 추가하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;하드웨어-친화적-알고리즘&quot;&gt;하드웨어 친화적 알고리즘&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/2498d478-ee60-486a-9068-5607b4365db5&quot; width=&quot;750&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이전의 모든 SSM을 위한 효율화 알고리즘은 selective SSM에는 적용되지 않는다. 이는 LTI(Invariant)와 같은 모델링 구조에서 기본적으로 입력 및 시간에 무관하게 시스템은 동일하다는 가정을 가지고 있기 때문이다. 따라서 기존 논문에서 제안된 콘볼루션 기반 방법들을 모두 사용할 수 없게 되었고, 오로지 recurrent 연산을 사용할 수 밖에 없으므로, 이를 하드웨어 친화적으로 연산하는 방법을 고안하게 된다. 실제로 구현된 하드웨어 친화적인 알고리즘은 시퀀스 길이에 대해 Linear한 복잡도를 가지게 되어, 이전 콘볼루션 기반 알고리즘이 pseudo-linearity를 가졌던 것에 비해 recurrent 연산을 더욱 효과적으로 수행할 수 있게 되었다.&lt;/p&gt;

&lt;h3 id=&quot;모델링&quot;&gt;모델링&lt;/h3&gt;

&lt;p&gt;모델 구조는 간단하다. Selective SSM 구조를 하나의 모듈처럼 취급하여, 기존 트랜스포머 모델을 구성하는 MLP 파트를(Attention 및 Projection 등등) SSM 모듈로 갈아끼워서 사용한다. Selective SSM이 아닌 일반적인 SSM에 대한 내용은 이전에 다루었던 글들을 통해 간단하게 이해하고 오면 좋다. 간단하게 소개하자면 대개 Structured SSM은 4개의 파라미터$(\Delta, A, B, C)$를 기본으로 적용되며, 이를 discretize한 ($\overline{A}, \overline{B}, C$)을 사용한다. SSM은 LTI 시스템을 기반으로 하여 시간에 따른 시스템 함수 불변성을 가정하였으나, 이러한 불변성이 필연적으로 가지는 한계 때문에 맘바에서는 기존에 적용될 수 있었던 연산 효율성을 포기하고 Selective SSM을 채택하게 된다. 이 부분에서는 어떠한 파트가 구체화되어 Selective SSM이 설계되었는지 단계별로 정리하고자 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Compression(축약)을 위한 Selection&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;시퀀스를 다루는 모든 모델링은 임의의 길이를 가지는 ‘문맥’을 어떻게 하면 작은 크기의 ‘hidden state’ 혹은 ‘latent’로 함축하는가?를 다루게 된다. 모든 시퀀스 모델링은 해당 관점에서의 trade-off를 고려할 수 밖에 없는데, 대표적인 시퀀스 모델링에 해당되는 ‘트랜스포머(Transformer)’는 문맥을 전혀 압축/함축하지 않는다는 특징을 가지고 있다. 이러한 특징은 autoregressive한 추론 단계에서 Key-Value 문맥 전체를 참조하기 위해 길이에 따라 연산 속도 및 메모리가 증가하게 되며, 이는 트랜스포머의 quadratic-time consuming의 주된 원인으로 작용한다. 반대로 RNN과 같은 recurrent model은 한정된 크기의 state를 가진다는 점에서 학습 효율성을 가지나, 과연 한정된 크기의 state에 얼만큼 context(문맥)이 잘 요약될 수 있는가가 문제점으로 작용된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/39368a70-a907-4870-8e8e-03b901b12e00&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 그림에 나타난 생성 task를 보게 되면 이러한 trade-off system을 SSM(LTI system)의 레벨 단에서 이해하기 쉽다.&lt;/p&gt;

&lt;p&gt;좌측에 나타난 task는 입력으로 들어온 연속 신호 중 일부분(연속되어 색칠된 부분)을 복사하여 생성하는 과정이다. LTI system이 처리할 수 있는 가장 기본적인 형태의 delay라고 볼 수 있다. 즉 LTI system으로 매핑 가능한 일반적인 모델로 간단하게 수행할 수 있는 과제이다.&lt;/p&gt;

&lt;p&gt;우측에 나타난 두 가지의 task 중 위쪽은 입력으로 들어온 연속 신호 중 관련 신호(색칠된 부분)과 무관한 신호(흰색 부분)을 구분하고, 관련 신호를 입력 순서대로 복사하여 생성하는 과제이다. 앞선 복사 task처럼 LTI system으로 수행될 수 없기에 time-varying system 및 non-linear system이 활용되어야하는 것을 볼 수 있다. 아래쪽은 Induction heads라는 과제로, 흔히 요즘 LLM에서의 In-context learning에서 대두되는 task라고 볼 수 있다. 입력으로 넣어준 일련의 신호에 대해 맥락을 파악하고, 이후에 특정 신호(검정색 토큰)를 입력으로 넣어줬을때 문맥에 맞는 정답을 내놓게되는 것이다(파란색 토큰). 이 역시 입력 신호에 대해 어떤 특정 신호가 뒤따를지 모르기 때문에 시스템이 문맥에 대한 추론이 필수적이고, 이를 위한 모델링을 추가로 수행해야한다.&lt;/p&gt;

&lt;p&gt;결국 위의 그림으로 이해하고자 한 것은 여러 복잡한 생성 이론을 효과적으로 수행하기 위해 “선택적으로” 문맥을 이해하는 과정을 모델링에 추가해야 한다는 사실은 시퀀스를 처리하는 모든 모델링이 다루는 문제라는 사실이다. 해당 문제를 수행하기 위해 기존 방법론들을 총정리했을때, trade-off로서 context 용량과 효율성 간의 합의점이 필요하고, 현재 다루는 모델에서 이를 어떻게 적용해낼지(Attention으로 일부 특징들을 걸러낼 것인지, Recurrent module로 문맥을 요약한 state를 구축할 것인지) 고민하게 된다. 그렇기 때문에 SSM에서도 비슷한 맥락으로의 구조가 필요하고, 기존 시퀀스 모델에서 개별적으로 적용되던 context compression의 수단으로 selection mechanism을 넣은 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SSM에 selection을 넣기&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/fd17a1e6-a66b-4e27-a041-44061794b346&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 Mamba는 SSM에 어떻게 selection mechanism을 심느냐는 것인데, 저자는 RNN의 recurrent dynamics나 CNN의 파라미터와 같이 직접적으로 입력에 영향을 주는 “파라미터”를 입력 신호에 따라 바꾸는 방식을 생각해냈다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/aac32a1f-45c9-452e-961a-6e58a352c9fd&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;좌측과 우측을 비교하게되면 그 차이가 나타난다. 주된 차이는 $B, C, \Delta$ 파라미터가 &lt;strong&gt;더이상 입력(배치) 및 각 입력의 타이밍(길이)에 무관하지 않고&lt;/strong&gt;, 입력 및 출력 신호와 동일한 크기를 가지는 것을 볼 수 있고, 이는 더이상 Time-invariant system이 아닌 Time-variant system이 되었다는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;또한 $B, C. \Delta$가 어떻게 정해지는지 우측을 잘 보게되면 $s_B(x), s_C(x), s_\Delta(x)$와 같은 방식으로 입력 신호에 대한 함수로 표현이 되어있는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
s_B(x) = \text{Linear}_{N}(x),~s_C(x) = \text{Linear}_{N}(x)
]&lt;/p&gt;

&lt;p&gt;이는 가장 간단한 형태로, dimension $D$인 입력을 받아 $N$인 출력을 해주는 Linear module로 parameterize하여 함수를 구성하고,&lt;/p&gt;

&lt;p&gt;[
s_\Delta(x) = \text{Broadcast}_D(\text{Linear}_1(x))
]&lt;/p&gt;

&lt;p&gt;이산 신호의 간격은 위와 같이 스칼라 값을 dimension에 브로드캐스팅하는 형태로 함수를 구성하였다. 이렇게 SSM을 시간 변화에 따른 함수로 파라미터화 하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;하드웨어 친화적 알고리즘&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Convolution 모델이라던지, Attention 모델들은 하드웨어 친화적으로 설계가 되었다. 콘볼루션 커널은 입력 크기와 무관하게 항상 일정한 receptive field 크기를 가져 메모리를 최적화할 수 있으며, 어텐션은 길이에 따라 메모리 및 시간이 증가하기는 하지만 HBM 대신 &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;SRAM에서 잘 동작할 수 있는 알고리즘&lt;/a&gt;이 등장했으니까 (실제로 FlashAttention 저자인 Tri Dao가 맘바 저자로 참여했음). Selective SSM도 비록 LTI system을 사용할 수 없게 되어버렸지만 분명 학습 효율화할 수 있는 부분은 있을 것이다. 기존 방법들의 한계점은 다음과 같다.&lt;/p&gt;

&lt;p&gt;(1) SSM과 같은 recurrent model은 표현력(state size)과 속도 사이의 합의점이 필요하다. 높은 표현력을 가지면서도 속도 저하가 심하지 않은 방법을 찾는 것이 목적.&lt;/p&gt;

&lt;p&gt;(2) Recurrent가 Convolution보다 더 유연하다. 후자가 전자의 확장판이기 때문에 latent state 구축을 위한 연산량이 (B, L, D, N) 만큼 필요한데, 이러한 문제를 해결하려는 방법이 나옴 (&lt;a href=&quot;https://arxiv.org/pdf/2111.00396&quot;&gt;S4 모델&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;(3) 기존의 LTI state model은 표현력 확보를 위한 state dimension $N$의 넉넉한 확보를 위해 dual recurrent-convolutional form을 고안함.&lt;/p&gt;

&lt;p&gt;우리는 이제 selection mechanism을 적용하기 때문에 LTI system을 사용할 수 없다. LTI system이 가지는 한계점을 해결하기 위해 Selective SSM을 고안하였으나 연산 비효율성 문제를 해결해야한다는 점에 직면하게 된다. 저자는 문제를 해결하기 전 두가지 중요한 특징을 활용한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent를 단순하게 적용할 경우 FLOPs는 $O(BLDN)$, Convolution은 $O(BLD\log(L))$으로 적용된다. 즉 시퀀스 길이가 길어질수록 적당한 크기의 hidden state dimension $N$에 대해 오히려 Recurrent 연산이 적은 연산량을 가진다.&lt;/li&gt;
  &lt;li&gt;두가지 주된 문제는 recurrent 연산의 순차성과(병렬적 연산이 안됨)과 큰 메모리 사용 문제에 직면한다. 후자의 경우에는 convolution과 같이 굳이 전체 state $h$를 구성하지 않아도 된다는 개선점이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결국 주된 아이디어는 엄청 특별한 내용은 아니고, hidden state $h$를 GPU에서 효율적으로 연산할 수 있는 방법들(kernel fusion, parallel scan, recomputation)로 빠르게 구해보자는 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ddf090fe-b5ec-44ca-8519-b3cc027c0298&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSM의 시스템 주축이 되는 $\overline{A}, \overline{B}$를 직접 HBM에서 계산하지 않고, SSM parameter $A, B, C, \Delta$를 SRAM단으로 로드, 이산화 작업을 거져 다시 HBM에 쓰는 방식을 취한다. 또한 순차성 부분은 스캔할 타이밍에서 parallel scan algorithm을 적용하게 된다. 이로써 적은 메모리 bandwidth를 가지는 SRAM과의 데이터 송수신 관련 코스트를 최소화하여 사용한다. 이외의 backpropagation 시의 recomputation 방식은 FlashAttention과 하드웨어적으로 동일하게 적용된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Neural Network에 Mamba 섞기&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Structured SSM(S4)와 마찬가지로 Selective SSM(Mamba) 또한 시퀀스에 대한 변환 모듈에 해당되기 때문에 neural network에 적용할 수 있다. 맘바의 구조를 종합적으로 이해하기 위해서는 H3와 Gated Unit을 이해하는 과정이 필요하다. 속칭 ‘배고픈 하마 (&lt;a href=&quot;https://arxiv.org/pdf/2212.14052&quot;&gt;Hungry Hungry Hippos&lt;/a&gt;)’라 불리는 H3의 경우 트랜스포머의 Attention Algorithm의 효과를 따라갈 수 있는 SSM 구조 모델링을 위해 Shifting SSM과 Recalling SSM을 구별하고, 이를 multiplication으로 엮는 시도를 하게 된다. 이렇게 모델링하게 되면 Q, K, V로 추출되는 입력에 대한 정보가 Shifting SSM에서 이전 입력을 참조하기 위해 옮겨주는 역할을 수행하고, 만약 현재 입력 정보가 기억이 된다면(Shifting $\odot$ SSM), 그 이후 입력에 대한 출력값(Value $\odot$ SSM)을 응답으로 내놓는 구조가 된다. Gated MLP의 경우에도 결국 트랜스포머의 Attention 구조를 MLP 구조에 통합하고자 한 구조에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/aeb62259-cc7e-4c50-aa37-b88ae37d7830&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;즉, 맘바의 경우에도 기본적으로 SSM 구조를 사용하기 때문에 트랜스포머의 Attention 효과를 활용하고자 했던 H3와 근본적으로 문제시하는 부분이 동일하다. 그렇기 때문에 내부적으로 연산되는 SSM 부분은 H3와 동일하다. 그러나 차이가 있는 점은 H3는 Linear Attention의 Q, K, V 구조를 활용하였지만, Mamba에서는 이러한 어텐션 구조를 전혀 사용하지 않고 Gated MLP를 2개의 SSM 시스템을 Wrapping하는 방식으로 구조화하였다.&lt;/p&gt;

&lt;h3 id=&quot;각-요소별-효과&quot;&gt;각 요소별 효과&lt;/h3&gt;

&lt;p&gt;이와 같이 모델링했다. 이때의 각 요소별 효과를 간략하게 서술하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable Spacing : 언어와 같이 Discrete data에 대해서 문맥 해석에 무관한 신호를 무시할 수 있게 된다. 문맥을 생성하는 상황에서 문맥에 무관한 신호를 제외하여($g_t = 0$) 보다 풍부한 문맥을 생성해낼 수 있다.&lt;/li&gt;
  &lt;li&gt;Filtering Context : 각 상황에서 문맥의 중요도를 결정한다. 경우에 따라 일부 문맥을 무시해야할 경우가 생기는데, 시간 불변성을 지니는 LTI system에서는 이를 적용할 수 없으나, selective SSM인 맘바에서는 상황에 따라 문맥을 필터링할 수 있게 된다.&lt;/li&gt;
  &lt;li&gt;Transformer는 구조상 문맥상에서 독립적인 문구를 어텐션하여 사용할 수 있지만(필요한 부분을 불연속적으로 추출 가능) LTI system에서는 문맥을 하나의 연속적인 형태로 보기 때문에 이러한 특성이 발견되지 않는다. 그러나 Selective SSM에서는 연속된 시간 단위에서의 $\Delta$, 혹은 이전 hidden state를 무시할 수 있는 gate $g_t$의 값이 유동적으로 학습될 수 있기 때문에 이러한 특성을 찾을 수 있다는 가능성이 있다.&lt;/li&gt;
  &lt;li&gt;일반적으로 문맥의 간격에 해당되는 $\Delta$는 현재 입력에 얼마나 집중할 지 결정해주게 된다. 바로 위의 꼭지에서 언급했던 것과 같이 $\Delta \rightarrow \infty$가 되는(커지는) 상황이 되면 이전의 state를 무시하고 현재의 입력에 집중하는 형태가 될 것이고, 반대로 작아지는 경우에는 이전의 state를 현재 입력보다 중요시하는 형태가 될 것이다.&lt;/li&gt;
  &lt;li&gt;$A$ 파라미터는 기존의 시스템에서는 hidden state를 구축하는 역할을 수행했었다. 마찬가지로 Selective SSM에서도 같은 역할을 수행하지만, 차이점은 유동적인 $\Delta$와 discretization되어 구축되는 문맥 시스템에 selective 속성을 부여할 수 있다는 것이다. (아래 수식 참고)&lt;/li&gt;
  &lt;li&gt;$B, C$ 파라미터는 gated system에서 현재 입력 $x_t$에 대한 정보를 문맥에 추가할 것인지, output $y_t$를 내보내는 과정에서 state 정보를 얼마나 활용할 것인지 결정하는 역할을 수행한다. (아래 수식 참고)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{h_t}^\prime = A\mathbf{h_{t-1}} + B\mathbf{x_t} \newline
\mathbf{y_t} = C\mathbf{h_t}+D\mathbf{x_t}
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/635b9e1c-33ec-47f9-b3d3-c527634e14d8&quot; width=&quot;350&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ada87c6d-2b88-4445-ace4-4c03105df758&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Selective copying(좌측) 그리고 Induction head(우측) 각각의 성능이 기존 SSM baseline에 비해 월등히 좋아지는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/179d4ba4-44de-4970-b535-a9f8afb3b005&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 확연히 좋아지는 부분은 Perplexity인데, 연산량이 늘어날수록(모델의 파라미터 수가 증가할수록) 문맥에 대한 생성 능력이 확연히 올라간 모습을 보여준다. 이전까지는 H3까지도 어텐션에 필적하지 못했던 부분이었는데, 맘바를 통해 꽤나 많이 따라잡은 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/22158474-be9b-4fb9-bba9-f3508a124be5&quot; width=&quot;850&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;여러 downstream task에 대해 zero-shot 성능을 확인하였다. 파라미터 수가 증가할수록 perplexity는 감소하고 average는 증가하였고, 다소 적은 파라미터 수를 가지고도 좋은 성능을 보인다. 이외에 DNA, Audio modeling등 다른 시계열 모달리티에 대해서도 좋은 성능을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/dd47e367-9432-483c-9c5e-24b2b1e38a56&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Mamba의 장점 중 가장 주요한 포인트는 context 길이가 길어질 경우에 연산량 및 추론 시간을 줄일 수 있다는 점인데, 실제로 Attention을 효율화한 FlashAttention과 비교했을 때에도 Mamba의 inference time 및 throughput이 좋아지는 것을 볼 수 있다.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Sep 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/mamba</link>
        <guid isPermaLink="true">http://localhost:4000/blog/mamba</guid>
        
        <category>Mamba</category>
        
        <category>Selective SSM</category>
        
        <category>H3</category>
        
        <category>Gated MLP</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</title>
        <description>&lt;h3 id=&quot;시작하기-전에&quot;&gt;시작하기 전에,&lt;/h3&gt;

&lt;p&gt;HiPPO는 SSM 구조에서 Long-term range를 구축하기 위한 matrix $A$ 구조의 중요성을 언급하였고, LSSL에서는 SSM을 연속적으로 존재하는 $A$ 전부에 대해 이를 일반화하였다. 이전 글에서 Mamba modeling의 기초가 되는 LSSL에 대해서 설명했었고, 해당 글에 간단한 수식과 관련된 증명을 첨부했었다. 솔직한 심정을 담아 말해보자면, 아직 본인은 이러한 기본 내용들을 전부 이해하지 못했다고 생각하고 있고 이 글을 통해서 맘바를 이해하고자 하는 것은 너무 돌아가는 과정이라고 생각되기도 한다. 자신감 없이 말한 감이 없지 않아 있지만 결국  맘바 모델링 자체를 이해하는데 있어서 state-space modeling을 완전히 뼛속부터 이해할 필요는 없다고 생각한다 (Bottom-up 보다는 Top-down이 맞는 방향이라는 개인적인 의견).&lt;/p&gt;

&lt;p&gt;그리고 여러 블로그에 보면 시각화와 함께 맘바 모델링을 한큐에 이해할 수 있게 쉽게 정리해둔 글도 많이 보인다. 그럼에도 불구하고 굳이 글을 길게 써서 리뷰했던 이유는 앞선 글에서 말했던 것처럼 맘바의 근본적인 내용에 대해 이해해보려 노력하는 과정이 의미가 있다고 생각했기 때문이다. 맘바를 간단하게만 이해한다면 맘바가 도대체 왜 transformer 구조가 가지는 문제들을 해결할 수 있었는지, 그리고 단순히 새로운 아키텍쳐로서 등장했다고 해서 무조건 좋다고 생각해야하는 것은 아니기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;무엇이든 쉽게 얻으면 그만큼 쉽게 잃는 법이니까.&lt;/em&gt; 분명이 맘바가 가지는 특징을 제대로 이해할 수 있다면 맘바가 가지는 근본적인 장단점을 발견할 수 있을 것이고, 이를 통해 향후 연구 및 모델링 개발의 기반이 될 것이다. 이번 글에서는 &lt;strong&gt;LSSL가 가지는 연산량과 연산 불안정성을 해결하고자 한&lt;/strong&gt; &lt;strong&gt;S4 모델링&lt;/strong&gt;에 대해서 정리해보도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;다시-시퀀스-모델링으로-돌아가서&quot;&gt;다시 시퀀스 모델링으로 돌아가서,&lt;/h3&gt;

&lt;p&gt;시퀀스 모델링에서 가장 중요한 것은 적당한 길이의 시퀀스를 얼마나 효율적으로 처리하는가와 시퀀스의 길이가 길어질수록 참조할 수 있는 문맥의 길이도 그에 따라 길어져야한다는 것이다. 트랜스포머는 연산량을 희생하여 단일 연산으로 전체 시퀀스에 대한 어텐션 정보를 획득할 수 있고, 토큰 임베딩의 길이를 늘임으로써 이를 해결할 수 있었으나 결국 연산량의 한계가 있다는 문제가 있다. RNN 및 CNN 각각이 가지는 특징들도 있지만 모든 모델링은 각자가 가지는 장단점 때문에 만능일 수는 없었고, 이에 대한 대응으로 SSM(state-space modeling)을 제안한 것이 바로 LMU, HiPPO를 비롯한 논문들이었던 것이다.&lt;/p&gt;

&lt;h3 id=&quot;그래서-ssm을-정리하자면&quot;&gt;그래서 SSM을 정리하자면,&lt;/h3&gt;

&lt;p&gt;SSM은 한마디로 Linear Time-Invariant System으로 latent space를 구축하고자 한 것이다. LTI system의 미분 방정식을 구성하는 matrix가 시간 불변성을 가진다는 특징은 결국 연속 신호를 이산화한 관측 단위에서 미분 방정식은 동일한 식으로 구축되며, 따라서 시퀀스 길이에 무관하게 동일한 latent space $x(t)$를 만들어낼 수 있다는 것이다. gate에 의존하는 RNN과는 다르게 &lt;strong&gt;특정 구조를 가지는&lt;/strong&gt; Matrix $A$가 있다면 실제 딥러닝 모델의 예측에 가장 중요한 특징 벡터인 latent를 long range dependent하게 뽑아낼 수 있게 된다. 트랜스포머로 각 토큰 단위로 어텐션을 구하든, RNN 구조로 연속으로 들어오는 데이터로 implicit latent를 만들든 그런 방법들이 아니라 실제로 시간 불변성이 성립하는 latent space를 예측해낼 수 있다면 아무리 오랜 시간이 흘러도 (관측 범위가 처음과 크게 벗어나도) hidden space는 동일한 함수로 구현될 것이기 때문에 Long Range Dependency를 보장할 수 있다는 것이다. 따라서 어텐션 연산에 대해 고정된 문맥 길이를 가지는 Transformer 구조에 비해 상대적으로 더 긴 길이의 시퀀스 데이터를 처리할 수 있고 이론상으로는 무한한 길이의 인풋을 감당 가능하다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/bfe4b2cd-4798-4b20-94ea-029f2a3a2d13&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;논문에 나와있는 그림을 통해 이해해보도록 하자. 우선 좌측의 “Continuous State Space” 부분을 확인해보면, SSM 구조에서 인풋에 해당하는 $u(t)$가 행렬 $A, B, C, D$로 구성된 Linear system의 상태(이를 hidden state, 혹은 latent state $x(t)$라 부른다)를 통해 모델링된 출력값 $y(t)$를 내게 된다. 이때 $x(t)$가 지속적으로 변화하는 input to output $\mathbf{u} \rightarrow \mathbf{y}$를 저장하는 메모리 역할을 수행하게 된다면, 이론상 불변하는(고정된 요소를 가지는) 행렬 $A, B, C, D$에 대해 꾸준이 이전 정보를 저장할 수 있게 된다. 바로 이것이 중앙에 보이는 “Long-Range Dependencies”에 해당되고, 실제로 이에 대한 구조화된 행렬의 효과성을 입증한 것이 “&lt;strong&gt;HiPPO: Recurrent Memory with Optimal Polynomial Projections”&lt;/strong&gt; 논문에 해당된다.&lt;/p&gt;

&lt;p&gt;이때 기본적으로 SSM은 “Recurrent system”을 가지게 되는데, 이는 시스템의 구조가 입력에 대한 출력 구조로 이어져있으며, 이전 입력 대비 출력 결과에 따른 시스템 변화가 이후 입출력에 영향을 미치게 된다는 것이다. 이를 연산하는 방식으로는 귀납적으로(Recurrent) 연산 후 연산을 하는 방식으로도 구현이 가능하지만 단순화하여 콘볼루션 연산으로 수행하는 것도 가능하다. 그러나 여전히 고차원의 데이터에 대해 필연적으로 증가하는 행렬 연산($e.g.$ $A, B, C, D$ 행렬이나 $\bar{K}$) 때문에 &lt;strong&gt;연산량이 높다는 문제&lt;/strong&gt;가 생긴다.&lt;/p&gt;

&lt;h3 id=&quot;잘-구조화된-ssm을-사용하면-되지-않을까-s4&quot;&gt;“잘” 구조화된 SSM을 사용하면 되지 않을까?? (S4)&lt;/h3&gt;

&lt;p&gt;따라서 지금 글에서 다루고자 하는 논문인 “&lt;strong&gt;Efficiently Modeling Long Sequences with Structured State Spaces&lt;/strong&gt;” (S4)에서의 목적은 다음과 같다. 기존에 LSSL(단순 SSM)의 높은 연산량을 해결하기 위한 방법들이 제안되었지만, 모두 연산상에 numerical stability(연산 안정성 혹은 연산 엄밀성)이 부족했다. 그렇기 때문에 연산 안정성도 높임과 동시에 기존에 존재하던 “잘” 구조화된 행렬에 적용 가능한 알고리즘들을 활용하기 위해 SSM의 토대가 되는 matrix $A$를 &lt;strong&gt;다시 구조화하는 전략을 사용&lt;/strong&gt;하였다. 바로 원래 $A$의 rank보다 훨씬 낮은 rank(서로 독립인 column의 갯수를 의미하며, 낮은 rank를 가지는 matrix는 독립이 아닌 column을 모두 배제할 경우 그만큼 차원 수를 줄여서 표현 가능하다)를 가지는 요소와 normal matrix(특수한 케이스의 정사각 행렬로, commutable한 특징이나 diagonal 요소로 분리가 가능하다는 등등의 특징을 사용하여 non-normal matrix에 비해 빠르게 연산이 가능하다) 요소로 분리가 가능하다는 점을 사용한다.&lt;/p&gt;

&lt;p&gt;또한 기존의 SSM이 coefficient space(latent를 표현하는 함수는 사전 정의된 여러 orthogonal한 함수들의 coefficient 가중합으로 표현하고자 했었다.)로 접근하는 방식을 사용했다면, 이번에는 주파수 차원으로 올려서 계산하게 된다. 시간 단위에서의 콘볼루션은 주파수 단위에서의 곱연산으로 표현된다.&lt;/p&gt;

&lt;p&gt;이를 통해 &lt;strong&gt;Low-Rank 행렬은 Woodbury identity로&lt;/strong&gt;, &lt;strong&gt;Normal 행렬은 Cauchy kernel로&lt;/strong&gt; 교정 가능하며 이를 토대로 연산량을 $O(N^2L)$에서 $\tilde{O}(N+L)$로, 메모리는 $O(NL)$에서 $O(N+L)$로 줄일 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;기존-ssm과-표현-방식&quot;&gt;기존 SSM과 표현 방식&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;긴 문맥을 보존할 수 있게끔 모델링&lt;/strong&gt;된 matrix $A$ (e$.g.$ HiPPO 행렬)을 사용한다. 그렇게 되면 다음과 같은 연립 미분 방정식으로 표현되는 시스템을 구축할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{x}^\prime = A\mathbf{x} + B\mathbf{u} \newline
\mathbf{y} = C\mathbf{x}+D\mathbf{u}
\end{aligned},\quad A^{\text{HiPPO}}_{n, k}= -\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
n+1,&amp;amp;\text{if }n = k \newline
0,&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;이때 일반적인 컴퓨팅 시스템에서는 Continuous system을 Discrete(이산화된 입력)으로 변화하는 과정이 필요하다. 이를 적용한 식이 실제 SSM에서 적용될 Discrete SSM이다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_k = \overline{A}x_{k-1} + \overline{B}u_k\quad &amp;amp;\overline{A} = (I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A)  \newline
y_k = \overline{C}x_k,\quad &amp;amp;\overline{B} = (1-\Delta/2\cdot A)^{-1}\Delta B \newline
&amp;amp;C = \overline{C}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이에 대한 증명이나 보다 자세한 내용은 &lt;a href=&quot;https://6unoyunr.github.io/blog/lssl&quot;&gt;이전 게시글인 LSSL&lt;/a&gt;을 보고 오면 좋다. 아무튼 Discrete SSM이 의미하는 바는 SSM이 결국 recurrent 연산 구조를 가지기 때문에 &lt;strong&gt;RNN&lt;/strong&gt;의 연산 특징을 가진다는 것. 이산화된 Matrix $\overline{A}$가 hidden state $x$의 transition matrix 역할을 수행한다. 헌데, 위의 식에 대한 hidden state와 output을 $x_{-1} = 0$라 가정한 후에 전개하면, convolution kernel에 대한 연산으로도 표현이 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_0 =&amp;amp; \overline{B}u_0,\quad x_1 = \overline{AB}u_0 + \overline{B}u_1,\quad x_2 = \overline{A}^2\overline{B}u_0+\overline{A}\overline{B}u_1 +\overline{B}u_2,~\cdots \newline
y_0 =&amp;amp; \overline{CB}u_0,\quad x_1 = \overline{CAB}u_0 + \overline{CB}u_1,\quad x_2 = \overline{CA}^2\overline{B}u_0+\overline{CA}\overline{B}u_1 +\overline{CB}u_2,~\cdots \newline
y_k =&amp;amp; \overline{K} \ast \mathbf{u},\quad \overline{K}\in\mathbb{R}^L := \mathcal{K}_L(\overline{A}, \overline{B}, \overline{C}) := (\overline{C}\overline{A}^i\overline{B})_{0\le i &amp;lt; L}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;그러므로 만약 convolutional filter $\overline{K}$에 대해 알고 있다는 가정을  한다면 FFT(빠른 콘볼루션 연산 알고리즘)을 통해 연산 속도를 개선할 수 있지만, 이 필터를 연산하는 과정 자체도 행렬곱이 필요하며 non-normal matrix $\overline{A}$에 대해 일반화된 연산을 하기에는 어려움이 따르게 된다. 즉, 이 논문에서 하고자 하는 것은 해당 필터를 어떻게 효율적으로 연산하는가에 대한 부분이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;방법론--diagonalization-대각화&quot;&gt;방법론 : Diagonalization (대각화)&lt;/h3&gt;

&lt;p&gt;행렬의 대각화 (Diagonalization)는 행렬의 고유값(eigenvalue)인 $\lambda$와 고유벡터(eigenvalue)를 활용하여 대상이 되는 행렬을 고유값이 대각선 성분인 행렬로 만드는 과정이다. 예컨데 대각화(Diagonalization)이 가능한 행렬 $A \in \mathbb{R^{n \times n}}$가 존재한다면, 이 행렬의 eigenvalue $n$개와 이에 대응되는 eigenvector $n$에 대해서 $\Lambda = V^{-1}AV$로 표현 가능하다. 이때, $\Lambda$와 $V$는 각각 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\text{For eigenvalues }\{ \lambda_i\}_{i=1}^n \text{ and eigenvectors } \{v_i\}_{i=1}^n,\quad\Lambda = \begin{bmatrix}
\lambda_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \newline
0 &amp;amp; \lambda_2 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \newline
0 &amp;amp; 0 &amp;amp; 0&amp;amp; \lambda_{n-1} &amp;amp; 0 \newline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \lambda_n 
\end{bmatrix},\quad V = \begin{bmatrix}
\mid &amp;amp; \mid &amp;amp; \cdots &amp;amp; \mid \newline
v_1 &amp;amp; v_2 &amp;amp; \cdots &amp;amp; v_n \newline
\mid &amp;amp; \mid &amp;amp; \cdots &amp;amp; \mid \newline
\end{bmatrix}
]&lt;/p&gt;

&lt;p&gt;이런 상황에서, 기존의 식을 조금 바꿔보면 다음과 같이 정리할 수 있다. 우선, 일반적으로 SSM에서 $D = 0$으로 간소화하여 사용하기 때문에 $\mathbf{y} = C\mathbf{x}$로 표현하도록 하겠다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\mathbf{x}^\prime =&amp;amp; A\mathbf{x} + B\mathbf{u} \newline
\mathbf{y} =&amp;amp; C\mathbf{x}
\end{aligned}\quad\rightarrow\quad
\begin{aligned}
\tilde{\mathbf{x}}^\prime =&amp;amp; V^{-1}AV\tilde{\mathbf{x}} + V^{-1}B\mathbf{u} \newline
\mathbf{y} =&amp;amp; CV\tilde{\mathbf{x}}
\end{aligned} 
]&lt;/p&gt;

&lt;p&gt;두 식이 서로 조금 달라보이지만, 우측과 같이 전개된 시스템의 좌측에 전부 $V$를 곱하게 되면, $x = V\tilde{x}$인 SSM과 동치인 것을 알 수 있다. 즉 입력 대비 출력으로 이어지는 관계성 $\mathbf{u} \rightarrow \mathbf{y}$은 동일한 SSM 시스템이고, 이때의 system latent는 행렬 $A$의 eigenvector matrix $V$에 의해 바뀌게 된다. 위의 식처럼 새로운 시스템에서의 $A$행렬에 해당되는 대각 행렬 $V^{-1}AV$을 구축할 수만 있다면, 앞서 보았던 콘볼루션에서 $A$의 곱연산의 연산량을 효과적으로 줄일 수 있다. (이를 &lt;strong&gt;Vandermonde product&lt;/strong&gt;라 한다. 그냥 그렇다고 생각하고 넘어가자)&lt;/p&gt;

&lt;p&gt;그러나 유감이지만, &lt;strong&gt;HiPPO 행렬에 대한 대각화는 진행할 수 없다&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1e6b6bfb-bed8-4ab7-b16d-b1ed95a3dd12&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그 이유는 슬프게도 HiPPO 행렬이 대각화되면서 고유벡터로 이루어진 matrix $V$의 성분이 &lt;strong&gt;너무 커지기 때문&lt;/strong&gt;이다. 쉽게 말하면, 컴퓨터 상에서 연산이 안정적이려면(실제 수학적 계산값과 일치하려면), 행렬 연산 과정에서 행렬 요소가 너무 큰 값을 가지면 안되기 때문에 불가능하다는 것이다. 앞서 보여준 HiPPO 행렬 $A$를 대각화하면 요소가 $V_{ij} = {i+j \choose i-j}$가 되는데 (combination), state size $N$이 커지면 커질수록 최대 $2^{4N/3}$까지 커지는 요소를 감당할 수 없다 ($e.g.$ 출력값인 $CV\tilde{\mathbf{x}}$ 연산에 문제가 생길 수 있다.).&lt;/p&gt;

&lt;h3 id=&quot;방법론--normal--low-rank-matrix&quot;&gt;방법론 : Normal + Low Rank Matrix&lt;/h3&gt;

&lt;p&gt;위에서 다룬 것은 대각화를 통해 연산의 용이성을 올려보자!라는 내용이지만, 결국 기본적인 HiPPO 행렬에 대해서 적용하기는 힘들고 추가 작업이 필요하다는 것을 암시한다. 가장 이상적인 조건은 대각화의 대상이 되는 행렬 $A$가 unitary matrix와 같이 특수 행렬로 대각화가 가능한 경우에 해당된다. 선형 대수에서 이러한 조건이 만족하는 행렬 $A$의 모음을 &lt;strong&gt;“normal matrices”&lt;/strong&gt;라 부른다. 눈치챘을 수도 있지만 당연히 HiPPO matrix는 normal matrix가 아니고, 그렇기 때문에 대각화할 때 &lt;strong&gt;고유벡터 요소가 커지는 문제&lt;/strong&gt;가 발생한다.&lt;/p&gt;

&lt;p&gt;다행이지만 저자는 HiPPO matrix $A$는 normal matrix가 아니지만, 해당 행렬을 normal matrix와 low rank matrix의 합으로 나타낼 수 있음을 발견하였다. 하지만 문제는 콘볼루션 필터 연산 시 합(Normal + Low Rank)에 대한 제곱 연산이 필요한데, 이 역시 시간이 오래 걸리고 최적화가 필요한 부분이라는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;[
\overline{K}\in\mathbb{R}^L := \mathcal{K}_L(A, B, C) := (CA^iB)_{0\le i &amp;lt; L}
]&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 &lt;strong&gt;세 가지&lt;/strong&gt; 알고리즘을 추가로 적용하여 커널 필터를 계산하게 된다. 각 알고리즘에 대한 내용을 아래 그림과 관련지어 정리하면 다음과 같다. 아직 디테일하게 설명한 부분이 없어 그림의 내용에 대한 이해는 못하지만 순차적으로 보도록 하자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/31d32528-7810-4b1d-8f2c-5bcbab8f192a&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 알고리즘을 따라가기 위해서는 행렬 $A$가 NPLR (Normal + Low-Rank) 혹은 DPLR (Diagonal + Low-Rank)로 표현이 가능함을 이해하고 넘어가야한다. 이 부분은 논문에 Appendix C.1. 파트를 보면 empirical하게 모든 HiPPO 행렬에 대해 입증해 놓았다. 사실 수식으로의 이해는 필요가 없는 부분이고, &lt;strong&gt;그냥 받아들이면 되는 파트&lt;/strong&gt;다.&lt;/p&gt;

&lt;p&gt;그런 뒤, 기존의 콘볼루션 커널을 계산하던 방식에서 차이를 두게 된다. $\overline{K}$를 직접 계산하지 않고  $\overline{K}$의 Discrete Fourier Transform(DFT) 변환 형태인 $\hat{K}$룰 사용한다. DFT는 이산화된 시간 축에서의 신호를 (여기서는 필터의 요소인 $CB, CAB, CA^2B, \cdots$ 를 연속된 시간 축에서의 신호로 생각하면 된다) 이산화된 주파수 축으로의 스펙트럼으로 바꿔주는 변환에 해당되고, DFT 변환 및 이의 역변환 IDFT 알고리즘은 Fast Fourier Transform (FFT)라 하며 연산 속도는 $O(L\log L)$ 선에서 해결 가능하다.&lt;/p&gt;

&lt;p&gt;[
\hat{K}_j = \sum_{k=0}^{L-1} \overline{K}_k\exp(-2\pi j\frac{k}{L})
]&lt;/p&gt;

&lt;p&gt;뒤에서 더 자세히 정리하겠지만, Truncate SSM을 구성하여 &lt;strong&gt;스펙트럼 단위에서 연산&lt;/strong&gt;하게 되면 필터 연산 시 $A$를 여러 번 곱하는 방식에서 벗어나 &lt;strong&gt;한번의 행렬 연산으로도 연산이 가능&lt;/strong&gt;하게 된다. 이 과정에서 골칫거리인 term인 $(1-\overline{A}^L)$가 발생하는데 (결국 powering이 필요), 이를 &lt;strong&gt;reparameterization&lt;/strong&gt;을 통해 반복된 연산을 피해 메모리 절약 및 속도 향상을 할 수 있게 된다. 그리고 위에서 가정한 구조화를 통해 스펙트럼 커널 $\hat{K}$를 연산하는 것이 “Cauchy kernel”과 동일함을 알 수 있고, 효율적인 알고리즘을 적용할 수 있다. 짧게 정리했지만 실제 순서대로 간단하게 표현하면 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;모든 HiPPO 행렬 $A$를 &lt;strong&gt;NPLR (DPLR)로 표현이 가능&lt;/strong&gt;하고, 이를 적용하여 $A \rightarrow \overline{A}$ (Discretize)를 $O(N)$ 연산으로 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;$\overline{K}$의 truncate SSM generating function은 DFT랑 동일하다. 따라서 &lt;strong&gt;주파수 축으로의 변환 및 역변환을 통해 연산 가능&lt;/strong&gt;하며, 이때 $\overline{A}$의 반복된 제곱 연산 대신 단일 연산으로 바꿀 수 있다.&lt;/li&gt;
  &lt;li&gt;위의 연산 과정이 &lt;strong&gt;Cauchy kernel 연산 구조와 동일&lt;/strong&gt;하므로 &lt;strong&gt;효율적인 알고리즘이 적용 가능&lt;/strong&gt;하다.&lt;/li&gt;
  &lt;li&gt;이때 inverse는 &lt;strong&gt;Woodbury’s Identity를 사용하면 간소화가 가능&lt;/strong&gt;하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;방법론에-대한-보다-자세한-설명&quot;&gt;방법론에 대한 보다 자세한 설명&lt;/h3&gt;

&lt;p&gt;앞서 소개한 효율화 과정에 대해서 자세하게 살펴보자. 개인적으로는 디퓨전 논문보다 어려운 것 같다. 그렇지만 힘내보자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/7f837568-1b56-4c87-b0ec-614dd27631bf&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;HiPPO 행렬은 모두 “NPLR (Normal Plus Low-Rank)”로 표현 가능하다. HiPPO 행렬은 총 4가지가 존재하지만, 가장 보편적인 케이스인 LegS에 대해서만 살펴보면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
A^{\text{HiPPO}} _{n, k}= -\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
n+1,&amp;amp;\text{if }n = k \newline
0,&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;요 식 모든 요소에 $\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2}$를 더해주게 되면 다음과 같은 식이 된다.&lt;/p&gt;

&lt;p&gt;[
-\begin{cases}
\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if }n &amp;gt; k \newline
\frac{1}{2},&amp;amp;\text{if }n = k \newline
-\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&amp;amp;\text{if } n &amp;lt; k
\end{cases}.
]&lt;/p&gt;

&lt;p&gt;이 식의 대각 성분을 따로 분리하게 되면 $-\frac{1}{2}I + S$로 표현 가능하고, 이때 $S$는 normal matrix에 속하는 &lt;strong&gt;skew-symmetric matrix&lt;/strong&gt;가 된다. 또한 원래의 행렬의 모든 요소에 같은 값을 더하는 행렬은 rank가 $1$인 행렬이다. 모든 HiPPO 행렬에 대한 증명은 논문을 참고하면 되고, 이를 통해 암시할 수 있는 사실은 모든 HiPPO state 행렬을 Diagonal part + Low-Rank part로 분리 가능하다는 사실이다. 이를 다음과 같이 표현하도록 하겠다. 논문에서는 증명 과정에서 conjugate($\ast$) 표시를 사용했는데, 본인은 이 기호가 조금 익숙치 않아서 &lt;strong&gt;transpose 기호($\top$)로 대체하여 사용&lt;/strong&gt;하도록 하겠다.&lt;/p&gt;

&lt;p&gt;[
A = \Lambda - PQ^\top
]&lt;/p&gt;

&lt;p&gt;이제 이렇게 대체된 식으로 discrete system matrix를 표현해보면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\overline{A} &amp;amp;= (I-\Delta / 2 \cdot A)^{-1} (I+\Delta / 2 \cdot A) \newline
\overline{B} &amp;amp;= (1-\Delta / 2 \cdot A)^{-1} \Delta B \newline \newline
I + \Delta/2\cdot A &amp;amp;= I + \Delta / 2 \cdot\left(\Lambda - PQ^\top\right) \newline
&amp;amp;= \frac{\Delta}{2} \left(\frac{2}{\Delta}I + \Lambda - PQ^\top\right) \newline
&amp;amp;= \frac{\Delta}{2}A_0 \newline \newline
(I-\Delta/2 \cdot A)^{-1} &amp;amp;= \frac{2}{\Delta}\left(\frac{2}{\Delta}-\Lambda+PQ^\top\right)^{-1} \newline
&amp;amp;= \frac{2}{\Delta}\left(D-DP(I+Q^\top DP)^{-1}Q^\top D \right) \newline
&amp;amp;= \frac{2}{\Delta}A_1, \text{ where } D = \left(\frac{2}{\Delta}-\Lambda\right)^{-1} \text{ (Diagonal term)}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;$A_0$을 계산하는 것은 크게 문제가 없는데 $A_1$ 연산에는 큰 문제가 있는데, 바로 역행렬 연산이다. 행렬 차원 수 $N$이 증가할수록 연산량이 기하급수적으로 늘어난다 . 따라서 역행렬 연산을 DPLR 행렬에 대해 위와 같이 &lt;strong&gt;Woodbury’s Identity를 통해 단순화&lt;/strong&gt;할 수 있다. 대각화된 행렬에 대한 inverse는 쉽게 구할 수 있으며, 뒤에 붙는 low-rank term에는 무관하게 연산이 가능하므로 전체 계산식에 대한 역행렬 연산보다 단순화할 수 있다는 것이다. Woodbury’s Identity의 경우에는 앞으로 전개될 증명 과정에 계속 활용되기 때문에 계속 인지하고 있는 편이 용이하다 (DPLR 구조의 행렬만 가지면 계속 효율적으로 적용이 가능).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Woodbury’s Identity&lt;/strong&gt;는 다음과 같이 적용할 수 있다. &lt;strong&gt;교환 법칙이 성립하는 세 행렬&lt;/strong&gt; $A\in\mathcal{R}^{N \times N}, U, V \in \mathcal{R}^{N \times p}$에 대해 (여기서 $\mathcal{R}$은 commutative ring으로, 이에 속하는 원소들에 대해서는 곱연산에 대해 교환 법칙이 성립한다고 생각하면 된다.)&lt;/p&gt;

&lt;p&gt;[
(A+UV^\top)^{-1} = A^{-1}-A^{-1}U(I_p + V^\top A^{-1}U)^{-1}V^\top A.
]&lt;/p&gt;

&lt;p&gt;위의 식을 만족하게 된다.&lt;/p&gt;

&lt;p&gt;암튼 구한 식으로 다시 discrete system을 정의해보면, DPLR인 행렬 $A_1,$ $A_0$에 대해 $O(N)$의 연산량으로 해결 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_k =&amp;amp; A_1A_0x_{k-1} + 2A_{1}Bu_k \newline
y_k =&amp;amp; C^\top x_k
\end{aligned}.
]&lt;/p&gt;

&lt;p&gt;그리고 저자는 이 부분에서 row vector였던 $C$를 column vector로 간주하여 다른 파라미터($B, P, Q$)들과 shape을 맞추었기 때문에, 필자도 이에 따라 요 부분부터는 기존 시스템 식의 $C$를 $C^\top$으로 바꿔 표기하도록 하겠다.&lt;/p&gt;

&lt;p&gt;이제 이산화된 시스템 행렬은 얼추 알겠고, 사실 가장 중요한 것은 Recurrent system에서의 콘볼루션 필터 $\overline{K}$를 빠르게 연산하는 것이다. DPLR이 행렬의 이산화 과정에서 Woodbury’s Identity를 활용할 수 있게 되면서 효율성을 올려준다는 사실을 인지하였으나, 실제로 콘볼루션 필터를 연산하는 과정에서의 반복곱 연산에서는 큰 도움이 되지 않는다는 것을 알 수 있다. 반복곱 연산 대신, DPLR를 활용하기 위해서는 역행렬 연산이 필요하므로 스펙트럼 단위로 넘기는 (coefficients) generating function을 생각해볼 수 있다. 예컨데 무한한 길이의 콘볼루션 필터 신호가 있다고 가정해보자.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}(\overline{A}, \overline{B}, \overline{C}) = \{\overline{C}^\top\overline{B},\overline{C}^\top \overline{A}\overline{B},\overline{C}^\top \overline{A}^2\overline{B},\ldots\}
]&lt;/p&gt;

&lt;p&gt;우리가 현재 신호에 대해 가질 수 있는 것은 이상적인 콘볼루션 필터 중 $L$의 길이를 가진 한정된 길이의 필터이다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}_L(\overline{A}, \overline{B}, \overline{C}) = \{\overline{C}^\top\overline{B},\overline{C}^\top \overline{A}\overline{B},\overline{C}^\top \overline{A}^2\overline{B},\ldots,\overline{C}^\top \overline{A}^{L-1}\overline{B}\}
]&lt;/p&gt;

&lt;p&gt;길이가 $L$인 이산(discrete) 신호는 주파수 $2\pi\times\frac{0}{L} \sim 2\pi \times \frac{L-1}{L}$의 성분으로 분해가 가능하다. 이 주파수를 표현하는 unit $z$라는 변수로 표현한다면, 이를 $z$함수에 대한 coefficient의 집합으로 대체 가능하며 이를 $z$-transform이라고 부른다. 이때, 일반적으로 $z$는 복소수 단위(&lt;strong&gt;Real + Imag&lt;/strong&gt;)를 의미하며 주파수 단위에서는 이를 오일러 각도 변환 식인 ($e^{-i\Omega}$)에서 $\Omega =\{\frac{2\pi l}{L}\}_{l=0}^{L-1}$의 합으로 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\hat{K}_L(z; \overline{A}, \overline{B}, \overline{C}) := \sum_{i=0}^{L-1} \overline{C}^\top\overline{A}^i\overline{B}z^i = \overline{C}^\top(I-\overline{A}^L)(I-\overline{A}z)^{-1}\overline{B}
]&lt;/p&gt;

&lt;p&gt;이러한 변환을 DFT(Discrete Fourier Transform)이라 부르며, &lt;strong&gt;이산화된 시간 축의 신호를 이산화된 주파수 축으로 변환하는 과정&lt;/strong&gt;에 주로 활용된다. 맨 앞단의 $\overline{C}^\top(I-\overline{A}^L) = \tilde{C}$로 두게 되면,&lt;/p&gt;

&lt;p&gt;[
\hat{K}_L(z; \overline{A}, \overline{B}, \overline{C}) = \tilde{C} (1-\overline{A}z)^{-1}\overline{B}
]&lt;/p&gt;

&lt;p&gt;이 식에서 discretized matrix $\overline{A}, \overline{B}$ 의 closed form으로 대체하여 $A, B$에 대해 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\overline{A} =&amp;amp; (I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A) \newline
\overline{B} =&amp;amp; (1-\Delta/2\cdot A)^{-1}\Delta B
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이 식을 그대로 위에 대입하게 되면,&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\tilde{C}^(I-\overline{A}z)^{-1}\overline{B} =&amp;amp; \tilde{C}\left(I-(I-\Delta/2 \cdot A)^{-1} (I+\Delta/2\cdot A)z \right)^{-1}\overline{B} \newline
=&amp;amp; \tilde{C}\overline{B}\left(I-\frac{\Delta}{2}A\right)\left(\left(I-\frac{\Delta}{2}A\right)-\left(I+\frac{\Delta}{2}A\right)z\right)^{-1} \newline
=&amp;amp; \tilde{C} \Delta B\left( I(1-z) - \frac{\Delta}{2}A(1+z)\right) \newline
=&amp;amp; \frac{\Delta}{1-z}\tilde{C} \left( I - \frac{\Delta A}{2\frac{1-z}{1+z}} \right)^{-1}B \newline
=&amp;amp; \frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -A \right)^{-1}B
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;+앞서 우리는 시스템 행렬이 DPLR(Diagonal Plus Low-Rank)임을 보였기 때문에 다시 표현하게 되면,&lt;/p&gt;

&lt;p&gt;[
\frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -A \right)^{-1}B = \frac{2}{1+z}\tilde{C}\left(\frac{2}{\Delta}\frac{1-z}{1+z}I -\Lambda+PQ^\top  \right)^{-1}B
]&lt;/p&gt;

&lt;p&gt;이제 앞서 언급했던 Woodbury’s Identity를 사용해볼 수 있다. 식을 간소화하기 위해 다음과 같이 두개 되면,&lt;/p&gt;

&lt;p&gt;[
R(z) = \left( \frac{2}{\Delta}\frac{1-z}{1+z} - \Lambda \right)^{-1}
]&lt;/p&gt;

&lt;p&gt;최종적으로는 다음 식으로 전개할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\tilde{C}(1-\overline{A}z)^{-1}\overline{B} = \frac{2}{1+z}\left(\tilde{C}R(z)B - \tilde{C}R(z)P(1+Q^\top R(z)P)^{-1} Q^\top R(z)B \right).
]&lt;/p&gt;

&lt;p&gt;여기서 생길 수 있는 의문점은, 앞서 식을 전개하는 과정 상에서 앞단의 $\overline{C}^\top(I-\overline{A}^L) = \tilde{C}$로 정의된 부분이다. 원래대로라면 매번 $\overline{A}^L$를 연산해야 하지만, 단순히 학습 파라미터를 초기에 $\tilde{C}$로 초기화한 상태로 학습 가능하게끔 &lt;strong&gt;reparameterization&lt;/strong&gt;을 하게될 경우 이에 따른 연산 코스트를 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;이제 마지막 단계까지 왔다. 결국 전개한 식을 요약하자면, $\overline{K}$를 연산하는 부분을 generating function $\hat{K}_L(\Omega; \overline{A}, \overline{B}, \overline{C})$로 계산하고, 이때 $\overline{A}$가 Diagonal 성분을 가짐을 사용하여 풀어낸 식이다. 그런데 이렇게 풀어낸 식이 결국 Cauchy kernel이랑 정확하게 일치하고, Cauchy kernel은 효율적으로 연산할 수 있는 알고리즘이 존재한다. 우선 Cauchy matrix / kernel의 구조는 다음과 같이 정의된다.&lt;/p&gt;

&lt;p&gt;[
M \in \mathbb{C}^{M \times N} = M(\Omega, \Lambda) = (M_{ij})_{0&amp;lt;=i &amp;lt; M,~0&amp;lt;=j&amp;lt;N},\quad M_{ij} = \frac{1}{\omega_i - \lambda_j}
]&lt;/p&gt;

&lt;p&gt;최종 식에서의 $Q^\top R(\Omega, \Lambda)P$ 부분을 살펴보게 되면(기존의 $z$를 unit $\Omega$의 각 요소에 대해 생각해볼 수 있다), 이를 계산하는 과정이 &lt;strong&gt;Cauchy matrix-vector multiplication 연산량으로 계산 가능&lt;/strong&gt;하다는 것이다. 예컨데 원래대로라면 길이 $L$인 콘볼루션 커널을 총 $N$만큼의 hidden state에 대해 연산하려면 $O(LN)$ 만큼의 연산량이 소모되는데, 약간의 오차를 허용하면 이를 $O((L+N) \log(L+N) \log \frac{1}{\epsilon})$의 연산량으로 처리가 가능하다. 증명하는 과정은 거의 불필요한데 요약하자면, $Q^\top R(\Omega, \Lambda)P$를 계산하는 것은 $\Omega$에 속하는 모든 $\omega$에 대해 우리는 $\sum_{j} \frac{q_j^\top p_j}{\omega-\lambda_j}$를 구하고자 하는 것과 같으며, 결국 이 식은 Cauchy kernel의 형태와 같기 때문이다 ($\omega \neq \lambda_j$ 라는 조건은 항상 성립함).&lt;/p&gt;

&lt;p&gt;이렇게 알고리즘이 모두 정리가 되었다. 다시 앞서 간단하게 언급했던 알고리즘을 다시 가져오면 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저, $A$가 DPLR이라는 점에서 시작하여 식을 전개하였고, 이때 $\overline{K}$를 다이렉트로 계산하지 않고 generating function $\hat{K}$를 계산하기 위해 푸리에 변환을 실시하였다. 이때 나온 식에서 $\overline{C}$를 reparameterization해준다. 그런 뒤, discretized된 모든 matrix를 state matrix의 closed form으로 표현한 뒤, Woodbury’s Identity를 통해 식을 다시 전개하면,&lt;/p&gt;

&lt;p&gt;[
\begin{bmatrix}\tilde{C}^\top &amp;amp; Q\end{bmatrix}^\top \left(\frac{2}{\Delta} \frac{1-\omega}{1+\omega} -\Lambda \right)^{-1} \begin{bmatrix}B&amp;amp;P\end{bmatrix}
]&lt;/p&gt;

&lt;p&gt;가 된다. 다만 본인의 식 전개와 실제 논문 알고리즘에서 살짝 다른 부분이 있다면 필자는 $\tilde{C}^\top = \tilde{C}$로 쭉 전개해왔다는 사실이다. 같은 구조이기 때문에 큰 문제는 없다고 생각된다. 아무튼 이렇게 계산된 각 요소들로 효율적으로 계산된 $\hat{K}$에 다시 푸리에 역변환을 적용하면 $\overline{K}$를 구할 수 있게 된다. 드디어 이 논문의 아이디어가 되는 모든 알고리즘을 이해할 수 있었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/714ac16f-d0ee-46c9-ab9d-c44614194d3f&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;s4로-구성한-deep-layer의-구조&quot;&gt;S4로 구성한 Deep layer의 구조&lt;/h3&gt;

&lt;p&gt;앞에서 본 내용을 통해 S4에 필요한 파라미터는 다음과 같다는 것을 알 수 있다. $A$는 HiPPO 행렬의 어떠한 형태로 초기화가 될 예정이고, 해당 $A$는 그 형태에 맞게끔 특정 Diagonal 및 vector인 $\Lambda, P, Q, B, C$로 구성된다. Diagonal도 실질적으로 대각 성분 이외에는 다른 파라미터를 저장할 필요가 없기 때문에 S4는 state dimension $N$에 대해 총 $5N$의 학습 가능한 파라미터 수를 가진다. S4 자체는 Linear mapping 이지만 (동일한 길이의 시퀀스를 뽑아내는 구조), 이를 여러층 쌓고 Non-linearity를 더하게 되면 결국 Deep layer로 사용될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;결론은&quot;&gt;결론은…&lt;/h3&gt;

&lt;p&gt;실험 결과를 따로 보여주기보다는 이 상태로 마무리하는게 좋을 것 같다. 실험 결과야 당연히 efficiency를 보여주면서 long-range 효과성을 유지하는 그림이겠거니, 예상했고 논문에 예상한 그대로가 display된 것을 볼 수 있다. 가장 중요하다고 생각한 점은 모델링의 기초가 되는 SSM을 풀어냈던 이전 논문으로부터 개선점을 지속적으로 잡아내고 (예컨데, 행렬곱을 단순화하기 위해 구조를 분해하고 이를 수식으로 증명하는 것) 이러한 과정에서 연속으로 contribution을 낼 수 있다는 점을 배우게 된 것 같다.&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Jul 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/s4</link>
        <guid isPermaLink="true">http://localhost:4000/blog/s4</guid>
        
        <category>Mamba</category>
        
        <category>HiPPO</category>
        
        <category>LSSL</category>
        
        <category>S4</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Mamba modeling의 기초 (1) - Linear State-Space Layer (LSSL)에 대하여</title>
        <description>&lt;h1 id=&quot;연속-데이터-구조에-대한-dnn의-발전&quot;&gt;연속 데이터 구조에 대한 DNN의 발전&lt;/h1&gt;

&lt;p&gt;Sequential한 데이터를 처리하기 위해 딥러닝 모델은 수많은 변화와 발전을 이루었다. 그 중 요즘 대표적으로 &lt;strong&gt;LLM&lt;/strong&gt; 및 &lt;strong&gt;multimodal&lt;/strong&gt; 연구에서 활발하게 활용되는 것은 &lt;u&gt;Transformer 구조&lt;/u&gt;이지만, 그 이전에는 &lt;strong&gt;LSTM&lt;/strong&gt;이나 &lt;strong&gt;GRU&lt;/strong&gt;같이 Long term(거리가 먼 문맥 간의 관계성 파악) 모듈과 함께 연구된 Recurrent Neural Network (RNN), 그리고 가장 베이직한 DNN 구조인 CNN(Convolutional Neural Network)를 temporal dataset에 적절하게 변형시켜사용하는 방법이 있었다(예컨데, 비디오 데이터셋에는 temporal information 간의 정보도 사용하기 위해 시간 축을 추가한 3D convolution을 사용하였다).&lt;/p&gt;

&lt;p&gt;이외의 방법으로는 신경망 자체의 발전으로는 유명하지는 않지만 &lt;u&gt;보다 복잡한 continuous data를 처리하기 위해&lt;/u&gt; neural differential equations (NDEs)를 직접 모델링하는 방법이 주로 사용되었다.&lt;/p&gt;

&lt;p&gt;하지만 모든 네트워크는 &lt;strong&gt;나름의 장단점이 확실&lt;/strong&gt;했다. RNN (Recurrent Neural Network)은 모두가 알다시피 Long-term module의 발전이 있었음에도 불구하고 여전히 긴 문맥을 처리하는데 연산량이나 시간이 비례해서 증가한다는 문제점이 있었으며, 또한 Long-term 모듈에 의존하기에 복잡한 데이터에서의 문맥 파악을 학습시키기 어렵다는 근본적인 문제가 있었다.&lt;/p&gt;

&lt;p&gt;대체로 &lt;u&gt;gradient vanishing problem&lt;/u&gt;이나 &lt;u&gt;gradient exploding problem&lt;/u&gt;은 continual learning에서와 더불어 RNN과 같은 연속 데이터를 학습함에 있어 catastrophic forgetting의 주된 이유로 등장하기도 했다.&lt;/p&gt;

&lt;p&gt;CNN(Convolutional Neural Network)는 local한 정보에 대해 (서로 차원이 붙어있는 특징) 최적화가 빠르다는 장점이 있으며, 어느 정도 문맥이 명확한 비디오 데이터셋이라던지, 이미지와 같은 object centric/semantic centric 데이터에 대해 inductive bias를 가진다는 장점이 있었다. 하지만 연산 자체가 sequence에 대응할 수 있는 구조가 아니다보니, 길이가 길어질수록 RNN과 같은 문제가 발생하였다. 결국 convolution 연산 또한 &lt;u&gt;정해진 context 내에서의 local information만 뽑아내는 구조&lt;/u&gt;다 보니, context length에 따라 연산량이나 시간이 비례한다는 문제는 똑같이 생기게 되었다.&lt;/p&gt;

&lt;p&gt;NDE (Neural Differential Equation) 모델링은 특정 modality나 정해진 문제를 수학 모델링을 통해 이론화했지만, 그리 효율적이지 않다는 문제가 있다. 대표적으로는 diffusion modeling을 생각해볼 수 있는데, 생성 모델인 diffusion을 이런 효율의 문제를 score function의 이산화로 해결했다. Implicit model의 부담을 줄여주어서 간단한 U-Net 구조를 사용했고, consistency modeling과 같이 또다른 implicit mapping을 통해 해결할 수 있었지만, 이는 각 구간에서의 미분 방정식 solution을 numerical하게 구할 수 있었기 때문이었고 모든 형태의 미분 방정식에서 &lt;strong&gt;일괄적&lt;/strong&gt;으로 신경망이 &lt;u&gt;효율적으로 학습될 수 있는 구조를 찾는 것&lt;/u&gt;은 불가능하다.&lt;/p&gt;

&lt;p&gt;결국 가장 이상적인 모델 구조의 발전 방향은&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convolution과 같이 병렬화 연산이 가능한 구조여야 효율적일 수 있음.&lt;/li&gt;
  &lt;li&gt;Recurrence 형태의 상태 추론과정을 통한 문맥 처리가 되어야함.&lt;/li&gt;
  &lt;li&gt;Differential equation과 같이 이산화된 신호가 아닌 time-scale에 적용 가능해야함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;로 요약할 수 있다. 이러한 모델링을 찾기 위해 끊임없는 시도가 있었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;여러-모델링-방법들-소개&quot;&gt;여러 모델링 방법들 소개&lt;/h1&gt;

&lt;h3 id=&quot;ckconv&quot;&gt;CKConv&lt;/h3&gt;

&lt;p&gt;그 중 하나인 &lt;a href=&quot;https://arxiv.org/abs/2102.02611&quot;&gt;CKConv(Continuous Kernel Convolution)&lt;/a&gt;는 &lt;strong&gt;콘볼루션 커널&lt;/strong&gt;을 일종의 vector continuous function $\psi : \mathbb{R} \rightarrow \mathbb{R}^{N_{out} \times N_{in}}$ 으로 보는 방식이다. 이때 연속 함수 $\psi$는 작은 신경망 MLP로 parameterize하여 학습시키게 되는데, MLP는 value로 time-step을 &lt;strong&gt;스칼라 값&lt;/strong&gt;으로 받아 해당 position에서의 &lt;u&gt;convolution kernel을 벡터로 내보내는 형식&lt;/u&gt;이 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9ac63e5a-f0bf-48df-ba20-8c4f5953e201&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;unicornn&quot;&gt;UnICORNN&lt;/h3&gt;

&lt;p&gt;RNN 계열에서 ODE 기반의 모델링 (time-scaling을 통한 long-time dependency 확보)에서는 &lt;a href=&quot;UnICORNN&quot;&gt;UnICORNN&lt;/a&gt;과 같은 연구가 진행되기도 하였다. 간단하게 방법만 소개하면 해당 RNN은 2차 ODE(일반 미방)을 시간 축으로 이산화 (discretization)할 수 있는 오일러 메소드를 사용한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/3834c367-2ed2-4fc6-b802-d58e71a04cfc&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위의 그림에서 나와있는 $y$가 얻고자 하는 함수이고 $z$는 얻고자 하는 함수의 1차 미분 함수에 해당된다. 2차 ODE를 직접 풀어서 &lt;u&gt;원하는 함수를 얻기가 힘들기 때문&lt;/u&gt;에 $y$의 1차 미분 함수인 $y^\prime$을 $z$라는 임시 변수로 선언함으로써 2차 미분 ODE를 $z, y$ 간의 1차 미방으로 바꿀 수 있다.&lt;/p&gt;

&lt;p&gt;이렇게 변경된 ODE 시스템을 &lt;strong&gt;“Hamiltonian system”&lt;/strong&gt;이라고 부른다. 이 Hamiltonian system을 풀어내는 과정에서 시간별 input에 의존하는 연속 함수가 구현이 되고,&lt;/p&gt;

&lt;p&gt;[
    H(y, z, t) = \frac{\alpha}{2} \parallel y \parallel^2 + \frac{1}{2}\parallel z \parallel^2 + \sum_{i=1}^m\frac{1}{w_i} \log (\cosh (w_iy_i + (Vu(t))_i + b_i))
]&lt;/p&gt;

&lt;p&gt;각 벡터 $y, z$의 유클리디안 norm 연산인 $\parallel \cdot \parallel$ 을 통해 위와 같이 정리된다. 이 연속 신호 미분 방정식을 오일러 메소드를 통해 이산화하면 얻고자 하는 discrete dynamical system이 추출된다.&lt;/p&gt;

&lt;h3 id=&quot;lmu&quot;&gt;LMU&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.11417&quot;&gt;Parallelizing Legendre Memory Unit Training (LMU)&lt;/a&gt; 에서는 RNN의 단점 중 하나인 병렬화 불가능 문제를 linear recurrence convolution으로 해결하는 시도를 보였다. 만약 우리가 특정 input의 이전/이후 state를 가져올 수 있는 딜레이 구조의 시스템을 구축할 수 있다면, 해당 시스템의 output으로 input의 recurrence 구조를 확보할 수 있다는 장점이 생긴다. 우리는 Linear system을 찾고자 하기 때문에 (애초에 학습하고자 하는 신경망 연산 자체가 텐서 및 행렬 기반이기 때문이라 생각하면 편하다), 다음과 같이 네 개의 matrices $&amp;lt;A, B, C, D&amp;gt;$ 로 표현되는 &lt;strong&gt;LTI system을 찾는 것&lt;/strong&gt;이 목표가 된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{m} =&amp;amp; Am + Bu \newline
    y =&amp;amp; Cm + Du
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;그리고 I/O 의 라플라스 변환 형태인 $u(s), y(s)$로 SISO system의 transfer function $G(s)$를 정의할 수 있게 된다. 하지만 해당 &lt;strong&gt;transfer function&lt;/strong&gt;이 내포하는 어려움은 infinite dimensional하며, continous delay $\theta$를 모두 커버치기 불가능하다는 문제가 생긴다.&lt;/p&gt;

&lt;p&gt;[
    G(s) = \frac{y(s)}{u(s)} = e^{-\theta s}
]&lt;/p&gt;

&lt;p&gt;이제 finite하고 causal한 state space realization 차원으로 가져오기 위해서는 transfer function $G(s)$를 $s$에 대한 polynomial로 구성을 해야한다. 보통 transfer function은 분자와 분모가 각각 특정 차수를 가지는 $s$의 다항식으로 표현되는데, proper 한 dimension을 가지는 시스템은 분모의 차수가 더 높아야한다(그래야 시스템의 convergence를 보장할 수 있기 때문이다). 아무튼 위에 있는 저 식을 approximation 해야한다는 결론에 다다르게 된다. 이를 Linear system에서 구현하기 위해서 앞서 확인했던 것처럼 matrices를 구해야하고, $i,~j \in [0,d-1]$ 에 대해서 &lt;strong&gt;다음이 성립하는 행렬 요소&lt;/strong&gt;를 사용하게 된다.&lt;/p&gt;

&lt;p&gt;디테일한 내용이나 증명 과정은 해당 페이퍼의 이전 논문인 &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf&quot;&gt;LMU&lt;/a&gt;를 보거나 아래에 있는 증명 과정을 보면 된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    A_{i,j} =&amp;amp; \frac{(2i+1)}{\theta}\begin{cases}
    -1 &amp;amp; i &amp;lt; j \newline
    (-1)^{i-j+1} &amp;amp; i \ge j
    \end{cases}\newline
    B_i =&amp;amp; \frac{(2i+1)(-1)^i}{\theta}
    \newline
    C_i =&amp;amp; (-1)^i \sum_{l=0}^i {i \choose l}{i+l \choose j}(-1)^l \newline
    D =&amp;amp; 0
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;해당 매트릭스들 중 세번째 matrix인 $C$가 &lt;u&gt;가장 주요 아이디어&lt;/u&gt;에 해당된다. $C$는 풀게 되면 르장드르 다항식으로 표현되며, $D = 0$이기 때문에 shifted input $u(t-\theta)$ 의 정확도를 현재 state $m_t$를 기준으로 판별할 수 있다. 예컨데, $\theta^\prime$만큼의 phase가 이동된 신호를 예측하고자 한다면 다음과 같이 &lt;strong&gt;shifted Legendre polynomial&lt;/strong&gt;를 통해 근사할 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    C_i(\theta^\prime) = (-1)^i \sum_{l=0}^i {i \choose l}{i+1 \choose j}&amp;amp;\left(-\frac{\theta^\prime}{\theta}\right)^l,~0 \le \theta^\prime \le \theta \newline
    u(t-\theta^\prime) \approx&amp;amp; C(\theta^\prime)^\top m_t
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;설명이 길었지만 다시 풀어서 설명하자면, 이상적인 딜레이 시스템을 LTI 시스템으로 구축하여 표현한 것이 기존의 Linear state machine 디자인이었고, 이를 다시 non-linear neural network system을 사용하여 학습한 것이 LMU 구조가 되겠다. &lt;u&gt;딜레이 시스템을 솔루션으로 삼아&lt;/u&gt; 네트워크를 학습하려고 한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;hippo&quot;&gt;HiPPO&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.07669.pdf&quot;&gt;HiPPO&lt;/a&gt;는 LMU를 &lt;strong&gt;일반화한 구조&lt;/strong&gt;에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9c4199c9-90d5-4349-aa75-b3535dc2e32d&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HiPPO의 방법&lt;/strong&gt;은 다음과 같다:&lt;/p&gt;

&lt;p&gt;원래 함숫값과 예측된 함숫값 사이의 차이를 measure할 수 있는 Hilbert space $\mu$상에서 각 구간의 연속 함수인 $f$를 $g$라는 subspace로 보내는 과정을 거친 뒤, 이를 적당한 vector basis의 coefficient의 배열로 표현한다. 그렇게 되면 Continous-time ODE를 LTI system의 미분 방정식으로 표현할 수 있게 되며, 이때 system의 주축이 되는 $A(t)$와 $B(t)$ 함수의 형태를 결정하여 시퀀스 메모리에 대한 중요도를 매핑한다. 이를 통해 기존 LMU를 continuous-time memorization으로 일반화시켰다. 왜냐하면 기존 LMU(르장드르 메모리 유닛)에서는 특정 슬라딩 윈도우 크기($\theta$)를 가지는 이상적인 delay system의 &lt;u&gt;LTI 미분 방정식을 그대로 이산화하여 사용&lt;/u&gt;하기 때문이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl-모델링&quot;&gt;LSSL 모델링&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4329c0be-ee3c-4cd5-9488-ff101c875aba&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이런 이전 모듈들의 발전은 모두 공통적으로 기존 CNN/RNN의 구조 및 단점을 &lt;u&gt;time-step 차원에서 접근했다&lt;/u&gt;는 점이다. 하지만 모든 방법들은 &lt;strong&gt;convolutional/recurrent model&lt;/strong&gt;의 문제점을 근본적으로 해결하지 못했다는 점이 한계점으로 작용했다.&lt;/p&gt;

&lt;p&gt;Linear State-Space Layer (LSSL)은 위의 그림에서 나오는 각각의 장점을 통합한 구조를 고안하는 것을 주된 목적으로 삼았다. 결국 formulation은 이전 approach와 큰 차이는 없다. LSSL은 1-dimensional function 혹은 sequence $u(t) \rightarrow y(t)$를 implicit function $x(t)$를 통해 mapping하고자 하는 방법이다.&lt;/p&gt;

&lt;p&gt;$A$는 앞서 봤던 LMU에서와 같이 system의 &lt;strong&gt;implicit function&lt;/strong&gt; $x(t)$의 &lt;strong&gt;evolution&lt;/strong&gt;을 조정하는 matrix이며, $B, C, D$는 &lt;strong&gt;projection&lt;/strong&gt;에 사용된다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{x}(t) =&amp;amp; Ax(t) + Bu(t) \newline
    y(t) =&amp;amp; Cx(t) + Du(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;만약 $\Delta t$를 &lt;u&gt;discrete step-size&lt;/u&gt;로 정한다면, LSSL은 정해진 갯수의 메모리와 연산으로 각 시간 축에 따라 state를 변화시키는 &lt;strong&gt;recurrent model&lt;/strong&gt;로 해석할 수 있으며, LTI system인 위의 두 수식은 결국 &lt;strong&gt;continous convolution&lt;/strong&gt;으로 표현될 수 있다. 고로 discrete-time version의 LTI system 또한 convolution으로 병렬화가 가능하다. 학습 속도가 빨라질 수 있다는 것이다. 마지막으로 LSSL은 LTI system의 모델링 자체가 differential equation이기 때문에 continous-time model의 모든 적용 가능한 상황을 그대로 모방할 수 있다.&lt;/p&gt;

&lt;p&gt;결국 이 논문에서 밝히고자 한 내용은 위의 LSSL이 고전적인 제어 이론으로부터 익히 알려져있는 사실과 같이 모든 형태의 1-D Convolution을 표현할 수 있을 뿐만 아니라, 적절한 step size인 $\Delta t$ 그리고 적절한 state matrix $A$를 가지고 RNN 및 ODE가 가지는 특성(특히 장점에 집중)을 그대로 가져올 수 있다는 것이다. $A$는 다시 말하지면 시스템의 변화를 주도하는 학습 행렬로 사용되는데, HiPPO와 같은 이전 연구에서 드러났던 것처럼 연속 시간에 대한 memory를 고려하면서 동시에 long dependency를 고려해야한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;continuous-time-memorization&quot;&gt;Continuous-time memorization&lt;/h1&gt;

&lt;p&gt;Continuous time memorization 에 대한 근사화(approximation)는 HiPPO 그리고 LSSL 논문에서 공통적으로 가지는 이론적/기술적 배경에 해당된다.&lt;/p&gt;

&lt;p&gt;필연적으로 연속 시간 모델링을 그대로 적용할 수 없기 때문에 이를 이산 시간 모델로 근사화 혹은 다운 샘플링하는 과정을 거치게 된다.&lt;/p&gt;

&lt;p&gt;디퓨전 모델링에서도 확인할 수 있었던 것처럼 결국 연속 시간 미분 방정식의 $dt$를 얼마나 조정하냐에 따라 생성 성능이 달라졌기 때문에, 결국 연속 시간 모델링을 이산화할 때는 step size/time scale인 $\Delta t$를 조절하는 것이 중요하다.&lt;/p&gt;

&lt;p&gt;해당 섹션에서는 LSSL 모델링으로부터 여러 property에 대한 insight를 얻을 수 있는 &lt;strong&gt;근거&lt;/strong&gt;라고 볼 수 있는 개념들에 대해서 정리하도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;approximations-of-differential-equations&quot;&gt;Approximations of differential equations&lt;/h3&gt;

&lt;p&gt;모든 형태의 differential equation $\dot{x}(t) = f(t, x(t))$는 integral equation $x(t) = x(t_0) + \int_{t_0}^t f(s, x(s))ds$을 동치로 가지게 된다. 해당 integral solution은 함수 $x$의 근사치를 $f(s, x(s))$에 넣고 계속 연산을 하는 방식으로 풀어낼 수 있다. 예컨데 $x_0(t) = x(t_0)$라는 함수 초기 조건을 가지고 있다면,&lt;/p&gt;

&lt;p&gt;[
    x_{i+1} (t) = x_0 (t) + \int_{t_0}^t f(s, x_{i}(t))ds
]&lt;/p&gt;

&lt;p&gt;위와 같이 근사화할 수 있다. 이를 &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem&quot;&gt;Picard iteration&lt;/a&gt;&lt;/em&gt; 이라고 부른다.&lt;/p&gt;

&lt;h3 id=&quot;discretization&quot;&gt;Discretization&lt;/h3&gt;

&lt;p&gt;그리고 이산화 과정에서 함수를 직접 적분해낼 수 없기 때문에 discrete times $t_i$에 대해, $x(t_i)$를 쪼개서 얻어내야한다. Integral equation의 form을 closed form으로 정확히 계산할 수 있다면 단순히 downsampling하는 방법으로 각 $x(t_0), x(t_1), \cdots$ 를 얻어내거나, closed form으로 알지 못하더라도 &lt;em&gt;picard iteration&lt;/em&gt;을 각 구간별 integral equation인&lt;/p&gt;

&lt;p&gt;[
    x(t_{k+1}) = x(t_k) + \int_{t_k}^{t_{k+1}} f(s, x(s)) ds
]&lt;/p&gt;

&lt;p&gt;에 적용하여 각 $t_k$ 시점의 함숫값들을 샘플링할 수 있다. 다른 방법으로는 &lt;strong&gt;generalized bilinear transform (GBT)&lt;/strong&gt;가 있는데, 이는 현재 우리가 관심있는 Linear ODE에 적용될 수 있는 방법이다. 풀고자하는 Linear ODE의 형태가 다음과 같을때,&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    \dot{x}(t) =&amp;amp; Ax(t) + Bu(t) \newline
    y(t) =&amp;amp; Cx(t) + Du(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;GBT update는 다음의 수식으로 진행된다. 수식에서의 $\Delta t$는 step size를 의미한다.&lt;/p&gt;

&lt;p&gt;[
    x(t+\Delta t) = (I-\alpha \Delta t \cdot A)^{-1}(I+(1-\alpha)\Delta t \cdot A)x(t) +\Delta t(I-\alpha \Delta t \cdot A)^{-1}B \cdot u(t)
]&lt;/p&gt;

&lt;p&gt;수식이 조금 복잡해서 한번에 잘 이해가 되질 않지만 특별한 케이스를 보면 이해하기 어렵지 않다. $\alpha = 0$을 위 수식에 대입하면,&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    x(t+\Delta t) =&amp;amp; x(t) + \Delta t \cdot (Ax(t) + Bu(t)) \newline
    =&amp;amp; x(t) + \Delta t \cdot \dot{x}(t)
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;위와 같이 표현되며 이는 가장 대표적인 방법인 &lt;em&gt;Euler method&lt;/em&gt;임을 알 수 있다. 결국 $\alpha$는 동일하게 함수를 구하는 방식에서 어느 위치에서의 미분값을 사용하냐에 따라 달려있다. $\alpha=1$이 되면 &lt;em&gt;backward Euler method&lt;/em&gt; 가 되는데, 이는 동일하게 함수를 예측할 때 특정 위치에서의 도함수에 기반한 first order approximation이라는 점은 같지만 특정 위치가 $t$ 가 아닌 $t + \Delta t$ 라는 점에서 차이가 있다.&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\Delta t A)^{-1}x(t) + \Delta t (I - \Delta t A)^{-1} B \cdot \dot{x}(t)
]&lt;/p&gt;

&lt;p&gt;따라서 $\alpha = \frac{1}{2}$를 사용하게 되면 서로 다른 두 위치의 도함수 평균을 쓰게 되므로, 만약 곡률이 큰 복잡도가 높은 함수가 솔루션을 구성하는 상황에서는 같은 $\Delta t$를 사용하더라도 보다 안정적인 함수 예측이 가능해진다. 이를 &lt;em&gt;bilinear&lt;/em&gt; 방법이라고 부른다.&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\Delta t / 2A)^{-1}(I+\Delta t / 2A) x(t) + \Delta t (I - \Delta t / 2A)^{-1} B\cdot\dot{x}(t)
]&lt;/p&gt;

&lt;p&gt;이렇게 &lt;em&gt;bilinear&lt;/em&gt; 방법에 사용되는 matrix A와 B를 $\bar{A}, \bar{B}$ 라고 했을 때, 이를 통해 위의 시스템을 discretize하게 되면 다음과 같은 discrete-time state-space model을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_t =&amp;amp; \bar{A}x_{t-1} + \bar{B}u_t \newline
y_t =&amp;amp; Cx_t + Du_t
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;timescale-factor&quot;&gt;Timescale factor&lt;/h3&gt;

&lt;p&gt;시퀀스 길이에 따른 dependency는 길이가 길어질수록 줄어든다. 예컨데 $\Delta t$ 만큼을 시간 간격으로 잡는다면 의존도는 그에 반비례하게 된다. 대부분의 ODE 기반 RNN 구조에서는 $\Delta t$를 고정값으로 사용하였지만, classical RNN의 gating 메커니즘은 이를 학습하는 것과 같은 효과를 지닌다. 그리고 CNN 관점에서의 $\Delta t$는 convolution kernel의 크기를 조절하는 형태로 해석이 가능하다. 즉, CNN이든 RNN이든 ODE 기반으로 해석한다면 모두 시간 간격인 $\Delta t$를 어떻게하면 최적화할 수 있을까에 대한 문제로 해석이 가능하다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;continuous-time-memory&quot;&gt;Continuous-time memory&lt;/h3&gt;

&lt;p&gt;입력되는 함수 $u(t)$와 고정된 probability measure(메트릭) $\omega(t)$가 있을 때, 함수의 기본꼴이 되는 $N$개의 basis가 있다고 가정해보자. 각 time step $t$마다 이전까지의 input들인 $u(\tau)\vert_{\tau &amp;lt; t}$ 는 $N$개의 basis의 조합으로 표현이 가능하고, 이는 곧 함수를 projection하여 획득한 coefficient vector $x(t) \in \mathbb{R}^{N}$ 이다. 이때 각 time step마다의 최적의 솔루션은 거리 메트릭 $\omega(t)$에 의존하게 된다. 이렇듯 함수 $u(t)$를 coefficient $x(t)$로 표현하는 과정이 앞서 소개했던 HiPPO (High-Order Polynomial Projection Operator)가 된다.&lt;/p&gt;

&lt;p&gt;HiPPO의 경우에는 두 가지 경우(해당 논문에서는 LegT, LagT라는 이름으로 제안된 메트릭)를 제안하였는데, 모든 time step에 같은 중요도를 매핑하는 uniform measure $\omega = \mathbb{I}{[0, 1]}$ 와, 가까운 time step에 보다 높은 중요도를 매핑하는 exponential-decaying measure $\omega(t) = \exp(-t)$ 가 있다. 논외긴 하지만 HiPPO에서는 정해진 sliding window 크기를 가지는 translated Legendre (LegT) 대신 long dependency 및 forgetting 문제를 해결하고자 scaled Legendre (LegS)를 사용하였다. 둘의 공통점은 window 안에서 균일한 measure weight을 가진다는 점이지만, LegS는 시간이 흐를수록 window 크기가 커진다는 차이점이 있다. 아무튼 중요한 점은 measure 종류에 따라 matrix $A$를 closed form으로 풀어낼 수 있으며, 이를 토대로 long dependency에 대한 모델링이 가능하다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/944c3337-2caf-4e96-91fc-3fe5053d33b3&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;각 메트릭에 따른 matrix $A$를 정하는 과정은 HiPPO 논문의 Appendix를 참고하면 되는데, 이를 조금 간단하게 정리해보고자 한다. 관련 내용을 이해하는데 필요한 사전 지식이 너무 방대하여 완벽한 증명 과정을 담기에는 무리가 있지만 그럼에도 HiPPO 전반적인 내용을 이해해야 LSSL 모델링을 해석할 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;orthogonal-polynomials&quot;&gt;Orthogonal Polynomials&lt;/h3&gt;

&lt;p&gt;Orthogonal polynomials (서로 수직 관계에 있는 다항식)은 함수를 해석하는데 사용되는 기본적인 툴이다. 모든 measure $\mu$ 상에서 해당 OP에 대응되는 unique한 함수 시퀀스가 나오게 된다. 여기서 measure metric은 적분이 이루어지는 서브 공간으로 이해하면 된다. OP의 특징은, 서로 다른 OP들을 measure 상에서 적분했을때 0이 나와야한다는 것이다. 그리고 $i$번째 Polynomial은 차수가 $i$라는 constraints도 포함된다.&lt;/p&gt;

&lt;p&gt;[
\langle P_i, P_j \rangle_\mu = \int P_i(x) P_j(x) d\mu (x) = 0~~(i \neq j),~\deg (P_i) = i
]&lt;/p&gt;

&lt;p&gt;이러한 조건에서 $f$라는 이상적인 함수에 근사하는 최적의 솔루션은 다음과 같이 계산된다.&lt;/p&gt;

&lt;p&gt;[
\sum_{i=0}^{N-1} c_i P_i(x) / \parallel P_i \parallel_\mu^2,~\text{where }c_i = \langle f,P_i \rangle_\mu = \int f(x)P_i (x) d\mu(x)
]&lt;/p&gt;

&lt;p&gt;가장 대표적으로 유명한 OP에는 Fourier series basis를 생각해볼 수 있고, Jacobi, Laguerre 혹은 Hermite Polynomial도 이에 포함된다. 여기에서 소개할 OP는 Jacobi Polynomial에 속하는 르장드르 다항식이다.&lt;/p&gt;

&lt;h3 id=&quot;legendre-polynomials&quot;&gt;Legendre Polynomials&lt;/h3&gt;

&lt;p&gt;르장드르 다항식은 흔히 구면 좌표계에서 많이 사용한다. 공학 수학을 배울 때의 악몽이 떠오르는 기분이다. 암튼 orthogonal 관계는 익히 알려진대로 구간 $[-1, 1]$ 내에서 $L^2$ 내적을 취했을 때 $\frac{2}{2n+1}$ 만큼 스케일링된 크로네커 델타를 획득할 수 있다. 그리고 유명한 성질 중 하나가 $P_n(1) = 1, P_n(-1) = (-1)^n$ 라는 경계조건을 가진다는 것.&lt;/p&gt;

&lt;p&gt;여기서 일종의 선형성을 통해 다양한 time-scale 축에 대한 Polynomial 또한 구할 수 있다. 결국 르장드르 다항식이 성립하는 measure 공간 자체도 균일 확률 분포였기 때문에 가능한 일이다.&lt;/p&gt;

&lt;p&gt;원래의 orthogonality는 $[-1, 1]$에서 성립했고, 이를 $[0, t]$ 구간에서 성립하게 하기 위해 함수 구간을 맞춰주게 되면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
(2n+1)\int_0^t P_n \left( \frac{2x}{t} - 1 \right) P_m \left( \frac{2x}{t}-1 \right) \frac{1}{t} dx = \frac{2n+1}{2}\int P_n P_m \omega_\text{leg} dx
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;적분 구간만 맞춰줬는데 다시 크로네커 델타를 획득할 수 있다. 고로 measure가 스케일링된 경우 르장드르 다항식은 원래의 다항식을 스케일링 해주면 쉽게 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;[
(2n+1)^{1/2} P_n \left(\frac{2x}{t} - 1\right)
]&lt;/p&gt;

&lt;h3 id=&quot;translated-legendre&quot;&gt;Translated Legendre&lt;/h3&gt;

&lt;p&gt;Translated Legendre는 윈도우 크기가 $\theta$이고, 현재 지점이 $t$인 경우의 Legendre measure를 의미한다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\omega(t, x) =&amp;amp; \frac{1}{\theta} \mathbb{I}_{[t-\theta, t]} \newline
p_n(t, x) =&amp;amp; (2n+1)^{1/2}P_n\left(\frac{2(x-t)}{\theta} + 1\right) \newline
g_n(t, x) =&amp;amp; \lambda_n p_n (t, x)
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;그리고 원래 여기에서 tilting 개념이 등장하면서 굳이 OP를 쓰지 않을 때 사용하는 함수가 등장한다. 이를 $\chi$라고 하는데, 만약 $p_n(t, x)$ 대신 조합된 함수 형태인 $p_n(x)\chi(x)$를 쓴다고 가정한다면 각 time step에서 이번에는 $\omega/\chi^2$에 orthogonal해지게 된다 (OP 곱하기 OP 곱하기 $\chi^2$이 되므로). 만약 normalized된 measure와 orthonormal basis를 구한다치면,&lt;/p&gt;

&lt;p&gt;[
\zeta(t) = \int \frac{\omega}{\chi^2} = \int \frac{\omega^{(t)}(x)}{(\chi^{(t)}(x))^2}dx
]&lt;/p&gt;

&lt;p&gt;해당 함수가 곧 normalization constant가 된다. 그렇기에 normalized된 measure인 $\nu^{(t)}$는 $\frac{\omega^{(t)}(x)}{\zeta(t)\cdot(\chi^{(t)}(x))^2}$를 density로 가진다. 이렇게까지 하는 이유는 결국 tilted OP를 orthonormal하게 맞춰주기 위함이다. 위의 수식을 사용하여 orthogonality를 확인하면 르장드르에서의 orthogonality가 원래의 measure $\omega$에 대해 정규화가 됨을 알 수 있다. 하지만 이건 특수한 경우에 formulation을 위해 사용하게 되지만, 르장드르에 의한 projection에는 사용되지 않는다. 따라서 그냥 일반적인 수식을 생각해주면 된다. 앞서 추가로 언급했던 르장드르 다항식의 특성을 활용하면 마찬가지로 shifted and scaled Legendre에 대해,&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
g_n(t, t) =&amp;amp; \lambda_n (2n+1)^{1/2} \newline
g_n(t,t-\theta) =&amp;amp; \lambda_n (-1)^n (2n+1)^{1/2}
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;위의 경계조건을 가진다.&lt;/p&gt;

&lt;h3 id=&quot;projection-and-coefficients&quot;&gt;Projection and Coefficients&lt;/h3&gt;

&lt;p&gt;$A$ 하나 유도하는데 너무 돌아가는 듯 하지만 HiPPO를 완전히 정복하기 위해선 필수적인 수식들이다. 앞서 tilting을 고려한 measure를 유도했었는데, 이를 사용하여 coefficient를 계산하기 위해 measure에 projection한 결과는 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
c_n(t) = \zeta(t)^{-1/2} \lambda_n \int fp_n^{(t)} \frac{\omega^{(t)}}{\chi^{(t)}}
]&lt;/p&gt;

&lt;p&gt;해당 수식을 토대로 end-to-end model을 구성하고, 해당 네트워크가 online prediction에 기반에서 이전의 함숫값 $f$ 그리고 현재의 함수를 제대로 대변하게 하기 위해서는 $c(t)$를 벡터로 표현해야하고, 이는 곧 coefficient의 벡터 형태로 얻고자 하는 목적에 부합한다.&lt;/p&gt;

&lt;p&gt;Coefficient는 항상 현재의 예측에 기반하여 업데이트되어야한다. 즉 coefficient는 고정되어있지 않고 지속적으로 변하는 함수로 고려해야하며, 이에 맞는 미분 방정식을 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
\frac{d}{dt} c_n(t) &amp;amp;= \zeta(t)^{-1/2} \lambda_n \int f(x) \left(\frac{\partial}{\partial t}p_n (t, x) \right) \frac{\omega}{\chi} (t, x) dx \newline
&amp;amp;+\int f(x) \left( \zeta^{-1/2}\lambda_n p_n(t, x) \right)\left(\frac{\partial}{\partial t} \frac{\omega}{\chi} (t, x)\right) dx
\end{aligned}
]&lt;/p&gt;

&lt;h3 id=&quot;coefficient-dynamics-with-translated-legendre&quot;&gt;Coefficient dynamics with Translated Legendre&lt;/h3&gt;

&lt;p&gt;르장드르 다항식의 projection을 구할 때 tilting을 무시한다고 했다. 그러면 위의 수식을 풀어낼 때 필요한 것은 OP의 편미분과 measure의 편미분이다. OP의 편미분은 자세한 과정은 생략하고 결과만 언급하자면 $n$번째 르장드르의 미분은 $n-1$번째의 르장드르까지의 linear combination으로 표현할 수 있다. 놀라운 르장드르의 세계.&lt;/p&gt;

&lt;p&gt;그래서 정말 다행이지만 $\lambda_n p_n(t, x)$의 미분은 수많은 $g$들로 간단하게 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\frac{\partial}{\partial t} g_n (t, x) = -\lambda_n (2n+1)^{1/2} \frac{2}{\theta} \left( \lambda_{n-1}^{-1}  (2n-1)^{1/2}g_{n-1} + \lambda_{n-3}^{-1} (2n-1)^{1/2} g_{n-3} + \cdots \right)
]&lt;/p&gt;

&lt;p&gt;그리고 measure에 대한 편미분은 rectangular function에 대한 미분과 같다.&lt;/p&gt;

&lt;p&gt;[
\frac{\partial}{\partial t} \omega (t, x) = \frac{1}{\theta}\delta_t - \frac{1}{\theta} \delta_{t-\theta}
]&lt;/p&gt;

&lt;p&gt;준비물이 모두 완료되었기 때문에 이를 통해 앞서 구했던 coefficient dynamics를 표현한 미분 방정식에 대입이 가능하다.&lt;/p&gt;

&lt;p&gt;[
\frac{d}{dt}c_n(t) = -\frac{\lambda_n}{\theta} (2n+1)^{1/2} \sum_{k=0}^{N-1} M_{nk} (2k+1)^{1/2} \frac{c_k(t)}{\lambda_k} + (2n+1)^{1/2} \frac{\lambda_n}{\theta} f(t)
]&lt;/p&gt;

&lt;p&gt;이며 이 때 $M_{nk}$는 $k$가 $n$보다 작거나 같으면 무조건 $1$이고 $k$가 $n$보다 크면 $(-1)^{n-k}$의 값을 가지는 value이다. 이제 임의로 정해줄 수 있는 $\lambda_n = (2n+1)^{1/2}(-1)^n$를 적용하면&lt;/p&gt;

&lt;p&gt;[
\frac{d}{dt} c(t) = -\frac{1}{\theta} Ac(t) + \frac{1}{\theta} B f(t)
]&lt;/p&gt;

&lt;p&gt;의 수식에서&lt;/p&gt;

&lt;p&gt;[
A_{nk} = (2n+1)\begin{cases}
(-1)^{n-k}&amp;amp; \text{if }k &amp;lt; n \newline
1 &amp;amp; \text{if }k \ge n
\end{cases},~~B_n = (2n+1)(-1)^n
]&lt;/p&gt;

&lt;p&gt;앞서 소개했던 LMU가 그대로 나오는 것을 확인할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl-해석해보기&quot;&gt;LSSL 해석해보기&lt;/h1&gt;

&lt;p&gt;다시 LSSL로 돌아와서 Fixed state space representation $A, B, C, D$가 주어진 상황을 가정해보자. 간단하게도 LSSL은 input sequence를 output sequence로 매핑하는 과정이 된다. LSSL는 이러한 매핑 과정에서 파라미터 행렬 $A, B, C, D$ 그리고 discretize에 필수적인 $\Delta t$로 정의된다. 이제 이러한 LSSL이 대체 어떻게 RNN, CNN 그리고 Neural ODE의 모든 특징을 가질 수 있는지 해석해보도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;lssl-to-rnn&quot;&gt;LSSL to RNN&lt;/h3&gt;

&lt;p&gt;LSSL에서의 recurrent state는 각 time step$t$$x_{t-1}$에 해당한다. 현재 state $x_t$ 그리고 output $y_t$는 이산화된 LSSL formulation에 의해 계산된다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
x_t =&amp;amp; \bar{A}x_{t-1} + \bar{B}u_t \newline
y_t =&amp;amp; Cx_t + Du_t
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;따라서 RNN 구조와 같이 동작하는 것을 알 수 있다. 심지어 RNN 구조에서의 gated recurrence 도 만족한다. 예컨데 1차원의 gated recurrence 구조 $(1-\sigma (z))x_{t-1} + \sigma(z) u_t$는 backward-Euler method로 $\dot{x}(t) = -x(t) + u(t)$를 이산화한 것과 동일하다. $z$는 임의의 expression이 모두 가능한데, sigmoid function 특성과 앞서 소개한 GBT를 생각하면 $\Delta t = \exp (z)$로 표현했을때 gated recurrence가 $A = -1, B = 1$인 backward-Euler method임을 증명할 수 있다. 그런데 여기서 의문이 생길 수 있는 점은, Linear system에서 구축한 state layer가 과연 일반적인 deep RNN이 가지는 non-linearity 및 복잡도를 표현할 수 있는가에 대한 문제이다.&lt;/p&gt;

&lt;p&gt;앞서 단순히 $\dot{x}(t) = -x(t) + u(t)$의 이산화에 대해 언급했었는데, 이를 다르게 해석해서 &lt;em&gt;Picard iteration&lt;/em&gt;  을 사용한다고 생각하면, 결국 deep RNN은 학습 과정에서 &lt;em&gt;Picard iteration&lt;/em&gt; 을 거치면서 함수를 찾아간다고 생각할 수 있다. 즉, 만약 linear recurrence가 아닌 non-linear recurrence를 사용한다면 LSSL 또한 non-linearity를 학습할 수 있게 된다. 이를 통해 RNN 구조와 LSSL는 필요충분 관계에 놓여있다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;lssl-to-cnn&quot;&gt;LSSL to CNN&lt;/h3&gt;

&lt;p&gt;간단한 상황을 가정하기 위해 initial state를 $0$이라 가정해보자. 그렇게 되면 Linear state system을 풀어낸 output을&lt;/p&gt;

&lt;p&gt;[
y_k = C(\bar{A})^k\bar{B}u_0 + C(\bar{A})^{k-1}\bar{B}u_1 + \cdots + C\bar{A} \bar{B}u_{k-1} + \bar{B}u_k + Du_k
]&lt;/p&gt;

&lt;p&gt;이처럼 정리할 수 있으며, 이는 곧 discrete-time convolution으로 표현 가능하다.&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
&amp;amp;y = \mathcal{K}_L (\bar{A}, \bar{B}, C) \ast u + Du \newline
&amp;amp;\mathcal{K}_L (\bar{A}, \bar{B}, C) = (CA^iB)_{i \in [L]} \in \mathbb{R}^L
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;따라서 LSSL은 output이 convolution에 의해 연산되는 모델로 해석 가능하며, 콘볼루션 연산은 FFT로 가속화가 가능하다.&lt;/p&gt;

&lt;p&gt;일반적인 continous state-space system의 관점에서 output $y$는 input $u$에 대해 시스템의 impulse response function $h$와의 콘볼루션 연산으로 표현된다.&lt;/p&gt;

&lt;p&gt;[
y(t) = \int h(\tau)u(t-\tau) d\tau
]&lt;/p&gt;

&lt;p&gt;이와는 조금 다르게, convolutional filter가 만약 rational functional degree ($N$)를 가지는 경우, 크기가 $N$인 state-space model로 필터를 나타낼 수 있다. 기존 연구들에서 밝혔던 점을 토대로 임의의 convolutional filter $h$는 유한한 degree 값을 가지는 rational function으로 표현이 가능하다. 앞서 봤던 HiPPO matrix의 케이스를 예로 들어보도록 하자. 필요한 사전지식을 정리할 때 Translated Legendre의 경우를 보게 되면, $A$는 특정 구간($\theta$) 내에서 동일한 확률 분포를 가지는 measure에서 정의되었다. 일반적인 LSSL에서 $dt$를 고정시켜서 생각했을 때, 첫번째 식인&lt;/p&gt;

&lt;p&gt;[
\dot{x}(t) = Ax(t) + Bu(t)
]&lt;/p&gt;

&lt;p&gt;은 history element를 기억하는 과정에 해당되고 두번째 식인&lt;/p&gt;

&lt;p&gt;[
y(t) = Cx(t) + Du(t)
]&lt;/p&gt;

&lt;p&gt;은 해당 윈도우 내에서 유의미한 feature를 뽑는 작업이다. 그렇기 때문에 LSSL은 결국 width가 학습 가능한 convolutional kernel filter를 학습하는 과정과 동치라고 생각할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;deep-linear-state-system-layers&quot;&gt;Deep Linear State-System Layers&lt;/h1&gt;

&lt;p&gt;일반적인 LSSL은 간단하게 요약하면 입력 시퀀스를 출력 시퀀스로 매핑하는 시스템이었다. 예컨데 길이가 $L$인 신호가 있다면, LSSL은 $\mathbb{R}^L \rightarrow \mathbb{R}^L$을 수행하는 하나의 vec to vec 함수 구조이며 이때 함수 자체는 parameterized 되어있다. 만약 LSSL을 $\psi$라고 한다면,&lt;/p&gt;

&lt;p&gt;[
\psi(\cdot \vert A, B, C, D, \Delta t),~A \in \mathbb{R}^{N \times N},~B \in \mathbb{R}^{N \times 1},~C \in \mathbb{R}^{1 \times N},~D \in \mathbb{R}^{1 \times 1}
]&lt;/p&gt;

&lt;p&gt;이처럼 표현할 수 있다. 앞서 언급했던 것처럼 단일 LSSL은 Recurrence, Convolution의 특징을 모두 가지고 있기 때문에 RNN과 CNN의 대표적인 레이어인 recurent unit이나 convolution kernel처럼 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;또한 입력 시퀀스가 transformer의 input처럼 $H$의 hidden dimension을 가지고 있다고 하면($L \times H$), LSSL은 $H$만큼의 LSSL을 독립적으로 학습하게 되고, Transformer의 multi-head 효과 또한 그대로 적용할 수 있다.&lt;/p&gt;

&lt;p&gt;말하고자 했던 것은 LSSL를 stacking하는 과정으로 기존 DNN 방법론과 같이 다양한 함수를 모사할 수 있으며 동시에  normalization, residual connection과 같은 방법론과 함께 모델링될 수 있다는 사실이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lssl과-continuous-time-memorization&quot;&gt;LSSL과 Continuous-time Memorization&lt;/h1&gt;

&lt;p&gt;LSSL이 기존 DNN 모델링의 특징을 살리면서 사용될 수 있다고 해서 무작정 사용할 수는 없는 노릇이고, LSSL이 장점을 보일 수 있어야 한다.&lt;/p&gt;

&lt;h3 id=&quot;long-dependency-into-lssls&quot;&gt;Long dependency into LSSLs&lt;/h3&gt;

&lt;p&gt;Discretized Linear system ODE에서, 시스템은 이산화된 parameter $\bar{A}$가 계속 곱해지며 발전해간다.&lt;/p&gt;

&lt;p&gt;[
x_t = \bar{A}x_{t-1} + \bar{B}u_t
]&lt;/p&gt;

&lt;p&gt;그 말은 gradient descent로 학습하게 되면vanishing gradient 문제를 피할 수 없다는 것이다. 이처럼 만약 $A$를 랜덤하게 초기화한 후 학습하는 형태를 사용하면, 기대하는 성능이 나오지 않을 것이라는 말이 된다.&lt;/p&gt;

&lt;p&gt;하지만 HiPPO와 같은 framework에서는 measure $\omega$에 따라 어떤 방식으로 이전 function을 기억할 지에 대한 문제를 언급했었다 (projection/coefficient화 과정을 통해). 그러나 HiPPO의 문제점이라고 한다면 이렇게 매뉴얼하게 정한 hippo matrix를 학습하지 못하고 그대로 사용해야한다는 점이다. 왜냐하면 HiPPO에서는 르장드르를 포함한 일부 measure에 대해서만 이를 풀어낼 수 있는 structured solution matrix $A$가 존재했고, 모든 일반적인measure에도 다른 형태의 $A$가 존재할 수 있다는 사실을 밝히지 못했기 때문이다.&lt;/p&gt;

&lt;p&gt;따라서 LSSL에서는 이를 arbitrary measure $\omega$로 확장시키고, 이때 Low-recurrence width $A$에 대한 미분 방정식을 찾을 수 있다고 증명하였다.&lt;/p&gt;

&lt;h3 id=&quot;efficient-algorithms-for-lssls&quot;&gt;Efficient Algorithms for LSSLs&lt;/h3&gt;

&lt;p&gt;그러나 A와 $\Delta t$가 상당히 중요한 parameter임이 드러났음에도 불구하고, naive LSSL에서는 학습하기 어렵다는 문제가 발생한다. LSSL은 MVM(Matrix Vector Multiplication) 그리고 Krylov function을 연산할 때 (각각 convolution/recurrence에 해당) 전자의 경우에는 matrix inversion이 필요하다는 어려움과,&lt;/p&gt;

&lt;p&gt;[
x(t+\Delta t) = (I-\alpha \Delta t \cdot A)^{-1}(I+(1-\alpha)\Delta t \cdot A)x(t) +\Delta t(I-\alpha \Delta t \cdot A)^{-1}B \cdot u(t)
]&lt;/p&gt;

&lt;p&gt;후자는 $\bar{A}$를 feautre의 길이인 $L$만큼 곱해야 한다는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{K}_L (\bar{A}, \bar{B}, C) = (CB, CAB, \ldots, CA^{L-1}B)
]&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 $A$를 학습할 때의 효율성을 증대하기 위한 조건이 하나 더 발생한다. 모든 기존의 fixed LSSL의 $A$는 &lt;em&gt;3-quasiseparable&lt;/em&gt;함이 증명되었다. 만약 학습되는 $A$ 또한 &lt;em&gt;quasiseparable&lt;/em&gt; 특성을 유지할 경우, MVM과 krylov function 연산이 보다 적은 연산량으로 처리될 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;evaluations-and-demonstrations&quot;&gt;Evaluations and Demonstrations&lt;/h1&gt;

&lt;p&gt;실제로 특정 조건을 가지는 $A$를 학습할 수 있으면, 이는 이전 HiPPO system $A$보다 더 좋은 성능을 보임을 확인하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/22d862b8-30bd-4508-8b9b-c321c90bb6e2&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 길이가 긴 음성 신호의 classification 성능을 통해 long time dependency 또한 입증하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e13e1a42-ae22-4b8a-a488-20d7055e905b&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 기존 SoTA에 필적하는 성능을 보이기까지 학습 epoch가 훨씬 적어질 수 있음을 보여주었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4051c4d1-39c4-4f20-9fef-b7cc0a6c4f7c&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Mamba modeling의 가장 기초가 되는 LSSL을 살펴보았으며, LSSL의 이해에는 HiPPO의 이해가 필수적이기 때문에 해당 논문도 함께 다루었다. 앞으로 몇개의 포스팅을 통해 Mamba를 리뷰하게 될지는 모르겠지만 State Modeling에 대해서는 아무도 제대로 정리를 안해놓을 것 같아서..&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Feb 2024 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/lssl</link>
        <guid isPermaLink="true">http://localhost:4000/blog/lssl</guid>
        
        <category>Mamba</category>
        
        <category>LSSL</category>
        
        <category>HiPPO</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>DINOv2(Learning Robust Visual Features without Supervision) 논문 리뷰</title>
        <description>&lt;h1 id=&quot;supervised-학습의-한계점&quot;&gt;Supervised 학습의 한계점&lt;/h1&gt;

&lt;p&gt;이전 게시글 중 &lt;a href=&quot;https://6unoyunr.github.io/blog/dino&quot;&gt;DINO&lt;/a&gt;에서는  Self-supervised learning은 NLP 뿐만 아니라 CV에서도 적절한 전략을 잘 사용한다면 기존 ViT/CNN 구조에서 발견하지 못한 &lt;strong&gt;유의미한 visual feature를 획득할 수 있음&lt;/strong&gt;이 증명되었다는 점을 소개했었다. Supervised learning은 label이 존재하는 형태의 학습에서 손실 함수로 적용되는 objective value가 명확하다는 점, 그렇기 때문에 학습되는 encoder 및 decoder의 hypothesis를 정확하게 align할 수 있다는 장점이 있었으나 특정 task에 최적화된 파라미터는 label space가 조금만 달라지거나 input image의 domain이 조금만 달라지더라도 optimal point에서 크게 벗어날 수 있기 때문에 일반화된 성능을 보여주기 힘들다는 명확한 한계점이 존재했다. 특히 classification과 같이 이미지를 전반적으로 해석하는 task에서는 큰 문제가 없지만 segmentation과 같이 입력 이미지와 동일한 resolution에서 픽셀별 prediction이 진행되는 high-level task에서는 더 큰 문제를 불러오게 된다. NLP가 상대적으로 성능 수렴을 빠르게 달성하고 거대 모델의 property를 찾거나, tuning 방법과 관련된 연구가 진행된 바탕에는 바로 supervised learning에서 벗어났다는 사실이 존재한다. 즉 task 마다 직접 생성해주는 ground truth는 high level로 올라갈수록 cost가 높아진다는 superficial한 단점 말고도 궁극적으로 얻고자 하는 robust한 visual feature를 얻는 과정에 악영향을 준다는 문제가 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-learning&quot;&gt;Self-supervised Learning&lt;/h1&gt;

&lt;p&gt;DINO 첫번째 논문을 간단하게 요약하면 EMA 구조를 가져가면서 batch size에 따라 overfitting/underfitting되지 않도록 무관한 안정적인 학습을 위해 model output의 entropy를 조절하는 centering/sharpening 작업을 도입했었다. DINO에서는 ‘ ViT를 위한 SSL, 그리고 이를 통해 획득할 수 있었던 visual feature의 특징에 대해 서술했다.’라고 한다면 &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;DINOv2&lt;/a&gt;는 ‘이러한 SSL 구조를 어떻게 확장시킬지 데이터/모델링 관점에서 서술했다.’고 요약할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/98f5a9a8-8f08-47f5-b857-57b17ad04208&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SSL이 가지는 장점을 보여주는 하나의 qualitative 예시는 위의 그림에서 확인할 수 있다. 첫번째 칼럼에서 ‘새’와 ‘비행기’는 low level에서 비슷한 semantic 특징을 가지지만 엄연히 다른 도메인에 속한다. 그럼에도 불구하고 각 이미지의 pixel 사이의 PCA를 수행하고 가장 큰 값을 가지는 3개의 component를 visualize하면 비행기의 날개 부분이 새의 날개 부분에 매칭되거나 몸통 부분은 몸통 부분에, 꼬리는 꼬리 부분에 매칭되는 것을 확인할 수 있다(색을 보면). 마찬가지로 세번째 칼럼에서 하나의 말이 존재하는 이미지, 여러 말이 존재하는 이미지와 같이 이미지 인스턴스 내에 특정 물체를 나타내는 feature가 많이 존재하는 상황에서도 이러한 correspondence가 잘 유지된다던지, sketch(drawing)과 같이 input에 대한 natural shift가 발생한 상황에서도 PCA 결과가 합리적인 것을 볼 수 있다. 학습된 모델 스스로가 같은 종류의 instance를 포함하는 여러 도메인 사진에 대해서도 correspondence를 잘 인지할 수 있고, 이는 곧 추가적인 fine-tuning 작업 없이도 학습된 visual feature를 다양한 downstream task/dataset에 활용할 수 있음을 보여준다. 이를 흔히 표현하는 방식으로는 &lt;strong&gt;“Out of Box model”&lt;/strong&gt;라고 부른다. 한국말로 &lt;strong&gt;“우물 안 개구리 형태의 모델에서 벗어남”&lt;/strong&gt;이라고 하고 싶다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dataset-filteringcurating&quot;&gt;Dataset filtering/Curating&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0d576f9a-a58b-4af4-8603-2661ce3b1542&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대용량의 데이터에 대해 SSL을 수행하기에 앞서 paper에서는 &lt;strong&gt;data curation(데이터 정제) 작업의 중요성&lt;/strong&gt;을 언급한다. 특정 dataset을 semantic하게 분석했을 때 bias가 존재한다면 다양한 이미지에 대한 visual feature를 뽑을 때 bias가 포함될 수 있기 때문이다. 이러한 ‘문맥상’의 정규화에 대한 중요성은 이미 SSL이 만연하게 적용된 NLP에서는 어느 정도 당연하게 인식하고 있는 사실이며, computer vision에서도 비슷한 맥락의 효과를 얻고자 한다면 data curation이 필수적이라는 것이다. 사실 생각해보면 supervised learning에서도 이와 같은 data curation이 필요하기는 하지만 label space 상에서 카테고리 간의 비율만 얼추 맞으면 학습 수렴에 큰 문제가 없었기 때문에 치명적으로 작용하는 문제가 아니었으나, self-supervised/unsupervised learning에서는 모델은 오로지 ‘이미지’에만 의존한 학습을 하기 때문이라고 볼 수 있다. 데이터를 처리한 과정은 다음과 같다. 위의 framework figure를 참고하면서 보면 이해하기가 편하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Curated dataset은 다음과 같이 다양한 데이터셋을 사용한다. Uncurated dataset으로는 Crawling이 가능한 웹사이트에서 img 태그에 포함된 url source를 통해 가져오게 되며, 이렇게 획득한 소스 이미지들을 PCA hash deduplication(중복 제거), NSFW filtering(성적인/폭력적인 이미지 필터링) 그리고 초상권 문제가 있기 때문에 사람 얼굴을 blurring하는 등의 작업을 추가로 진행한다. 이렇게 수집한 이미지는 대략 1.2B 이미지이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0c0ec5b8-b69e-4a15-8bc9-0d0bc468cb87&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Uncurated dataset 자체적으로 진행한 중복 제거 및 이런저런 필터링을 제외하고도, 기존 curated dataset을 기준으로 curation을 진행하기 전 이미 존재하는 데이터셋과 중복되는 이미지를 없애는 작업을 시작한다. 굳이 dataset에 이미 존재하는 이미지를 다시 retrieval하는 과정을 진행할 필요는 없기 때문이다.&lt;/li&gt;
  &lt;li&gt;Self-supervised image retrieval
중복 제거가 완료된 uncurated dataset을 기준으로 이미지 retrieval은 curated dataset과 잘 align되는 샘플들을 추출하는 과정이다. 각 이미지에 대한 임베딩을 ImageNet-22k에 사전 학습된 ViT-H/16 네트워크로 추출하고, 이미지 벡터 간의 코사인 유사도를 통해 벡터 간 거리를 계산하게 된다. 만약 retrieval의 구심점이 되는 이미지가 충분하다면 query를 기준으로 $N$개의 가장 가까운 이미지들을 찾은 뒤 이를 그대로 데이터셋에 넣고(위의 표에서 sample에 해당), 충분하지 않다면 cluster로부터 샘플링하는 방법을 채택한다. Cluster 방식에서는 uncurated data source를 $100,000$개의 분리된 cluster로 구성한 뒤 retrived image가 포함된 cluster에서 $10,000$개의 이미지를 가져온다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-pre-training&quot;&gt;Self-supervised pre-training&lt;/h1&gt;

&lt;p&gt;데이터셋 정제와 더불어 학습법도 DINO에 비해 일부 추가된 점이 있는데, 각 요소들에 대해 간단히 요약하면 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;image-level-objective&quot;&gt;Image-level objective&lt;/h3&gt;

&lt;p&gt;DINO 원래 논문에서 사용하기로는 student/teacher network에 각각 local/global feature를 구분해서 넣고, 나오는 output에 대해 consistency loss를 cross entropy term으로 적용했었다. Loss에 대한 최적화는 student에만 적용하고 teacher는 EMA로 파라미터 업데이트하는 것까지 동일하게 사용하였다.&lt;/p&gt;

&lt;h3 id=&quot;patch-level-objective&quot;&gt;Patch-level objective&lt;/h3&gt;

&lt;p&gt;Student model로 들어가는 일부 patch를 랜덤하게 마스킹하고,  각 mask patch 위치의 feature 간의 cross-entropy loss를 추가하였다. Mask에 의한 augmentation 효과가 더해졌다고 보면 된다.&lt;/p&gt;

&lt;h3 id=&quot;untying-head-weights-between-both-objectives&quot;&gt;Untying head weights between both objectives&lt;/h3&gt;

&lt;p&gt;image/patch loss를 적용할 때 같은 head(classifier)를 사용하면 image-level loss는 overfitting되고 patch-level loss는 underfitting되는 문제가 발생하였고, 각 loss가 적용되는 헤드의 분리를 통해 이 문제를 해결할 수 있었다고 한다.&lt;/p&gt;

&lt;h3 id=&quot;sinkhorn-knopp-centering&quot;&gt;Sinkhorn-Knopp centering&lt;/h3&gt;

&lt;p&gt;DINO에서 teacher softmax-centering하는 방식을 SWaV로 바꾸게 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4ce93f23-5ee2-48bf-bd11-ef1ee2a91062&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SWaV 논문에서는 prototype $C$가 일종의 code book 역할이 되는 $Q$로 향하는 경로로서 학습이 진행된다.  서로 다르게 augmentation된 이미지는 각자 prototype에 의해 코드북으로 매핑이 진행되고, 각자 본인의 코드북을 예측하는 것이 아니라 다르게 augmentation된 이미지에 의해 매핑된 코드북을 예측한다. 이로써 일반적인 contrative learning을 대체할 수 있다는 논리 전개가 가능하다. Collapse (모든 latent인 $z$가 동일한 code $q$로 매핑되는 케이스)를 막기 위해 code book $Q$는 각 배치 단위에서 구성되는 모든 샘플들을 각각의 코드북에 균등하게 배분하는 과정을 거친다. 즉 $B$만큼의 배치 사이즈로 $K$개의 코드북에 매핑될 때, 각 iteration 마다 코드북 하나는 &lt;strong&gt;최소한&lt;/strong&gt; $B/K$ 만큼 선택될 수 있어야한다는 조건이 필요하다.  해당 조건 내에서 최적화 문제를 풀어내는 과정을 쭉 요약하는 것이  SWaV 에 대한 내용이다. 이 논문에서 제안된 방법은 centering을 위한  빌드업이었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/3ff3ff88-e990-4622-aea9-e6bffa9375b4&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/60c219f3-c6d0-43a3-b04a-9b6345af04ce&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Centering은 단일 헤드의 학습 불안정성을 해소하기 위한 일종의 앙상블 장치였다. 과거의 prediction 정보를 accumulation 함에 따라 이전 input들의 정보가 이후 input의 prediction을 보다 bias되지 않게 해줄 수 있었다. 이러한 방법 대신 representation에 head를 $m$개씩 달아두고, 해당 예측 정보들에 대한 SWaV에 weight를 주어 앙상블하는 방법을 제안한 것이 바로 &lt;a href=&quot;https://arxiv.org/abs/2211.09981&quot;&gt;Weighted Ensemble Self-supervised learning&lt;/a&gt;이다. 엔트로피(예측의 확실성 지표)에 따른 weight가 가장 좋은 성능을 보였고, centering을 SWaV 방식에 여러 head로부터의 앙상블로 대체한 것이 DINO의 representation 성능을 증가시키는 것을 확인할 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;koleo-regularizer&quot;&gt;KoLeo regularizer&lt;/h3&gt;

&lt;p&gt;배치 내에서 각 샘플들이 embedding되는 point 간의 간격을 동일하게 유지하고자 하는 정규화 term이다. 예컨데 embedding point $n$개 모두에 대해 가장 가까운 다른 point와의 거리를 log 분포로 나타내면, 이 분포가 균등하면 균등할수록 엔트로피는 커질 것이고 regularizer는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt; 가 되게끔 역수를 취해 사용한다. 물론 embedding point 간의 거리 자체가 확률 분포를 표방하는 값으로 간주되므로 정규화 작업 전 normalization을 통해  scale을 조정해준다. 즉 거리를 균등하게 함으로써 필요한 정보량을 최소로 하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;[
    \mathcal{L}_{\text{koleo}}= -\frac{1}{n}\sum_{i=1}^n \log (d_{n, i}),~~d_{n, i} = \min_{j \neq i} \parallel x_i -x_j \parallel
]&lt;/p&gt;

&lt;h3 id=&quot;adapting-the-resolution&quot;&gt;Adapting the resolution&lt;/h3&gt;

&lt;p&gt;보다 높은 해상도의 이미지를 사용했을 때 segmentation/detection과 같은 pixel level task에서의 성능이 올라간다. 이는 small object의 경우 low resolution에서 백본에 연산을 돌리면 작은 물체의 feature가 일종의 노이즈처럼 사라지는 현상이 발생되기 때문이다. 그렇다고 해서 무작정 고차원의 이미지를 가지고 모델을 학습시키는 건 메모리나 시간이 투머치로 소모적이기 때문에 권장되지는 않는다.&lt;/p&gt;

&lt;p&gt;따라서 DINOv2에서는 pretraining 마지막 일부만 이미지의 해상도를 $518 \times 518$로 증가시켜서 학습시키는 전략을 사용했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;efficient-training-details&quot;&gt;Efficient Training Details&lt;/h1&gt;

&lt;p&gt;DINO-v2는 일종의 테크니컬 리포트다. 사실 앞부분만 보아도 데이터 정제 과정이나 학습에 사용한 프레임워크를 방법론으로 제시했다기보단 기존의 연구로부터 이어지는 여러 insight 및 approach를 사용해서 좋은 모델을 만들어보겠다는 노력이 보이기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;utilizing-faster-transformer--flash-attention&quot;&gt;Utilizing faster transformer : Flash attention&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ed1cc1d1-5daa-47ac-a190-c2f915818d9c&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash attention&lt;/a&gt;에서 제안한 방법을 사용하면 하드웨어에 최적화해서 적용되기 때문에 더 빠르고 효율적인 연산이 가능하다. 이 논문 저자의 경우 논문에 나온 FlashAttention을 직접 구현하여 사용하였다. 사실 아직 본인은 FlashAttention 논문 자체는 이해하지 못하고 있다. 하드웨어 요소에 대한 이해가 부족한데, 어떻게 하면 이쪽으로 지식 및 기술 스택을 쌓을 수 있으려나…??&lt;/p&gt;

&lt;h3 id=&quot;nested-tensors-in-self-attention&quot;&gt;Nested tensors in self-attention&lt;/h3&gt;

&lt;p&gt;이전의 implementation에서는 서로 다른 patch token 수를 가지게 되는 global crop/local crop이 서로 따로 forward passing 및 backward passing 과정을 거쳤었다. 그러나 새롭게 구현된 버전에서는 이를 동시에 수행함으로써 그만큼의 연산량을 줄일 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;efficient-stochastic-depth&quot;&gt;Efficient stochastic depth&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.09382&quot;&gt;Stochastic depth&lt;/a&gt;는 네크워크 깊이에 따른 layer dropout을 수행하여 레이어 간의 의존성 문제를 해결하고 학습 시 각 layer의 feature map을 빠르게 최적화하는 것이 주된 contribution이었다. DINO-v2에서는 이를 다르게 수행하는데, 레이어를 skip한다는 개념이 아니라 batch 단위로 랜덤하게 들어오는 샘플들을 각 레이어에서 drop out rate $d$에 따라 $(1-d)$만큼의 batch만 block에 통과시키는 전략을 취한다. 어차피 unsupervised setting이기 때문에 batch 순서에 따른 label space의 영향을 무시할 수 있으며, 학습마다 네트워크의 모든 block에 대해 stochastic하게 drop-out을 해줄 경우 오히려 학습 시간이 늘어날 수 있기 때문에 이를 최소화한 전략으로 보인다.&lt;/p&gt;

&lt;h3 id=&quot;fully-shared-data-parallel-fsdp&quot;&gt;Fully-shared data parallel (FSDP)&lt;/h3&gt;

&lt;p&gt;AdamW로 EMA 구조를 가지는 모델을 최적화할 때, model로 하여금 4개의 replica가 필요하다. Student, teacher와 이에 추가로 Adam/AdamW 최적화에 사용되는 first momentum 그리고 second momentum이 필요하기 때문이다. 보다 큰 모델을 사용할 때 memory footprint가 급격히 증가할 수 밖에 없는데, 이를 해소하기 위해 data parallel을 사용하였다. 그리고 이렇게 replica를 gpu에 분리를 할 때 GPU memory가 분리된다는 점에서 또다른 장점이 생기는데, 이는 weight 저장 자체는 float32로 하고 최적화 시 gpu 간 통신에서는 float16으로 부동 소숫점 절반을 날려버려도 학습 성능의 큰 저하 없이 메모리를 줄일 수 있다는 것이다. 원래대로라면 GPU 갯수가 증가하더라도 메모리 총합은 같은게 DDP의 특징이었는데, FSDP를 사용하면 communication 단의 메모리를 절반으로 줄여버리니까, 결론적으로는 보다 많은 GPU를 분리해서 사용할수록 학습 전체 메모리는 줄어드는 장점이 생긴다.&lt;/p&gt;

&lt;h3 id=&quot;model-distillation&quot;&gt;Model distillation&lt;/h3&gt;

&lt;p&gt;큰 모델이 가지고 있는representation을 효과적으로 작은 모델에 넘겨줄 때, 작은 모델을 scratch부터 학습시키는 것보다는 큰 모델의 prediction에 align하는 distillation 학습법이 효과적인 것은 어느 정도 알려진 사실이다. 따라서 DINO-v2에서도 결론적으로 작은 모델에 넘겨주는 방식을 distillation으로 했는데, 이때 기존 학습 framework인 EMA는 그대로 가되, 약간의 차이가 발생한다. 우선 self-distillation 구조가 아니므로 teacher는 학습된 large model을 frozen한 채로, 작은 모델을 student로 잡아 spare EMA를 수행한다. 또한 앞서 설명했던 stochastic depth, masking 같은 방법론은 제외한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4d5ce6e4-4f94-488c-ab34-320b6a2a1806&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;DINO-v2는 베이스라인 논문인  &lt;a href=&quot;https://arxiv.org/pdf/2111.07832.pdf&quot;&gt;iBOT&lt;/a&gt;(image BERT 논문)을 기준으로 짜잘한 방법들이 추가로 사용되었는데, 그래서 제안한 방법들이 얼마나 효과적인지를 task를 고정한 채로 ablation을 진행한 표이다. 이외에 여러 실험 결과들에 대한 내용이 페이퍼에 있는데, 대부분 성능이 올라갔다는 점을 드러내고 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;

&lt;p&gt;DINO-v2 논문을 보면 주인공이 모든 무술을 연마하여 마스터해버리는 무협 영화(?)장르가 생각난다. 메타의 이 연구는 단순히 scaling-up하는데 목적을 두지 않고 보다 효율적인 학습법과 그러면서도 어떻게 좋은 representation을 얻을 수 있는지 다양한 방법들을 적용해보고 실험해본 결과물로 보인다. 어느새 학계에서 성능 좋은 베이스라인 모델을 만드는 것은 불가능에 가까운 게 아니라 불가능이 되어버렸다. 이제는 논문을 쓰는 과정에서 집중해야 할 곳들은 이런 SSL 자체보다는 학습된 representation을 어떻게 활용하고, mapping하고 혹은 tuning할 것인가에 있어 보인다.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Dec 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/dino2</link>
        <guid isPermaLink="true">http://localhost:4000/blog/dino2</guid>
        
        <category>SSL</category>
        
        <category>ViT</category>
        
        <category>DINOv2</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>DINO(Emerging Properties in Self-Supervised Vision Transformers) 논문 리뷰</title>
        <description>&lt;h1 id=&quot;들어가며-&quot;&gt;들어가며 …&lt;/h1&gt;

&lt;p&gt;제목에서 알 수 있듯이 이 논문은 &lt;strong&gt;Vision Transformer&lt;/strong&gt;가 자기 학습을 통해 &lt;strong&gt;습득할 수 있는 능력이나 특성&lt;/strong&gt;에 대해 논의한다. ViT의 프레임워크가 제안된 배경에는 자연어 분야의 Transformer 구조가 존재하는데, 이미 GPT나 BERT와 같은 후속 연구를 기반으로 NLP에서는 Large Dataset의 self-supervised learning이 downstream task에서 보다 풍부한 semantic information을 제공한다는 사실이 증명된 바 있다. 이와는 다르게 ViT의 학습 구조를 보게 되면 언어 모델과 같이 대용량의 이미지 데이터셋을 사용하여 사전 학습하는 과정이 이후의 downstream task에도 도움이 된다는 사실은 증명이 되었으나, 여전히 &lt;u&gt;supervised learning 구조&lt;/u&gt;에서 벗어나지 못한 것을 알 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;발견한-특성들&quot;&gt;발견한 특성들&lt;/h1&gt;

&lt;p&gt;논문 구성은 간단하게도 아이디어를 develop하는 과정(self-supervised learning 방법론에 대한 approach)에 대한 motivation으로 시작하게 되고, 해당 방법론으로부터 온 &lt;strong&gt;효과&lt;/strong&gt;를 언급하면서 이를 증명할 &lt;strong&gt;여러 실험 결과들&lt;/strong&gt;을 보여주게 된다.  논문에서 발견한 ViT의 self-supervised learning 특성을 요약하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;자기 학습을 통해 획득한 ViT의 feature는 이미지의 semantic segmentation 정보를 가지게 되고, 이는 지도 학습으로 학습된 ViT나 convnet에서도 발견되지 않은 특성이다.  실제로 아래 그림과 같이 attention 정보를 통해 네트워크가 각 이미지 단위로 포커싱하고있는 영역이 곧 이미지 상에서 object의 semantic한 정보 그 자체라는 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/eb6f733b-13f1-47bd-a927-140a73b8379e&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;또한 이렇게 획득된 feature는 유사도에 기반한 $k$&lt;em&gt;-NN&lt;/em&gt; classifier로 활용될 수 있고 small ViT로도 ImageNet(recognition task)에서 좋은 정확도를 보임을 확인하였다.&lt;/li&gt;
  &lt;li&gt;마지막으로 여러 셋팅에서의 실험 및 충분한 ablation을 통해 ViT의 자기 학습 과정에서 효과적으로 쓰일 수 있는 방법론들을 직접 실험들을 통해 규명했으며, momentum encoder, multi-crop training 그리고 smaller patch(more number of patches)가 중요하게 쓰인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;self-supervised-learning-frameworks&quot;&gt;Self-supervised learning Frameworks&lt;/h1&gt;

&lt;p&gt;대표적인 label이 없는 환경에서의 unsupervised(self-supervised) learning 접근법으로는 &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1911.05722&quot;&gt;MoCo&lt;/a&gt; 그리고 &lt;a href=&quot;https://arxiv.org/abs/2006.07733&quot;&gt;BYOL&lt;/a&gt;가 있었다. 갑자기 이 얘기를 꺼낸 이유는 이 페이퍼에서 논하고자 했던 property가 곧 ViT의 self-supervised learning으로부터 나오기 때문에, 제안된 structure의 근거를 알기 위해서는 이전 논문들의 참고가 필수적이기 때문이다. DINO 논문에서는  SimCLR, MoCo 그리고 BYOL 중 BYOL에서 inspiration을 얻었다고 한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/6fd9d9d2-501b-41ef-8f25-a7160b657d16&quot; width=&quot;500&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0ce868d7-e974-449a-988a-2a9966c0daf1&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SimCLR의 구조는 첫번째 그림과 같다. Input $x$가 주어지면 이를 정해진 random augmentation을 적용한 각각의 샘플 $\tilde{x}_i$ 그리고 $\tilde{x}_j$로 만들게 되고, 이를 뉴럴 네트워크 $f(\cdot)$에 통과시킨 output $h_i$, $h_j$ 를 하나의 representation/embedding이라고 했을 때 이를 Linear operation($g(\cdot)$)으로 mapping한 최종 latent인 $z_i$ 그리고 $z_j$를 contrastive하게 학습하는 방법을 사용하였다.  Moco도 큰 틀에서는 contrastive learning과 두 개의 branch를 사용한다는 점에서 SimCLR와 거의 동일하지만, 차이점이라고 한다면 SimCLR은 배치 내에서 동일한 인코더를 기준으로 representation 학습을 진행하지만 MoCo는 학습이 되지 않고 EMA 방식으로 업데이트되는 momentum encoder가 사용된다는 점이다. 쿼리에 사용되는 배치는 매 학습마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enqueue&lt;/code&gt; 및 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dequeue&lt;/code&gt; 를 통해 최신 mini-batch가 지속적으로 업데이트되며, positive logit은 동일 배치의 샘플에 대해, negative logit은 이전 queue의 샘플에 대해 연산을 진행하게 된다. Querying에 사용되는 encoder를 점진적으로 학습하는 방법을 적용했다는 점에서 차이가 생긴다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;byol-paper에-대한-짧은-논문-리뷰&quot;&gt;BYOL paper에 대한 짧은 논문 리뷰&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/bbb5f5a9-775e-4e5c-b55d-dfee0a51db60&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;BYOL은 momentum encoder의 장점을 가져오면서 학습의 전반적 형태는 SimCLR와 같은 방식을 가져왔다. 그러나 알고리즘 측면에서 큰 차이가 있는데, 이는 바로 BYOL은 contrastive learning을 하지 않는다는 점, 즉 negative pair가 필요하지 않다는 것이다.&lt;/p&gt;

&lt;p&gt;BYOL에서 가장 크게 주목할 점은 어떻게 negative pair와 같이 collapse를 방지할 만한 장치가 없이도 안정적인 학습이 가능한가에 대한 부분이다. 바로 이 부분에서 대체 왜 online/target 두 브랜치가 서로 assymetric(비대칭)하게 구성되었는가를 확인할 수 있다. 예컨데 predictor $q_\theta(\cdot)$는 projection 목적이 되는 $g_\theta$와 동일한 신경망 구조를 가진다.  단순하게 생각했을때 prediction은 하나의 classifier라고 생각할 수 있지만 그렇지는 않고 projection network와 같은 dimension의 output을 내보낸다. 하지만 바로 이 projector 부분이 학습되면서 optimal point에 놓여있는 것이 가장 주요한 학습 키포인트로 작용한다.&lt;/p&gt;

&lt;p&gt;Projector가 수렴했다는 것은 곧 projector가 어느 정도 optimal한 영역에 있다고 볼 수 있고 이를 $q^\ast_\theta$라고  한다면 online branch의  input이 되는 $z_\theta$에 대해 $q^\ast_\theta(z_\theta) = \mathbb{E}[z^\prime_\xi \vert z_\theta]$로 표현이 가능하다.  수식 상에서 조건부 expectation은 $z_\theta$에 대한 함수가 되며, 조건부 확률 분포와 동일한 의미를 가진다. 즉 우리가 흔히 optimal하게 학습된 neural network를 특정 도메인의 데이터셋 ${X, Y}$에 대해 parameterized posterior $p_\theta(Y \vert X)$로 표현하는 것처럼 projector가 수렴했다는 가정 하에 $z^\prime_\xi$와의 수식으로 해석할 수 있다. 결국 이 가정을 통해 수식을 다시 전개하게 되면 simplified BYOL loss(원래 BYOL에서는 view를 교차하는 형태로 symmetric한 cost function을 구성하는 것과 더불어 latent의 정규화 작업이 추가됨)은 다음과 같이 표현 가능하며&lt;/p&gt;

&lt;p&gt;[
\mathcal{L}_\text{BYOL} = \mathbb{E}\left(\parallel \mathbb{E}(z^\prime_\xi\vert z_\theta)-z_\xi^\prime\parallel_2^2\right)
]&lt;/p&gt;

&lt;p&gt;결국 학습 파라미터(online branch) $\theta$에 대한 gradient는 다음과 같이 expected variance의 gradient로 수렴하게 된다.&lt;/p&gt;

&lt;p&gt;[
\nabla_\theta \mathcal{L}_\text{BYOL}= \nabla_\theta\mathbb{E}\left(\sum_i \text{Var}(z^\prime_{\xi, i} \vert z_\theta) \right)
]&lt;/p&gt;

&lt;p&gt;이러한 가정은 optimal projector가 수렴했다는 전제에서 성립하게 되는데, 이를 통해 BYOL loss는 수렴된 projector를 변화시키지 않고 online network를 업데이트할 수 있다. 위의 수식은 파라미터 및 projection을 다변수로 가지는 최적화 함수를 &lt;strong&gt;Lagrangian으로 표현했을 때&lt;/strong&gt;의 envelop theorem 그리고 optimality condition에 기반한다.&lt;/p&gt;

&lt;p&gt;BYOL에서는 이렇게 업데이트되는 $\theta$에 대해 online branch와 target branch $\xi$가 동시에 감소하는 방향은 loss surface $\mathcal{L}$에는 정의될 수 없다는 것이다. Target branch에서의 projection $z^\prime_\xi$와 online branch에서의 $z_\theta$에 대한 Variance로 loss 최적화 식을 만들었었고 이게 의미하는 바는 projector가 어느 정도 수렴한 상황에서 가장 말단의 posterior는 고정된 상태라고 보는 것이다. 임의의 random variable에 대해 조건부 분산은 조건 변수가 추가될수록 이전 분산의 lower bound가 된다. 만약 BYOL을 통한 최적화 과정이 collapse를 일으킨다면 online network의 projection인 $z_\theta$는 더이상 무작위로 분포한 확률 랜덤 변수가 아닌 constant $c$로 고정될 수 있고, 이는 parameter space에서기존 업데이트 과정이 lower bound가 됨을 명시할 수 있는 근거가 된다.&lt;/p&gt;

&lt;p&gt;[
\text{Var}(z^\prime_\xi \vert z_\theta) \le  \text{Var}(z^\prime_\xi \vert c)
]&lt;/p&gt;

&lt;p&gt;즉 collapse가 일어날 수 있는 환경이 parameter surface에서 보다 큰 값을 가지기 때문에 더 불안정(unstable), collapse가 발생하지 않는다.&lt;/p&gt;

&lt;p&gt;만약 반대라면 어떻게 될까? 이 논문에서는 $\xi$를 loss function을 기준으로 업데이트하지 않고 EMA를 사용하여 점진적 과부하를 걸었는데, 이는 같은 위의 수식으로 그 이유를 찾을 수 있다. Target network에 collapse가 발생한다면 이번에는 $z^\prime_\xi = c$ 인 deterministic constant가 되고, 이번에는 조건부 변수가 아닌 메인 변수에 해당되므로 분산이 0이 된다.&lt;/p&gt;

&lt;p&gt;[
\text{Var}(c \vert z_\theta) = 0 \le \text{Var}(z^\prime_\xi \vert z_\theta)
]&lt;/p&gt;

&lt;p&gt;즉 $\xi$에 대해서 학습하게 되면 무조건 collapse가 발생하게 된다는 것을 볼 수 있다. BYOL는 이러한 이론적 배경에 근거하여 negative pair를 굳이 구하지 않더라도 similarity loss를 기반으로 점진적으로 latent를 bootstrapping (과거의 online parameter가 미래의 online parameter의 학습에 도움이 되는 과정)하는 방법을 제시하였고, 이는 batch size로부터의 자유 및 자기 학습 방법의 지평을 보다 넓힐 수 있는 계기가 되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/f5fa2a42-8f41-4b0a-982a-ad4eb53015b8&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dino-approach&quot;&gt;DINO approach&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/76fd785f-1d59-412d-913e-732d4d404c30&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;DINO는 ‘knowledge distillation with no labels’의 줄임말로, 말 그대로 ViT 학습에 SSL 프레임 워크를 제안한 형태가 된다. 이 방법 역시 student/teacher(혹은 online/target)의 두 브랜치 간의 학습이 진행되는데, 안정적인 pseudo label을 만들어내는 teacher은 loss term에 대한 파라미터 최적화가 발생하지 않고consistency를 통해 지속적으로 update되는 student parameter를 지수 평균 이동(exponential moving average)으로 가져온다.&lt;/p&gt;

&lt;h3 id=&quot;knowledge-distillation&quot;&gt;Knowledge Distillation&lt;/h3&gt;

&lt;p&gt;논문에서 접근한 SSL은 다음과 같다. Student model과 teacher model은 학습되는 중간에는 데이터가 매핑되는 함수로 작용하고, 이는 곧 probability mapper로 해석 가능하다. 만약 student network($g$)의 파라미터를 $\theta_s$라 한다면 입력 신호 $x$에 대한 output logit $g_{\theta_s}(x)$를 구할 수 있다. 그리고 이 logit에 softmax function을 적용하면 확률로의 직접 매핑이 가능하다. 이때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt;가 적용되는 dimension은 특징자(feature embedding) 축이라고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;[
P_s(x)^{(i)} = \frac{\exp(g_{\theta_s}(x)^{(i)}/\tau_s)}{\sum_{k=1}^K\exp(g_{\theta_s}(x)^{(k)}/\tau_s)}
]&lt;/p&gt;

&lt;p&gt;여기서 temperature $\tau_s$가 사용되는데, 이는 다양한 논문들에서 probability distribution의 분포를 결정하는 하이퍼파라미터 혹은 학습 가능한 파라미터로 많이 사용된다. 이 논문에서의 temperature parameter의 목적은 student network에 의한 probability의 sharpness 조절 역할을 하게 된다. 마찬가지로 teacher network에 대해서도 다음과 같은 formulation이 가능하다.&lt;/p&gt;

&lt;p&gt;[
P_t(x)^{(i)} = \frac{\exp(g_{\theta_t}(x)^{(i)}/\tau_t)}{\sum_{k=1}^K\exp(g_{\theta_t}(x)^{(k)}/\tau_t)}
]&lt;/p&gt;

&lt;p&gt;Knowledge distillation에서 학습은 teacher의 output을 일종의 ground truth로 가정한 student output과의 consistency loss이다. 즉 cross entropy에서 one-hot label을 teacher network의 output으로 바꿨다고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;[
\underset{\theta_s}{\min} H(P_t(x),~P_s(x)) = \min_{\theta_s} {-P_t(x) \log P_s(x)}
]&lt;/p&gt;

&lt;p&gt;크로스 엔트로피가 의미하는 것이 정보이론에서 “하나의 확률분포”가 “또다른 확률분포”가 가지는 정보와 얼마나 가까운지에 따른 거리 metric이기 때문에 결국 학습 목적은 학생으로 하여금 선생의 지식을 잘 모방하도록 하는 것이 된다. 하지만 단순히 이 방법론으로 마무리되는 알고리즘은 아니고, DINO는 효과적인 학습을 위해 다양한 방법들을 추가하게 된다. 예컨데 위의 수식은 앞서 보여준 framework와는 다르게 augmentation에 대한 내용이 없지만, 저자는 바로 이 수식 전개 직후 단순 distillation을 사용함에 따라 생기는 문제점들을 언급한다.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h3&gt;

&lt;p&gt;Transformer 구조가 가지는 가장 큰 문제점 중 하나가 local-to-global correspondence가 적다는 것이다. Transformer는 attention을 기반으로 단번에 global information을 인지하기 때문에 convolution network에 비해 가지는 장점도 있겠지만, local information을 포착하기 전에 모든 attention map들이 global feature에서 수렴해버린다면 CNN이 가지는 계층적 구조에 의한 correspondency(feature간 상관관계에서 얻을 수 있는 high to low level 효과)를 SSL에서 기대할 수 없다는 문제가 있다.&lt;/p&gt;

&lt;p&gt;따라서 이를 해결하는 방법으로 augmentation의 비대칭을 사용하였다. 이 프로세스를 요약하면 다음과 같다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Teacher가 local/global에 대한 consistency를 가지고 어느 정도 수렴했다는 가정 하에, teacher는 이미지의 global한 형태를 보고 ‘그럴 듯한’ 예측을 한다.&lt;/li&gt;
  &lt;li&gt;위의 가정이 있다면 teacher network에는 계속 global한 image 정보만 주면 된다.&lt;/li&gt;
  &lt;li&gt;Teacher는 student의 파라미터로부터 EMA된다. 즉, teacher의 바람직한 수렴을 위해서는 student가 앞서 말했던 local/global에 대한 consistency 정보를 학습할 수 있는 환경이 되어야한다.&lt;/li&gt;
  &lt;li&gt;따라서 student에는 local image 정보를 같이 준다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;길게 설명했지만 풀어쓰자면 teacher network에는 이미지에 큰 범위에서의 augmentation이 들어간 global view $x_1^g$, $x_2^g$만 예측에 사용되고, student network에는 multi-crop strategy와 같은 이미지의 작은 범위까지의 augmentation이 적용된 local/global  정보가 예측에 사용된다.&lt;/p&gt;

&lt;p&gt;[
\underset{\theta_s}{\min}\sum_{x \in {x_1^g,~x_2^g}} ~~\sum_{x^\prime\in V,~x^\prime \neq x} H(P_t(x), P_s(x^\prime))
]&lt;/p&gt;

&lt;p&gt;논문에서는 보통의 방식과 같이  multi-crop image들을 생성했는데, 2개의 global views는 원본 이미지 대비 $50\%$보다 큰 크기만큼 잘라서 쓰고 여러 local view는 반대로 $50\%$보다 작은 크기만큼 잘라서 쓴다.&lt;/p&gt;

&lt;h3 id=&quot;avoiding-collapse&quot;&gt;Avoiding collapse&lt;/h3&gt;

&lt;p&gt;Self-supervised learning의 문제점은 representation 학습에 대한 ground truth가 없기 때문에 collapse가 발생할 수 있다는 것이다. 사실상 우리가 많이 알고 있는 contrastive learning이든, clustering 방식이든, predictor를 다는 BYOL과 같은 구조라던지 Batch Normalization을 도입하는 등등의 approach는 공통적으로 collapse를 막는 역할을 하게 된다. 물론 DINO 또한 normalization 구조라던지 앞서 언급한 여러 방법론으로 stabilization을 수행할 수 있었지만, 이 논문에서는 momentum teacher network의 output을 centering 및 sharpening하는 구조를 통해 이러한 효과를 얻을 수 있다고 한다.  Sharpening/Centering에 대한 내용은 조금 알아보기 쉽게 나타내면 Sharpening은 temperature 조절을 통해 softmax 예측값 분포를 보다 명확하게 드러내는 것이고 centering은 teacher output에 center value $c$를 bias term으로 더해주어 예측값 사이의 차이를 조절해주게 된다. 즉 sharpening과 centering은 효과만 보게 되면 서로 반대의 역할을 수행한다. 여기서 드는 의문점은, 굳이 sharpening을 통해 prediction의 entropy minimization을 수행할 목적이었다면 왜 다시 centering이라는 방법으로 다시금 prediction을 재조정하는 과정을 거치는지에 대한 부분이다. 이 부분에 대해서 나름대로 이해한 것은 다음과 같다.&lt;/p&gt;

&lt;p&gt;우선 centering에 사용될 bias term $c$는 다음 식을 통해 exponentially update가 된다. EMA 방식으로 teacher parameter가 업데이트되는 것과 동일하다.&lt;/p&gt;

&lt;p&gt;[
c \leftarrow mc + (1-m)\frac{1}{B}\sum_{i=1}^B g_{\theta_t}(x_i)
]&lt;/p&gt;

&lt;p&gt;식을 자세히 보면 center $c$에는 결국 학습 시 사용되는 batch size랑은 무관하게, 기존 input에 대한 model의 output(prediction) 정보를 평균으로 저장하게 된다. 이제 centering에 대한 맥락은 얼추 이해했고, 다시 sharpening으로 돌아가보도록 하자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/3300523b-cf01-4d51-bb52-9572577dce41&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;예컨데 고양이라는 이미지에 대해 모델이 낸 prediction을 sharpening하는 작업을 하게 되면 뭉뚱그려진 예측값을 어느 정도 명확하게 하면서 feature map을 선명하게 만들어 준다는 장점이 있지만, 만약 배치 단위로 들어오는 특정 input이 모델로 하여금 지속적으로 collapse가 발생하게 한다면, 이는 불난 집에 부채질하는 격이 된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4fe5d55f-ea03-40d0-880f-19d149b711d7&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;원래 목적이라 함은 다른 input에 대해 특징을 잘 잡아내는 feature를 뽑고자 sharpening을 도입했는데, contrastive learning과 같은 제어장치가 없다면 모델은 그냥 단순히 네트워크 예측 자체의 entropy를 낮추는 방향으로 끊임없이 학습이 될 것이기 때문에 이미지의 종류에 상관없이 단일의 feature를 뽑게 되고, 이러한 문제를 trivial solution이 발생한다고 한다. 결국 기존 SSL approach를 사용하지 않고는 이를 근본적으로 해결하기 어렵다는 문제가 발생한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e39fe5bd-3db7-45b0-acc5-704a6b375c28&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그렇기 때문에 만약 centering term이 있다면 이를 단순히 model prediction에 더해주는 것만으로도 이전 배치들의 정보를 가져올 수 있으며, batch size의 크기에 robust한 학습 효과를 보여주는 것이다. 예컨데 contrastive learning에서는 positive sample과 negative sample 쌍을 얻기 위해 최대한 많은 배치 수가 필요했고, 그 이유는 모델이 학습할 때 metric learning을 적은 단위의 배치 내에서 진행하는 것보다는 큰 배치 내에서 진행하는 것이 전체 데이터셋의 확률 분포를 잘 나타낼 수 있기 때문이었다. 하지만 위와 같이 output을 뽑아서 배치 단위의 prediction을 저장하고, 이를 이후의 output을 sharpning할 때 smoothing에 사용하는 것만으로도 배치 사이즈를 키우지 않고 이러한 효과를 볼 수 있다는 것이 바로 sharpning/centering이 가지는 장점이다. 사실 까놓고 말하자면 단순하게 이전 prediction을 일종의 prototype으로 저장해놓고 쓴다는 느낌인데, 저자들은 이 방법론이 실제로 학습에 미치는 영향을 보여주기 위해 실험을 진행하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/3f832aed-a68e-423d-a4e1-62753184677f&quot; width=&quot;950&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Sharpening의 효과는 entropy를 0으로 만든다. 그리고 centering의 효과는 smoothing을 통해 어떤 input이 들어오든 entropy를 유지시킨다. 둘 중 하나만 사용하면 epoch에 따라 representation overfitting/underfitting이 발생하는모습을 잘 확인할 수 있다. 무엇보다 이를 잘 보여주는 실험이 KL divergence에 있지 않을까 싶다.&lt;/p&gt;

&lt;p&gt;Teacher/student 구조를 쓰면서 얻고 싶은 장점은 EMA 방식으로 기존 representation 정보를 차곡차곡 모아가는 teacher network의 prediction을 student가 따라가면서 서로 간의 학습에 bootstrapping이 일어날 수 있다는 것인데 만약 representation이 collapse가 된다면 이러한 효과를 볼 수 없을 것이고, 결국 bootstrapping이 없다는 것은 학습이 진행되면서 student/teacher prediction 차이가 없어진다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;실험-결과&quot;&gt;실험 결과&lt;/h1&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;p&gt;SSL/Unsupervised Learning의 경우 학습된 feature를 증명하는 과정이 여러 가지로 분류된다.
우선 downstream task에 맞게 head를 다는 과정이 필요하고, 이 head를 어떻게 써먹냐에 따라 linear classifier/fine-tuning/k-NN classifier로 분류된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear classifier : 학습된 backbone을 frozen한 채로 linear classifier만 학습해서 representation의 효과를 보고자 하는 것&lt;/li&gt;
  &lt;li&gt;Fine tuning : 학습된 backbone을 head에 붙인 채로 fine tuning하여 representation의 효과를 보고자 하는 것&lt;/li&gt;
  &lt;li&gt;k-NN classifier : classifier 같은 부수적인 요소 없이 단순히 embedding으로 retrieval해서 representation/metric learning 자체 효과를 보고자 하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/cc62eada-796a-49e8-accf-6bd3adcdd6ad&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0f84aae6-f90f-4b25-8cc6-97289836756b&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;뭐 성능 자체와 관련해서는 상당히 좋게 나온 것을 확인할 수 있고, ViT baseline의 다른 SSL 방식과 비교했을 때도 유의미하게 높은 classification 성능을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;vit-attention-map&quot;&gt;ViT Attention map&lt;/h3&gt;

&lt;p&gt;하지만 classification 보다는 DINO의 가장 큰 특징은 ViT의 attention map을 보면 잘 드러나는데, 바로 local feature에 attention을 집중할 수 있다(localization)는 것이다. 이는 기존 ViT 방식으로는 얻을 수 없는 feature map에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9911292e-5ea2-473d-9a88-ab2070eb996a&quot; width=&quot;350&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1ad1c235-9d6f-4817-a7f5-4810188080e0&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0453a259-0cb5-43e7-b0b8-acd9f7afa2d2&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Segmentation처럼 high-level image task의 경우 모델의 예측이 정교해야하기 때문에 상대적으로 classification task에 비해 SSL이 달성할 수 있는 성능 수치가 그리 높지 않았다. 그럼에도 불구하고 DINO의 attention map을 보면 알 수 있듯이 이 페이퍼에서 제안한 학습 방법은 ViT의 input에 대한 attention을 효과적으로 localization하는 것에 성공하였고, 정량적으로도 그 수치를 증명하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;

&lt;p&gt;DINO는 BYOL을 비롯한 기존 SSL 방식에서 motivation된 self-distillation 구조와 더불어 collapse를 방지하고 학습 안정화를 위해 sharpening/centering을 도입하여 ViT를 효과적으로 학습하였다. 그런데 이렇게 안정적으로 ViT를 라벨 없이 SSL로만 학습하고 보니 이게 무슨 일이람. ViT의 attention map이 localization되는 중요한 변화를 확인할 수 있었다. 이러한 점이 시사하는 바는 상당히 크다.&lt;/p&gt;

&lt;p&gt;지금까지는 supervised learning이 절대적인 학습법이었으며, 사실 SSL이 학습 안정화를 토대로 가끔 supervised learning의 성능을 넘는 경우도 있긴 했지만은 모든 task에 정통으로 사용될 수 있는 방법은 아니었으며 linear probing이나 fine tuning 시에 미리 학습된 representation의 효과를 강하게 보여주었지 실질적으로 SSL로 학습된 representation이 가능성을 보여주는 경우는 많지 않았다. 하지만 애초에 구조상 inductive bias가 없어 localization이 힘든 transformer baseline인 ViT를 SSL하였더니 attention map이 segmentation 효과를 보여주었고, 이는 NLP가 아닌 Computer Vision 분야에서도 classification 뿐만 아니라 여러 task에 SSL이 우월한 성능을 보여줄 수 있음을 증명하였다.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Nov 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/dino</link>
        <guid isPermaLink="true">http://localhost:4000/blog/dino</guid>
        
        <category>SSL</category>
        
        <category>ViT</category>
        
        <category>DINO</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>Continual learning/Lifelong learning의 개념과 방법론 총정리</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;이 글은 survey 논문인 &lt;a href=&quot;https://arxiv.org/abs/2302.00487&quot;&gt;A Comprehensive Survey of Continual Learning: Theory, Method and Application&lt;/a&gt;를 각색 및 요약한 글입니다.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h1 id=&quot;continual-learning-이란&quot;&gt;Continual Learning 이란?&lt;/h1&gt;

&lt;h3 id=&quot;인공지능과-유기체의-근본적-차이&quot;&gt;인공지능과 유기체의 근본적 차이&lt;/h3&gt;

&lt;p&gt;인공지능의 주체가 되는 ‘모델’을 중심으로 돌아가는 지성 체계인 intelligent system에서 &lt;strong&gt;학습&lt;/strong&gt;은 &lt;strong&gt;주어지는 데이터 환경의 기본&lt;/strong&gt;이 된다. 외부의 자극이 달라지게 되면 사람을 포함한 다양한 유기체들은 지속적으로 이를 통해 정보를 수집하고, 기존 knowledge를 수정하는 형태로 업데이트하거나 축적해가는 과정을 거친다. 일반적인 오프라인 학습이 전제된 Static한 모델의 경우 학습 프레임워크과 인퍼런스 프레임워크가 분리되어있는 것을 알 수 있는데,  이는 결국 학습을 위해 일반화가 가능할 정도의 big data를 train domain으로서 정의한다는 점에서 명확한 한계를 가질 수 밖에 없다. 예컨데 단순히 categorize(혹은 classification) 문제만 하더라도, data augmentation이나 여러 모델 일반화를 위한 방법론을 적용하더라고 어찌되었든 ‘정해진 카테고리 내에서의 분류’라는 점과, ‘model representation에 open-world(실생활)의 모든 데이터를 내포할 수 없음’라는 점이 한계라고 볼 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/7c075df7-4b81-4079-baa7-34356761be9e&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;물론 categorize 문제에서만 드러나는 문제점은 아니고, computer vision의 대표적인 task인 detection이나 segmentation에 대해서도 마찬가지이다. 물론 최근 연구에 따라 closed set을 사전 정의하지 않고 다양한 semantic information에 무관하게 작용할 수 있는 &lt;a href=&quot;https://segment-anything.com/&quot;&gt;SAM(Segment-Anything Model)&lt;/a&gt;와 같은 연구 방향도 제시되었지만, 결국 근본적으로 학습 데이터의 pool(범위)를 증가시켰을 뿐이지 사람이나 일반적인 유기체들이 하는 것처럼 학습 프레임워크과 인퍼런스 프레임워크가 통합된 구조는 아니라는 것을 알 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9fb0c7e6-243c-4ec3-bf1b-70ebf6c9a61c&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;continual-learning가-시사하는-문제점-그리고-딜레마&quot;&gt;Continual learning가 시사하는 문제점 그리고 딜레마&lt;/h3&gt;

&lt;p&gt;물론 모델 일반화에 사용되는 데이터가 증가할수록 그에 따른 performance도 증가하는 것은 당연하고, 이러한 연구 자체도 앞으로의 인공지능을 위해 발전을 이루어야한다. 하지만 언제까지나 우리가 추구하는 인공지능의 범위는 서비스를 제공하는 주체인 server 뿐만 아니라 edge device까지 포함할 것이고, 그렇게 된다면 모델 일반화에 사용되는 데이터 증가와 함께 수반되는 모델 파라미터의 증가를 감당할 수 없는 것은 당연할 것이다. 따라서 continual learning이 시사하는 문제점은 바로 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;유기체의 knowledge 축적에 대한 natural한 특성을 고려해야한다.&lt;/li&gt;
  &lt;li&gt;실생활에서 마주하는 인퍼런스 환경은 학습 데이터가 포함하지 못하는 영역들이 더 많다.&lt;/li&gt;
  &lt;li&gt;따라서 미리 정해진 범위 내에서의 학습이 아닌, 지속적으로 변화하는 상황에 대한 학습법을 논하고자 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Continual learning의 대표적인 문제점은 매우 명확하게도 불안정한 학습법과 관련된 문제, 그리고 기존 representation에 대한 망각 문제로 이어진다. 사람은 강아지란 존재를 알고 있으면서 고양이라는 새로운 개체에 대한 정보를 얻는다고 하더라도 강아지에 대한 정보를 잊어버리지 않는다. 심지어, 어떠한 형태의 새로운 정보는 기존에 가지고 있던 다른 정보의 이해를 도우며 서로 강화되기도 한다. 그러나 딥러닝이 모방한 뉴럴 네트워크 체계는 한정된 수의 parameter를 가지고 intractable한 posterior를 모방하는 과정이기 때문에  필연적으로 습득 가능한 지식에 한계가 있으며, 무엇보다 explicit하게 이전 데이터에 대한 정보가 주어질 수 없는 implicit한 학습법이므로 새로운 정보 습득이 이전 지식에 미치는 영향을 독립화할 수 없다는 단점이 있다. 이를 바로 ‘Catastrophic forgetting’이라고 부르며, 단순히 새로운 정보를 조금 학습했을 뿐인데도 기존 representation이 빠르게 붕괴하는 문제를 일컫는다. 그렇다고 해서 무작성 새로운 정보에 대한 습득을 막을 수는 없는 노릇이다. 간단한 설명을 위해 다음과 같이 면접 상황을 가정해보겠다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;면접관 :&lt;/strong&gt; A씨와 B씨, 각각 본인의 장단점을 말씀해주세요.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;A :&lt;/strong&gt; 저는 새로운 환경에 빠르게 적응할 수 있다는 점이 장점입니다! 하지만 저는 기억력이 단점입니다. 그래도 제가 가진 강점으로 이를 잘 극복할 수 있습니다!&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;B :&lt;/strong&gt; 저는 가진 경험이 많아, 새로운 환경에서도 이를 토대로 적합한 결정을 내릴 수 있다는 점이 장점입니다! 하지만 저는 적응력이 단점입니다. 그래도 제가 가진 경험으로 이를 잘 극복해낼 수 있습니다!&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;A라는 사람은 새로운 환경에 빠르게 적응할 수 있다는 점이 강점이지만, 만약 기억력이 정말 부족한 사람이라면 (극단적인 경우를 가정하여 1시간으로 하자), 하루 전에 가르쳤던 업무 내용도 기억하지 못하기 때문에 했던 교육을 계속 다시 받아야한다는 점이 문제가 될 수 있다. 반면 B라는 사람은 가진 경험이 많아 지식이 풍부하다는 점이 강점이지만, 새로운 환경에 적응력이 정말 부족한 사람이라면 (극단적인 경우를 가정하여 단순 업무 하나를 가르치는데 2년이 걸린다고 하자), 업무 환경이 급격하게 바뀌는 상황에서는 큰 문제가 될 수 있다.&lt;/p&gt;

&lt;p&gt;이처럼 &lt;strong&gt;continual learning&lt;/strong&gt;도 딜레마가 존재한다. 새로운 데이터가 포함된 task에 최적화된 알고리즘을 짜게 된다면 기존 데이터가 포함된 tasks에 대한 성능이 바닥을 치게될 것이고, 그렇다고 해서 기존 데이터가 포함된 task를 잊지 않게끔 알고리즘을 짜게 된다면 새로운 데이터가 포함된 task에 대한 적응력이 현저히 줄어들 것이다. 이러한 딜레마를 “&lt;strong&gt;Learning plasticity and Memory stability&lt;/strong&gt;”라고 부르며, plasticity는 빠른 적응력에 해당되는 단어, memory stability는 기존 지식 보존과 관련된 단어라고 생각해볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;continual-learning의-방법론들&quot;&gt;Continual learning의 방법론들&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4fae18b7-61c7-4066-8716-eb0305e4ec81&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;단순히 plasticity 혹은 memory stability 사이에 균형을 맞춘다는 점만 고려하면, 모든 task 및 domain을 내포하는 continous한 상황 변화에 대해 일반화가 가능한 학습법을 추구하는 것이다. 가장 단순한 approach를 한다고 생각하면 기존 training sample에 새로운 task의 데이터셋을 추가하여 모두 학습하는 방법을 생각해볼 수 있지만, 연산 효율성이나 학습 데이터셋 자체가 가지는 라이선스 및 사생활 문제를 극복할 수는 없다. 만약 정말 continual한 환경에서 내 데이터가 모두 기록된다고 생각하면 마냥 마음 편하게 AI 서비스를 사용할 수 있는 사람이 많지는 않을 것이다. 사실 continual learning은 이러한 단순한 접근법을 사용하지 않고, &lt;strong&gt;사용할 수 있는 자원(Usable Resource)을 최대로 활용하는 선&lt;/strong&gt;에서 소개된다. 따라서 continual learning이 추구하는 이상에 있는 방법은 오직 새로운 training sample만 가지고 학습하는 방법이라고 볼 수 있다. 위의 그림 중 &lt;strong&gt;C&lt;/strong&gt;에서 볼 수 있듯이 continual learning은 딥러닝 학습 프레임워크에서 어느 부분에서 솔루션을 찾냐에 따라 방법론을 크게 분류할 수 있다. 각각의 방법을 다시 소개하겠지만 대충 한 문장으로 요약해보면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Replay : 기존 task의 샘플을 다시 가져와서 최적화에 활용해보자&lt;/li&gt;
  &lt;li&gt;Architecture : 학습되는 네트워크 구조를 적절히 변형하여 필요한 부분만 잘 학습시켜보자&lt;/li&gt;
  &lt;li&gt;Representation : 변하는 상황에 대해서 robust한 representation을 잘 학습시켜보자&lt;/li&gt;
  &lt;li&gt;Optimization : 기존 representation에 도움이 될 수 있는 방향으로 최적화를 진행해보자&lt;/li&gt;
  &lt;li&gt;Regularization : 학습 시 정규화를 통해 학습 안정성을 챙겨보자&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h1&gt;

&lt;p&gt;논문을 보게 되면 분야에 따라 공통적으로 해결하고자 하는 문제가 있고, 조금 더 디테일하게 들여다보면 논문 자체에서 해결하고자 하는 보다 세밀한 문제를 찾을 수 있다. Continual learning을 메인으로 하는 페이퍼 등등에서 자주 보이는 notation에 대해 정리를 하고 넘어가는 것이 좋을 것 같다. Continual learning은 다양한 data distribution으로부터 학습하는 방법을 소개한다. 이때 학습되는 network를 $\theta$에 의해 parameterized되었다고 언급하고, $t$번째 task의 속하는 학습 배치는 $\mathcal{D}_{t,b} =\{\mathcal{X}_{t,b}, \mathcal{Y}_{t,b}\}$로 표현한다. 해당 notation은 일반적으로 continual learning이나 domain adaptation과 같은 task에서 주로 사용되는 편인데, input data인 $\mathcal{X}_{t,b}$와 그에 상응하는 ground truth(data label) $\mathcal{Y}_{t,b}$ 를 포함하는 data 묶음을 표현하는 방식이다. 따라서 &lt;strong&gt;data distribution의 변화&lt;/strong&gt;라는 상황이 포함하는 범위는 data 영역에서의 shift(fine detail부터 semantic한 부분까지 포함)가 될 수도 있으며 label 영역에서의 shift(long-tail problem, subclass 문제 등등)가 될 수도 있다. Task의 변화 $t \in \mathcal{T} = \{1, \cdots, k\}$는 task를 나타내는 지표로 구분할 수 있으며, batch의 변화에 대한 index는 $b \in \mathcal{B}_t$ 로 표현된다. 이를 통해 각각의 “Task”를 training sample인 $\mathcal{D}_t$가 대표하는 distribution인 $\mathbb{D}_t := p(\mathcal{X}_t, \mathcal{Y}_t)$로 명시할 수 있게 된다. Continual learning이 내포하는 범위 자체는 다음과 같이 굉장히 다양하고, 따라서 &lt;strong&gt;“Continual Learning을 공부한다”&lt;/strong&gt;라는 점이 생각보다 다양한 setting을 포함한다고 볼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;continual-learning의-다양한-시나리오들&quot;&gt;Continual learning의 다양한 시나리오들&lt;/h1&gt;

&lt;p&gt;딥러닝 모델에 input으로 들어오게 되는 각각의 배치 단위에 따라서, 그리고 task의 구분 여부에 따라 일반적인 continual learning 시나리오를 구분하곤 한다. 각 시나리오를 소개함에 앞서 training/testing에 대한 셋팅은 앞서 소개한 problem statement의 모든 notation을 따르며, &lt;strong&gt;‘요구되지 않는다(Not required)’&lt;/strong&gt;라고 표현한 부분은 필수가 아니라는 점에 대해 말하는 것이지, 해당 시나리오에서 무조건 없어야 되는 것은 아니라는 것을 짚고 넘어가도록 하겠다. 또한 굳이 명시되어있지 않다면 각 task는 모두 충분한 갯수의 라벨링된 학습 데이터를 가지며, 이는 곧 continual learning에서 굳이 명시하지 않는다면 supervised learning setting을 가정한다는 점을 시사한다. 아래에 표현한 시나리오의 큰 구분을 제외하고도 low-shot setting, open-world setting, un-/self-supervised setting 등등 다양한 연구들이 제안되었지만 그런 논문들 모두 커버하려면 글이 팔만대장경 급으로 길어지기 때문에 간단하게만 분류하도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;instance-incremental-learning-iil&quot;&gt;&lt;strong&gt;Instance-Incremental Learning (IIL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;모든 학습 sample은 같은 task에 속하며, 배치 단위로 들어온다. 즉 학습 단계에서 단일 task의 sample이 배치 단위로 들어오는 상황이며, ${{\mathcal{D}_{t, b}, t}_{b \in \mathcal{B}_t}}_{t = j}$  인퍼런스 단계에서도 동일한 task를 가정한다($\{p(\mathcal{X}_t)\}_{t = j}$). 이는 곧 일반적인 딥러닝 학습 알고리즘에서의 셋팅과 동일한 상황에서 input dataset만 continual하게 들어오는 상황을 가정하며, 단일 task를 가정하기 때문에 task specific identification($t$)이 요구되지 않는다 (Not required).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4fae18b7-61c7-4066-8716-eb0305e4ec81&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;domain-incremental-learning-dil&quot;&gt;&lt;strong&gt;Domain-Incremental Learning (DIL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;IIL과는 다르게 단일 task가 아닌 여러 tasks가 존재하는 상황을 전제로 한다. 모든 task들은 동일한 data label space를 가지고 input distribution만 다르다. 예컨데 CIFAR10, CIFAR10-C와 같은 관계라고 생각해볼 수 있다(같은 label space (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)를 가지지만 input distribution만 달라짐). 학습 단계에서는 여러 task의 sample이 배치 단위로 들어오는 상황을 가정하고, 이를 식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
    \{\mathcal{D}_t, t\}_{t \in \mathcal{T}}; p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \text{ and }\mathcal{Y}_i = \mathcal{Y}_j \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;그리고 단일 task가 아님에도 불구하고 인퍼런스 단계에서 IIL과 마찬가지로 task specific identification($t$)이 요구되지 않는다($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$) (Not required).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/c98e7258-8d7c-4775-8c50-c56d73f2aaff&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;task-incremental-learning-til&quot;&gt;&lt;strong&gt;Task-Incremental Learning (TIL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;각 task는 서로 disjoint한 data label space를 가진다(완전히 겹치지 않는 상황). 또한 이 셋팅에서는 task specific identity($t$)에 대한 정보가 training 및 testing에서 제공된다. 학습 단계에서의 상황을 식으로 표현하면:&lt;/p&gt;

&lt;p&gt;[
    \{\mathcal{D}_t, t\}_{t \in \mathcal{T}}; p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \text{ and }\mathcal{Y}_i \cap \mathcal{Y}_j=\emptyset \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;위와 같으며 인퍼런스 단계($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$)에서 이번에는 $t$를 사용할 수 있게 된다 (Available).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/fffcb6fc-4148-403c-b7e7-fc2042d94788&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;class-incremental-learning-cil&quot;&gt;&lt;strong&gt;Class-Incremental Learning (CIL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;각 Task는 서로 disjoint한 data label space를 가진다(완전히 겹치지 않는 상황). 그리고 Task specific identity($t$)는 오직 학습 시에만 사용 가능하다.&lt;/p&gt;

&lt;p&gt;[
    \{\mathcal{D}_t, t\}_{t \in \mathcal{T}}; p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \text{ and }\mathcal{Y}_i \cap \mathcal{Y}_j=\emptyset \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;따라서 기본 학습 단계에서는 위의 식을 그대로 따르며, 인퍼런스 단계($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$)에서는 $t$를 제한하게 된다 (Unavailable).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b05ba0f7-6fe5-4acc-b034-965f729aa46c&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;task-free-continual-learning-tfcl&quot;&gt;&lt;strong&gt;Task-Free Continual Learning (TFCL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;각 Task는 서로 disjoint한 data label space를 가진다(완전히 겹치지 않는 상황). 그리고 Task specific identity($t$)를 학습 그리고 인퍼런스 단계 둘 중 하나에서 절대로 사용할 수 없다.&lt;/p&gt;

&lt;p&gt;[
    \{\{\mathcal{D}_{t, b}, t\}_{b \in \mathcal{B}_t}\}_{t \in \mathcal{T}}; p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \text{ and }\mathcal{Y}_i \cap \mathcal{Y}_j=\emptyset \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;따라서 기본 학습 단계에서는 기존 식과는 다르게 task 구분이 없이 배치 단위로 input을 구분하게 되며, 인퍼런스 단계($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$)에서는 $t$를 일부 상황에서 제한하게 된다 (Optionally available).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/7c9f7877-26e2-4a8b-9cbd-529647c7f838&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;online-continual-learning-ocl&quot;&gt;&lt;strong&gt;Online Continual Learning (OCL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;각 Task는 서로 disjoint한 data label space를 가진다(완전히 겹치지 않는 상황). 그리고 학습 시 각 task의 모든 샘플은 one-pass로 최적화에 사용된다 (한번 최적화에 사용된 데이터 배치는 다시 input으로 사용되지 않음).&lt;/p&gt;

&lt;p&gt;[
    \{\{\mathcal{D}_{t, b}, t\}_{b \in \mathcal{B}_t}\}_{t \in \mathcal{T}},\lvert b \rvert = 1; p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \text{ and }\mathcal{Y}_i \cap \mathcal{Y}_j=\emptyset \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;따라서 굳이 task를 구분하지 않더라도 모든 배치는 single-stream framework를 따르기 때문에 task에 무관한 학습이 된다.  그리고 인퍼런스 단계($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$)에서는 $t$를 일부 상황에서 제한하게 된다 (Optionally available).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/58d37cbd-362f-4192-8604-57113dfcf30c&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;blurred-boundary-continual-learning-bbcl&quot;&gt;&lt;strong&gt;Blurred Boundary Continual Learning (BBCL)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Task의 경계선이 애매한(blurry) 상황을 가정하고, disjoint(교집합이 아예 없는) data label이 아닌 일부 겹치는 상황에 대한 시나리오를 의미한다.&lt;/p&gt;

&lt;p&gt;[
    \{\mathcal{D}_t, t\}_{t \in \mathcal{T}}; p(\mathcal{X}_i) \neq p(\mathcal{X}_j), \mathcal{Y}_i \neq \mathcal{Y}_j \text{ and }\mathcal{Y}_i \cap \mathcal{Y}_j=\emptyset \text{ for } i \neq j
]&lt;/p&gt;

&lt;p&gt;즉 data label space가 일부 겹쳐야한다는 점만 제외하면 class-incremental learning과  모두 동일하다. 따라서 인퍼런스 단계($\{p(\mathcal{X}_t)\}_{t \in \mathcal{T}}$)에서는 $t$를 사용할 수 없다(Available).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/7a2516f1-128c-4dd7-985d-7410582eb953&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;continual-pre-training-cpt&quot;&gt;&lt;strong&gt;Continual Pre-training (CPT)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;사전 학습되는 data가 continual(sequence)로 도착하는 상황을 의미한다. 주된 목적은 downstream task의 성능을 올리는 것이다(good representation).&lt;/p&gt;

&lt;p&gt;[
    \{ \mathcal{D}_t^{pt}, t\}_{t \in \mathcal{T}^{pt}}, \text{ followed by a downstream task }j
]&lt;/p&gt;

&lt;p&gt;이번에는 pre-train 단계를 continual한 상황으로 가정했기 때문에 testing 단계에서는 단일 downstream task($t = j$)를 기준으로 삼게 된다. 따라서 downstream task의 task label인 $\{p(\mathcal{X}_t)\}_{t = j}$는 굳이 요구되지 않는다 (Not required).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1c81f81f-b4ed-449e-bc3b-b87efe9ab7ae&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;continual-learning에서의-evaluation-metric&quot;&gt;Continual Learning에서의 Evaluation metric&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/dd6bfb8b-286c-44bc-9bd4-622202d6137a&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;특정 방법이 다른 방법보다 &lt;strong&gt;‘유리하다’.&lt;/strong&gt; 혹은 contribution이 충분하다고 증명할 수 있는 방법은 performance가 우수한 것을 보이는 것이다. 이처럼 deep learning에서 성능 평가를 내릴 수 있는 각 상황별 evaluation metric이 굉장히 중요하게 작용하는데, continual learning과 같이 여러 상황이 동시에 주어지는 환경에서는 evaluation을 어떤 방식으로 진행할까? 앞서 continual learning을 소개할 때 간단하게 언급했던 딜레마인 “&lt;strong&gt;Learning plasticity and Memory stability&lt;/strong&gt;”을 떠올려보자. Continual learning이 가지는 목적은 결국 &lt;strong&gt;‘이전 representation을 얼마나 잘 유지하면서 현재 task의 성능을 잘 올릴 수 있는가?’&lt;/strong&gt;로 정리할 수 있다.  따라서 continual learning에서의 metric은 세가지 측면으로 정리될 수 있다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;기존(Old) task들의 성능 평가 &lt;strong&gt;: Overall performance&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;기존(Old) task에 대한 memory stability &lt;strong&gt;: Memory stability&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;새로운(New) task의 성능 평가 &lt;strong&gt;: Learning plasticity&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;언뜻 보면 결국 1번이 2번이랑 같은 의미 아니냐고 할 수 있지만 각각의 evaluation에 대해 명확하게 살펴보면 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;overall-performance&quot;&gt;Overall performance&lt;/h3&gt;

&lt;p&gt;기존 task들의 성능 평가는 보통 avaerage accuracy (AA)나 average incremental average (AIA)로 나타낸다. 예컨데 $k$번째 task까지 incremental learning 이후 $j$번째 task의 accuracy 측정 값을 $a_{k, j} \in [0, 1]$ 이라고 하자. $a_{k, j}$를 측정하기 위해 상정하는 output space는 $j$번째 class만 포함할 수도, 아니면 
$j$번째 class까지의 모든 output space를 포함할 수도 있다. 예컨데 classifier head를 하나만 사용하는 CIL의 경우에는 $\cup_{i=1}^k \mathcal{Y}_i$를, classifier head를 여러 종류를 사용하는 TIL의 경우에는 $\mathcal{Y}_j$를 output space로 잡는다. 이때의 AA와 AIA는 다음과 같은 식으로 정리된다.&lt;/p&gt;

&lt;p&gt;[
    \text{AA}_k = \frac{1}{k} \sum_{j=1}^k a_{k, j},~\text{AIA}_k = \frac{1}{k}\sum_{i=1}^k \text{AA}_i
]&lt;/p&gt;

&lt;h3 id=&quot;memory-stability&quot;&gt;Memory stability&lt;/h3&gt;

&lt;p&gt;기존 task를 얼마나 잘 기억하고 있는지 측정하는 지표로는 보통 forgetting measure (FM)이나 backward transfer (BWT)를 사용한다. 전자의 경우 각 task에 대한 잊음(forgetting) 정도 $f_{j, k}$를 &lt;strong&gt;과거에 획득한 성능 중 최대치&lt;/strong&gt;와 &lt;strong&gt;현재 성능과의 차이&lt;/strong&gt;를 통해 계산한다.&lt;/p&gt;

&lt;p&gt;[
    f_{j, k} = \underset{i \in \{1,\cdots, k-1\}}{\max} (a_{i,j} - a_{k, j}),~\forall j &amp;lt; k
]&lt;/p&gt;

&lt;p&gt;그리고 $k$번째의 FM은 old tasks에 대한 forgetting 전반에 대한 평균으로 측정된다.&lt;/p&gt;

&lt;p&gt;[
    \text{FM}_k = \frac{1}{k-1} \sum_{j=1}^{k-1} f_{j, k}
]&lt;/p&gt;

&lt;p&gt;후자인 BWT의 경우에는 $k$번째 task가 이전 $k-1$번째 task까지의 성능에 주는 영향을 평균 내어 계산한다.&lt;/p&gt;

&lt;p&gt;[
    \text{BWT}_k = \frac{1}{k-1}\sum_{j=1}^{k-1} (a_{k, j}-a_{j, j})
]&lt;/p&gt;

&lt;p&gt;보통의 경우 $k$번째 task의 학습 이후 측정된 accuracy $a_{k, j}$가 각 task 학습 직후 측정된 accuracy $a_{j, j}$보다 낮기 때문에 negative BWT의 정도가 forgetting을 반영한다고 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 overall performance, memory stability 모두 old task에 대한 성능을 반영하는 점에서 공통점을 가지지만, &lt;strong&gt;overall performance&lt;/strong&gt;는 &lt;strong&gt;성능 자체&lt;/strong&gt;를, &lt;strong&gt;memory stability&lt;/strong&gt;는 &lt;strong&gt;성능 변화&lt;/strong&gt;를 기록한다는 점에서 차이점이 있다.&lt;/p&gt;

&lt;h3 id=&quot;learning-plasticity&quot;&gt;Learning plasticity&lt;/h3&gt;

&lt;p&gt;새로운 task를 얼마나 빠르게 학습하는지에 대한 지표가 되는 learning plasticity는 일반적으로 instransience measure (IM) 그리고 forward transfer (FWT)로 측정한다. IM은 네트워크가 새로운 task를 잘 배우지 못하는 정도를 의미하게 되고, joint training performance와 continual learning performance 사이의 차이로 정의한다.&lt;/p&gt;

&lt;p&gt;[
    \text{IM}_k = a^\ast_k - a_{k, k}
]&lt;/p&gt;

&lt;p&gt;$a_k^\ast$는 기준이 되는 모델을 $k$번째 task까지의 데이터 $\cup_{j=1}^k \mathcal{D}_j$로 jointly(한꺼번에) 학습시켰을 때의 성능을 의미한다. 성능은 오직 $k$번째 데이터셋에 대해서만 구하게 된다. Jointly 학습시켰을 때보다 continual하게 학습을 시킬 경우 plasticity가 낮다면 해당 task에 대한 성능이 줄어들 것이기 때문에$(a^\ast_k &amp;gt; a_{k, k})$ , 해당 지표가 클수록 &lt;strong&gt;새로운 task에 잘 적응하지 못하는 정도로서 측정되는 것&lt;/strong&gt;이다. FWT는 이와는 반대로, 모든 old task가 $k$번째 task에 미치는 영향력을 평균으로 측정하게 된다.&lt;/p&gt;

&lt;p&gt;[
    \text{FWT}_k = \frac{1}{k-1} \sum_{j=2}^k (a_{j,j} - \tilde{a}_j)
]&lt;/p&gt;

&lt;p&gt;$\tilde{a}_j$는 기존이 되는 모델을 $j$번째 task 데이터셋 $\mathcal{D}_j$에만 학습시켰을 때의 성능을 의미한다. $k-1$번째까지 학습된 모델의 posterior가 prior로서 $k$번째 task를 마주하는 상황이 되었을 때, 해당 지표가 클수록 &lt;strong&gt;이전 task가 현재 task의 performance에 미치는 영향력이 크다&lt;/strong&gt;고 볼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;continual-learning-methods&quot;&gt;Continual learning Methods&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4e322b7f-4c95-4c91-8368-19f0a1b0949f&quot; width=&quot;1100&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 continual learning의 성능을 재는 방식은 $k$번째 task 자체에 대한 performance와, performance 변화 그리고 실제로 continual learning 방식의 효과성에 대한 입증으로 구성된다. 이를 위해 다양한 방향의 연구가 진행되었으며, 각각에 대해 윗부분에서 한 문장으로만 요약하고 넘어왔었다. 이를 도식화한 것이 바로 위에 보이게 되는 tree 구조가 되는데, 각 부분의 연구는 개별적으로 활발히 진행되었고, 모델 학습에서 타겟되는 부분에 따라 구분할 수 있었다. 모든 논문들을 이 글에서 다루기는 무리가 있을 것 같고, 간단한 논문 몇 개만 정리하는 것을 목표로 하는 중이다.&lt;/p&gt;

&lt;h3 id=&quot;regularization-based-approach&quot;&gt;Regularization based approach&lt;/h3&gt;

&lt;p&gt;일반적으로 생각해볼 수 있는 방법은 바로 old task와 new task 간의 균형을 위해 explicit한 regularization term을 더해주는 것이다 (아래 그림 참고).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/834fecf5-2f9a-4c51-8250-b176152e2c94&quot; width=&quot;750&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그림을 보면 알 수 있겠지만 정규화가 consistency하게 보고자 하는 위치에 따라 크게 두 방향으로 구분되는데, 어찌되었든 정규화를 위해서 공통적으로 old task에 대해 학습된 frozen model을 reference로 가지고 있어야한다는 점은 변하지 않는다.&lt;/p&gt;

&lt;p&gt;정규화의 가장 첫번째 방법론은 weight regularization으로, 네트워크 파라미터의 변화를 선택적으로 정규화를 하는 방법이 된다. 일반적인 구현은 loss function에 각 parameter의 변화에 대한 quadratic penalty $(\lvert \theta - \tilde{\theta} \rvert^2)$를 추가한다던지, 그게 아니라면 각 parameter의 중요도(contribution/importance)에 따라 penalty를 주는 방법이 있을 수 있다. 예컨데 가장 대표적인 방법으로는 각 파라미터가 구현하는 probability surface $p(\Theta)$를 라플라스 근사하는 방법이 있는데, 이를 이해하기 쉽게 풀어서 설명해보도록 하겠다. 모든 deep neural network가 구성하는 함수는 특정 distribution에 대한 input/output 관계로 규명된다. 예컨데 우리가 간단한 classification task를 하고자 한다면 input image $X$에 대해 대응되는 output label $Y$가 있을 것이고, 해당 관계는 probabilistic modeling인 $p_\Theta(Y \vert X)$로 정의된다. 이를 곧 parameter에 대한 posterior로 표현하곤 한다. 뉴럴 네트워크는 사실상 $L$개의 연속된 레이어가 이전 레이어의 output에 대해 distribution mapping을 하는 구조로 구성되었을 것이고, 따라서 우리는 네트워크 전체가 아니라 이를 각 파라미터 단위로도 생각해볼 수 있다 $p_{\theta^l}(Z_{l} \vert Z_{l-1})$. 하지만 고차원의 뉴럴 네트워크를 다룰 때 ground truth가 되는 각 latent modality의 distribution을 우리가 직접 알 수는 없다는 점이 real-world에 대한 근본적 어려움으로 작용한다. 하지만 만약 log-likelihood를 전체 dimension에 대해 구하지 않고 라플라스 근사를 하게 되면, 적어도 우리는 해당 분포의 local point에서의 분포 형태를 가우시안 분포로 구할 수 있게 된다. 바로 이러한 관점에서 탄생한 것이 &lt;a href=&quot;https://arxiv.org/pdf/1612.00796.pdf&quot;&gt;EWC&lt;/a&gt;라는 논문이며, 실제로는 라플라스 근사를 하는 과정에서 필요한 헤시안 matrix를 직접 구하는 방법 대신 FIM(Fisher Information Matrix)을 통해 간접적으로 구하게 된다.&lt;/p&gt;

&lt;p&gt;[
    H(D_k, \mu_k) \approx \mathbb{E}\left(\nabla_\theta \log p(\mathcal{D}_k \vert \theta^l)\nabla_\theta \log p(\mathcal{D}_k \vert \theta^l)^\top\right)\vert_{\theta = \mu_k}
]&lt;/p&gt;

&lt;p&gt;그렇게 되면 FIM은 결국 log-likelihood를 가우시안 근사를 했을때 분포의 curvature 정보를 담게 되는데, 이를 토대로 중요한 parameter와 중요하지 않은 parameter를 구분할 수 있는 척도가 된다. 물론 이처럼 weight importance를 정하는 방식이 FIM에만 방법론이 고정된 것은 아니다. &lt;a href=&quot;https://arxiv.org/pdf/1805.06370.pdf&quot;&gt;Online EWC&lt;/a&gt;에서는 새로운 task에 대한 학습 및 이를 기존 representation에 추가하는 모듈 형태를 제시하며 online 환경에서 task agnostic하게 업데이트될 수 있는 weight importance 방식을 제안하였으며, &lt;a href=&quot;https://arxiv.org/pdf/1703.04200.pdf&quot;&gt;Synaptic Intelligence(SI)&lt;/a&gt; 논문에서는 실제 다음과 같이 parameter에 따른 biological한 plasticity framework를 딥러닝 학습 과정에 도입함으로써 regularization을 진행한다.&lt;/p&gt;

&lt;p&gt;[
\Omega_k^\mu = \sum_{\nu &amp;lt; \mu} \frac{\omega_k^\nu}{(\Delta_k^\nu)^2 + \xi}
]&lt;/p&gt;

&lt;p&gt;Denominator(분모) term에 존재하는 $\Delta_k^\nu$는 task index $\nu$를 기준으로 각 파라미터의 변화에 따른 trajectory를 의미하며, 온라인 환경에서 $\omega_k^\nu$는 각 parameter의 loss에 대한 gradient에 parameter 변화를 곱한 값을 지속적으로 accumulation해서 구하게 된다. 요약하자면 각 파라미터 단위로 정리되는 중요도는 loss function에 대해 parameter의 기여도를 구한 값을 catastrophic forgetting을 방지하기 위해 parameter 변화의 trajectory로 정규화한다고 볼 수 있다. 이러한 방법들은 모두 weight parameter에 대해 quadratic penalty term을 간접적으로 사용한다는 점에서 공통점을 가진다.&lt;/p&gt;

&lt;p&gt;이러한 penalty term을 사용하지 않고 &lt;a href=&quot;https://arxiv.org/abs/1802.02950&quot;&gt;factorized rotation을 기반&lt;/a&gt;으로 parameter space를 FIM에diagonalize하는 방법이나,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/cb5897fa-7f43-4923-9982-9721f7377df2&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Batch normalization이 포함된 구조에 유리한 &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Continual_Learning_With_Extended_Kronecker-Factored_Approximate_Curvature_CVPR_2020_paper.pdf&quot;&gt;Kronecker-Factored Approximate Curvature&lt;/a&gt;이 제안되기도 하였다. 하지만 이런 기존 방법들은 모두 “이전에 학습된 old task parameter”를 기준으로 한다는 점에서 parameter 변화를 막는다는 공통적인 constraint를 가지고, 이는 곧 새로운 task에 adaptation이 적용되는 과정에서 보수적인 효과를 불러온다. 이러한 문제점을 해결하고자 expansion 및 renormalization 방법이 제안되기도 하였고, 이는 new task solution을 독립적으로 obtain한 뒤에, 이를 old model에 재배치하는 형태로 구현이 된다. &lt;a href=&quot;https://arxiv.org/pdf/1703.08475.pdf&quot;&gt;IMM(Incremental Moment Matching)&lt;/a&gt;이 초기 approach에 있는 논문인데, 논문 제목이랑 아래 figure에서도 볼 수 있듯이 old task와 new task 간의 moment matching을 통해 점진적으로 새로운 task의 representation을 추가해가는 전략을 취한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/212a1f79-600a-4ae0-8e42-25531486baee&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이후 논문으로는 &lt;a href=&quot;https://arxiv.org/abs/2002.06774&quot;&gt;ResCL&lt;/a&gt;(IMM에 추가로 combination coefficient를 학습 가능하게 바꾼 것), Online EWC로도 유명한 &lt;a href=&quot;https://arxiv.org/pdf/1805.06370.pdf&quot;&gt;P&amp;amp;C&lt;/a&gt;을 볼 수 있다. P&amp;amp;C에서 주가 되는 메소드는 추가 network에 학습된 task를 기존 network에 distillation하는 과정인데, 이때 weight consolidation을 formulation하는 과정에서 online EWC를 사용한다. &lt;a href=&quot;https://arxiv.org/abs/2110.12187&quot;&gt;AFEC(Active Forgetting)&lt;/a&gt; 논문의 경우에는 forgetting rate(시냅스가 새로운 task를 받아들일 때 이전 정보와 conflict되는 정도를 active하게 설정)를 제안하였다. 이는 새로운 knowledge가 transfer되는 과정에서 잠재적으로 기존 representation과 공존했을 때 발생하는 negative transfer 문제를 다루고자 하였다. ResCL이랑 비슷한 approach라고 생각할 수도 있는데, plasticity 및 stability 간의 trade-off(old task와 new task loss 사이의 trade-off) 간의 균형을 위해 low-error path간에 &lt;a href=&quot;https://arxiv.org/abs/2110.07905&quot;&gt;linear connector를 구성한 방법&lt;/a&gt;도 제안되었다. 물론 parameter의 변화 자체를 규정하기보다는 learning rate를 줄이는 &lt;a href=&quot;https://arxiv.org/pdf/1907.13322.pdf&quot;&gt;NPC&lt;/a&gt;와 같은 논문들도 제시되었다. Penalty를 통해 간접적으로 weight update를 막는 방법 대신 important neuron의 학습을 막음으로써(freeze) hard regularization을 채택한 방법들도 존재한다.&lt;/p&gt;

&lt;p&gt;정규화의 두번째 방법론은 function regularization으로, prediction function의 최종 output 혹은 그 중간의 output을 기준으로 정규화를 진행하는 방법이다. Weight 정규화랑은 관점이 조금 다른게 prediction layer에 대해 consistency를 보는 과정이 되므로 기존 model의 output을 teacher로 삼고 학습 output을 student로 맛아서 knowledge distillation을 진행하면서 새로운 task를 학습하는 것과 같다. 사실 조금 웃긴 관점인게 새로운 지식을 습득하는 주체는 학생이고 오히려 선생은 이전 지식을 유지하도록 가이드한다는 것이다. 가장 대표적으로는 &lt;a href=&quot;https://arxiv.org/abs/1606.09282&quot;&gt;LwF(Learning without Forgetting)&lt;/a&gt;을 예시로 들 수 있다. 이외에도 생성 모델을 기반으로 모델링하여 feature reconstruction의 변화를 줄이는 방법이라던지 replay-based method와 결합되어 이전 task의 샘플들을 정규화 과정에 도입하는 경우가 많다. Replay based approach는 바로 아래에서추가로 설명하도록 하겠다.&lt;/p&gt;

&lt;h3 id=&quot;replay-based-approach&quot;&gt;Replay based approach&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/a6d603d7-09fb-4b6f-8f5f-643c975fa9f9&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;새로운 task를 학습할 때 old data distribution을 recover하고 approximate하는 방법들도 생각해볼 수 있는데, 각각 replay하는 주체에 따라 세부적으로는 3가지로 구분한다.&lt;/p&gt;

&lt;p&gt;첫번째는 &lt;strong&gt;experience replay&lt;/strong&gt;다. 이는 작은 memory buffer를 가지고 여기에 약간의 이전 task에 대한 training sample을 저장하는 형태로 구현된다. 물론 당연하게도 저장 공간에는 제약이 있기 때문에, 얼마나 많은 정보를 저장할 것인지에 대한 문제를 해결하는 것이 주된 해결 방향으로 작용한다. 구성하는 과정(construction)에서 샘플 선정이 중요하며, 학습 과정마다 이를 업데이트하는 방식도 고려해야한다. 초창기 연구인 &lt;a href=&quot;https://arxiv.org/abs/1902.10486&quot;&gt;reservoir sampling 방법&lt;/a&gt;은 가장 간단하게 sample selection 과정마다 고정된 갯수만큼의 old sample을 유지하고 나머지를 랜덤하게 대체하는 방식을 사용한다. &lt;a href=&quot;https://arxiv.org/abs/1706.08840&quot;&gt;Ring Buffer sampling 방법&lt;/a&gt;은 이를 보다 발전시켜서 old training sample을 새로이 저장하는  과정마다 class마다 같은 샘플 수가 유지되도록 한다. &lt;a href=&quot;https://arxiv.org/abs/1611.07725&quot;&gt;Mean-of-Feature 방법&lt;/a&gt;에서는 각 class의 특징자 벡터의 평균을 일종의 prototype로 간주하여 이와 가장 closest(유사한) 샘플을 저장하는 방식을 사용하였다.&lt;/p&gt;

&lt;p&gt;간단한 예시들을 열거하는 형태로 소개했는데, 이러한 내용들을 간단하게 보면 알 수 있듯이 샘플 선정 기준은 따로 정해진 것은 없고 empirical한 것을 알 수 있다. 예컨데 $K$-means를 사용할 수도 있으며, plane distance나 샘플의 entropy를 기준으로 thresholding할 수도 있다. 그러나 이러한 방법들은 모두 어느 정도 적당한 성능 선에서 가능성만 보여주었다. 이보다 좀 더 발전한 형태로는 gradient 기준으로 다양성을 최대화하는 방법이나(&lt;a href=&quot;https://arxiv.org/abs/1903.08671&quot;&gt;Gradient based sample selection&lt;/a&gt;) entity간의 관계 조건인 cadinality constraint를 task performance와 &lt;a href=&quot;https://arxiv.org/pdf/2006.03875.pdf&quot;&gt;연관짓는 방법&lt;/a&gt;, batch 단위로 gradient 유사성을 보는 방법이 있다.  또한 최적화가 가능한 방법으로 latent decision boundary를 조정하는 등 고정되지 않은 알고리즘으로서 정의된 approach가 발전하기 시작했다.&lt;/p&gt;

&lt;p&gt;이와 parallel하게 샘플 저장 효율성과 관련된 방법도 동시에 발전하기 시작했는데, vector quantize 방법을 포함하여 sample point를 압축하여 저장하기도 했으며 augmentation을 사용하여 한정된 샘플에 다양성을 부여하기도 하였다. 추가적으로는 의도적으로 forgetting boundary(잊기 쉬운 샘플들)을 adversarial하게 합성하여 replay 효과를 높이는 방법도 존재한다.  이후에도 많은 연구들이 진행되었다. 그런데 여기서 들 수 있는 의문은 “그럼 굳이 샘플 저장하지 말고 feature 저장하면 안되는가?”인데, 이게 바로 다음에서 소개할 feature replay에 해당된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feature replay&lt;/strong&gt;는 experience replay에 비해 용량 측면에서 이점이 있으나 feature extractor가 업데이트되면서 같은 샘플에 대해서도 representation이 달라진다는 문제가 발생한다. 이는 곧 feature level에서의 replay가 catastrophic forgetting 위험성을 수반하는 이야기로 흘러가게 된다. 이러한 문제를 해결하려는 방법으로 old model과 new model 간의 feature distillation을 사용하게 된다. 또다른 approach는 experience replay를 기준으로 feature representation(평균이나 표준편차와 같은 통계)를 복구하는 방법을 사용하기도 한다. &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Toldo_Bring_Evanescent_Representations_to_Life_in_Lifelong_Class_Incremental_Learning_CVPR_2022_paper.pdf&quot;&gt;RER 논문&lt;/a&gt;에서는 old sample의 representation을 저장해두고 이를 업데이트하면서distribution shift를 예측하는 방식을 사용한다. 이외에는 초기 layer를 고정시킨 채 중간 feature를 추출하여 이를 후반 layer를 업데이트하는 과정의 replay sample로 사용하는 방법도 있다.&lt;/p&gt;

&lt;p&gt;마지막 방법으로는 &lt;strong&gt;Generative replay&lt;/strong&gt;가 있는데, 이는 old task의 학습 데이터를 생성 모델을 사용하여 replay에 사용하는 전략을 취한다. 예컨데 새로운 task에 대해 생성된 데이터와 old task에 대해 생성된 데이터 간의 consistency를 사용한다던지 하는 방식이 있을 수 있다. VAE, GAN과 같은 다양한 구조가 사용된다. 방법 자체에 큰 차이는 없어서 간단하게 이쯤 언급하도록 하겠다. 결국 replay-based approach의 공통점은 task가 학습되면서 기존 학습에 사용되던 input sample 혹은 representation에 대한 정보를 직/간접적으로 사용하여 catastrophic forgetting을 방지한다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;optimization-based-approach&quot;&gt;Optimization based approach&lt;/h3&gt;

&lt;p&gt;앞서 소개했던 regularization 방법이나 replay는 결국 기존에 학습된 데이터를 implicit하게 혹은 explicit하게 활용할 수 있는 방법이었다. 이러한 “기존” 이라는 키워드에서 벗어나 explicit하게 최적화 구조를 바꾸거나 조정하는 방법이 소개되었으며, 이를 곧 optimization based approach라는 큰 틀로 묶을 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/8549f8d4-0398-4055-bef9-fdae46caf9ec&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;가장 흔한 아이디어는 gradient projection(그래디언트 사영)에 해당된다. 몇몇의 replay-based approach는 experience replay의 업데이트 방향에 따라 parameter update를 align하는 방식을 채택하였는데, 이를 통해 새로운 task에 대한 parameter 업데이트가 이루어질 때 기존 input이 구성하는 implicit한 공간 및 gradient 공간을 유지하는 효과를 가지고 올 수 있었다. 이러한 컨셉에서 replay라는 관점을 제거하여, &lt;a href=&quot;https://arxiv.org/abs/1810.01256&quot;&gt;OWM&lt;/a&gt;이나 &lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/20634&quot;&gt;AOP&lt;/a&gt;에서는 parameter update를 previous input space를 기준으로 orthogonal한 방향(input space를 유지하는 방향)으로 업데이트하는 전략을 취했다.  두 방법은 input space에 대한 orthogonality를 전략으로 취한 반면, &lt;a href=&quot;https://arxiv.org/abs/1910.07104&quot;&gt;OGD&lt;/a&gt;는 기존 학습 시의 parameter update를 보존한 뒤, 이후 task의 gradient optimization 방향을 이에 orthogonal한 방향으로 조정하는 전략을 통해 input space 대신 gradient space를 사용하게 된다. 이전에 소개했던 regularization based method인 Bayesian weight regularization과 gradient projection을 통합시킨 논문도 소개되었다.&lt;/p&gt;

&lt;p&gt;여러 task에 대해 robust한 optimization이라고 하면 비슷한 관점으로 “meta-learning”을 떠올려볼 수도 있다. 아니나 다를까 meta-learning도 continual 방법론에 속하게 된다. &lt;a href=&quot;https://arxiv.org/abs/1905.12588&quot;&gt;OML&lt;/a&gt;은 메타러닝 학습 프레임워크를 제안함으로써 연속적으로 계산되는 샘플 input에 대해 학습 과정에서의 간섭을 줄이면서 online update에 도움이 되는 방법을 제안하였다. &lt;a href=&quot;https://arxiv.org/abs/2002.09571&quot;&gt;ANML&lt;/a&gt; paper에서는 이를 보다 확장하여 점진적으로 증가하는 task에 대해 context-dependant 정보를 함수화하여 특정 뉴런을 활성화하는 방식으로 학습을 진행하였다.&lt;/p&gt;

&lt;p&gt;다른 방법론으로 고려해볼 수 있는 것은 generalization(일반화) 관점인데, 바로 loss landscape를 안정적으로 만드는 것이다. Task 및 Domain 관점에서 loss landscape가 안정적인 형태(curvature가 낮은, flat한 local minima)를 가질수록 adaptation에 도움이 된다는 관점에서 출발하게 된다(아래 그림 참고).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/edf15a22-0b2c-452c-9ba9-61ab316f58db&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이러한 loss landscape를 고려하는 방법들은 대부분 optimizer에 관한 연구로 구성되며 &lt;a href=&quot;https://arxiv.org/abs/2006.06958&quot;&gt;SGD&lt;/a&gt;나 &lt;a href=&quot;https://arxiv.org/abs/2103.07113&quot;&gt;Adam optimizer&lt;/a&gt;에 솔루션을 제공한다.&lt;/p&gt;

&lt;h3 id=&quot;representation-based-approach&quot;&gt;Representation based approach&lt;/h3&gt;

&lt;p&gt;사실 일반화 관점이라면 한번 더 고려해볼 수 있는 것이 robust representation이다. Domain generalization에서 얻고자 하는 효과에 가까운 솔루션이라고 볼 수 있다. 각 task에 specific한 representation이 아닌 일반화에 가까운 representation을 얻는 과정을 sparse(넓은 범위의 확률 분포를 커버하는) representation이라고 부른다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/5de8927a-6c35-4de7-b254-c82517711513&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;최근 meta-training이나 self-supervised learning(SSL)이 보여주는 promising한 결과들을 기반으로 많은 연구가 추가로 진행되었다. 최근 들어 transformer 기반 모델이 발전하기 시작하면서 large-scale pre-training의 효과 또한 입증되기 시작하였고, 결국 adaptation 관점보다는 generalization 관점에서 보다 넓은 범위의 representation을 커버하자는 의도의 approach 또한 continual learning에 제안되기 시작하였다. SSL이나 large-scale pre-training은 사실상 서로 같은 목적 및 학습 형태를 공유하는 점이 많고, 이는 large scale dataset이 라벨링이 힘들다는 점을 들 수 있다. 차이점이라고 한다면 SSL task는 주로 downstream task로 approch의 당위성을 증명한다.&lt;/p&gt;

&lt;p&gt;SSL을 사용한 방법들로는 다음과 같은 논문들이 제안되었다. SSL approach를 사용한 논문들의 공통점은 “SSL로 학습된 representation”일수록 catastrophic forgetting에 강인하다는 점이다.  &lt;a href=&quot;https://arxiv.org/abs/2110.06976&quot;&gt;LUMP&lt;/a&gt;는 old task와 new task 간의 representation이 연속이라는 가정 하에 interpolation을 진행하는 방식을 사용하였다. &lt;a href=&quot;https://arxiv.org/abs/2203.12710&quot;&gt;MindRed&lt;/a&gt;는 저장된 old training sample을 사용하여 replay experience를 기존 representation으로부터 decorrelate시키는 방법을 사용했고, 이는 곧 새로운 task를 통한 학습이 보다 다양한 데이터로부터 오는 일반화 효과로 접근한 것을 알 수 있다. 다른 방법들로는 self-supervised loss를 distillation 방법론과 결합하여 현재의 representation을 이전 state로 mapping하거나 모델 간의 mapping을 하는 전략을 취하였다.&lt;/p&gt;

&lt;p&gt;Pre-training을 도입한 방법들은 주로 continual learning 과정에서 얻는 representation의 이점보다는 사전 학습된 representation을 활용하여 여러 downstream continual setting에서 좋은 성능을 내고자 하는 것이 목적이다. 주로 대량의 데이터셋에 대해 학습된다던가, 보다 큰 파라미터 수를 가지는 larger model에 대해 학습된다던가 아니면 contrastive loss 등등 SSL 전략들과 함께 사용되었을 때 좋은 성과를 보였다. 이러한 방법론에서 가장 주된 문제점 및 해결 사항으로 제안되는 것이 이렇게 사전 학습된 representation을 continual 환경에서 어떻게 잘 전달하느냐인데, 이러한 전략은 사전 학습된 backbone 파라미터가 고정된 경우와 고정되지 않은 경우로 나뉜다. 고정된 backbone을 가지는 경우 parameter-efficient tuning 전략인 &lt;a href=&quot;https://arxiv.org/abs/1912.13503&quot;&gt;Side-Tuning&lt;/a&gt;과 같이 기존 parameter와 parallel하게 학습될 수 있는 방법이 제안되었다. &lt;a href=&quot;https://arxiv.org/abs/2206.00388&quot;&gt;TwF&lt;/a&gt;(Transfer without forgetting)은 마찬가지로 별도의 네트워크를 학습하는데, backbone의 knowledge를 레이어별로 distillation한다는 전략을 취한다. 이외에도 생성 모델의 representation의 각 layer로부터 task-specific 파라미터를 학습하는 구조나, Adapter를 기반으로 사전 학습된 transformer를 tuning하는 방법이 제안되기도 했다.&lt;/p&gt;

&lt;p&gt;앞서 continual learning과 관련된 여러 task를 소개하는 과정에서 continual pre-training (CPT)를 언급했었는데, 사전 학습 시 사용되는 대용량의 학습 데이터를 일반적으로는 incremental한 방법으로 획득되다보니 마찬가지로 학습 과정에서 continual하게 학습한 뒤 이에 downstream performance가 향상되도록 하는 것이 주요 목적이다. 기존 연구 결과들에 따르면 continual learning을 진행하는 VL model에 대해 supervised 학습법보다 self-supervised 학습법이 promising한 결과를 보인다는 결과가 나타났다.&lt;/p&gt;

&lt;h3 id=&quot;architecture-based-approach&quot;&gt;Architecture based approach&lt;/h3&gt;

&lt;p&gt;위에서 언급한 방법들은 대부분 공유된 parameter(단일 model을 하나의 파라미터 단위로 생각했을 때)를 여러 task에서 잘 활용하는 방법에 대한 해결책이었다. 그러나 이렇게 단일 파라미터를 여러 task에서 employ할 경우 근본적으로 interference 문제를 해결하기 어렵다는 단점이 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/e920912e-253f-406c-a179-33dbaaad4134&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이런 기존 방법들과는 다르게 task-specific parameter를 공유되는 파라미터와 독립적으로 학습함으로써 간섭 문제를 해결하고자 한 방법이 바로 architecture로 접근한 논문들이다. Approach는 네트워크 구조가 고정되었느냐 아니냐에 따라 parameter-isolation 방법과 dynamic architecture 방법으로 구분된다. 그러나 최근에는 이런 식으로 분류하지 않고 parameter allocation, model decomposition 그리고 modular network 이렇게 세 가지로 분류해서 보는 듯하다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter allocation&lt;/strong&gt;은 parameter isolation을 확장시킨 개념으로, 네트워크 전반에 걸쳐 각 task에 기여하는 parameter 부분집합(subspace)를 분리한다. 이때 architecture는 고정되어있을 수도 있으며 dynamic하게 변할 수도 있다. 고정된 네트워크 구조를 차용한 여럿 방법들의 경우에 학습하고자 하는 뉴런이나 파라미터를 각 task마다 분리해줄 수 있는 &lt;a href=&quot;https://www.notion.so/Continual-Learning-92b3c34e1e46474981dda12613c4639d?pvs=21&quot;&gt;binary mask를 explicit하게 학습하는 형태&lt;/a&gt;를 취한다. 이를 조금 다르게 틀어서 중요한 뉴런이나 파라미터자체를 identify하는 방법들이 사용되기도 하는데, &lt;a href=&quot;https://arxiv.org/abs/1711.05769&quot;&gt;iterative pruning&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1903.04476&quot;&gt;activation value&lt;/a&gt; 혹은 &lt;a href=&quot;https://arxiv.org/abs/1905.11614&quot;&gt;uncertainty estimation&lt;/a&gt;이 이 방법들에 속한다. 네트워크가 담을 수 있는 용량이 한정되어있기 때문에, task가 진행되면 진행될수록 freeze하지 않은 parameter의 saturation이 발생하고, 이는 곧 획득할 수 있는 성능에 한계점이 있다고 볼 수 있다. 필연적으로 네트워크 파라미터를 적게 학습하는 전략을 취함으로써 성능 수렴으로 인해 발생하는 trade-off가 딜레마로 작용하는 상황이다. 이러한 문제를 줄이고자 dynamic하게 architecture를 변화하는 방법들(특히 expanding의 방향으로)이 제안되기 시작하였고, 이는 기존 네트워크 파라미터의 수용력이 새로운 task를 받아들일 정도로 충분치 않을 때 사용하기 적합하다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model decomposition&lt;/strong&gt;은 model을 task-sharing(task 변화에도 무관한 파라미터)와 task-specific(task에 특성화된 파라미터) 성분으로 구분하는 approach다. 보통 task-specific component는 앞선 연구의 흐름에 따라 확장 가능한 network를 가정하는 것이 일반적이다. Task specific components를 구성하는 구조로는 parallel branches인 &lt;a href=&quot;https://arxiv.org/abs/2003.09553&quot;&gt;ACL&lt;/a&gt;과 같은 방법이라던지 adaptive layer인 &lt;a href=&quot;https://arxiv.org/abs/2011.12328&quot;&gt;GVCL&lt;/a&gt;이 주로 알려져있으며, 중간 feature map에 대한 &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html&quot;&gt;mask를 생성하는 generator를 고안&lt;/a&gt;하는 방법도 제안되었다. Feature mask를 model decomposition에 사용하는 것은 parameter spcae에서 동작하거나 앞서 parameter allocation에서 간단하게 언급했던 binary mask의 형태는 아니기 때문에 parameter allocation과는 좀 다르다고 할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/be0e152a-e4ab-4073-9d55-c4a8c4a6f7fa&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modular network&lt;/strong&gt;는 parallel한 sub-network나 sub-module을 기존 네트워크 구조 상에 제안한 형태이다. Progressive Network 논문에서는 각 task마다 동일한 sub-network를 제안하고 adaptor connection으로부터 서로 다른 sub-network 끼리의 knowledge transfer를 수행하는 방법을 제안하였다. 다른 approach에서는 여러 parallel branch를 통해 candidate path(task 학습에 따른 경로를 의미함)을 설계하는 방식을 취한 뒤 가장 최적의 경로를 선택하는 접근도 포함한다. 후보군을 모집한다는 개념에서 &lt;a href=&quot;https://arxiv.org/abs/2207.06543&quot;&gt;여러 sub-네트워크간의 앙상블&lt;/a&gt;을 사용하기도 한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/291d6635-22b8-4ce7-8280-7de5d1bc2971&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결론-및-요약&quot;&gt;결론 및 요약&lt;/h1&gt;

&lt;p&gt;딥러닝에는 정말 다양한 task들이 있고 이는 곧 딥러닝을 활용한 알고리즘을 실생활에 가깝게 묘사하기 위해 여러 가지 상황에 대한 assumption이 필수적이기 때문이라고 생각이 된다. 며칠 전 양자 컴퓨터와 관련된 영상에서, 지금의 binary 컴퓨터는 실제 자연을 전혀 묘사하지 못하기 때문에 같은 task를 수행하더라도 리소스가 천문학적으로 필요하다고 보았다. 아무튼 그만큼 전세계 인공지능 관련 연구자들이 인간이 아닌 환경에서 인간이 내리는 사고를 묘사하기 위해 많은 노력을 기울이고 있는 것 같다.&lt;/p&gt;

&lt;p&gt;Continual learning은 실제로 뇌가 인지하고 사고하는 방식과는 다르다. 왜냐하면 딥러닝 모델이 학습하는 것은 경험에 대한 기억 그 자체가 아니라 해당 경험을 mapping할 수 있는 뉴런 사이의 representation이기 때문이다. 그렇기 때문에 보다 망각하기 쉽고, 가르치기 어렵다는 문제에 직면한다. Continual learning에서 주된 목적은 “이전에 가르친 내용을 망치지 않으면서 새로운 내용을 잘 집어넣는 방법”이다. 최근 Machine Unlearning이라는 새로운 task에서는 이러한 continual learning의 역과정에 대해서 다루는 듯하다. 딥러닝 모델은 파라미터 value가 곧 학습된 내용에 해당되기 때문에 이를 잘 분석하는 것이 곧 우리가 현재 상황에서 수행할 수 있는 최선의 explainable AI일 것이다.&lt;/p&gt;

&lt;p&gt;가장 간단한 방법인 regularization부터 베이시안 모델링이나 생성 모델로부터 출발하여 수학적으로 접근한 여러 방법들, weight consolidation이나 parameter allocation과 같이 task에 따른 파라미터 분류를 통해 네트워크의 일부분만 학습하는 방법도 있었으며, 딥러닝 모델 구조는 처음부터 끝까지 한정적인 형태여야만 한다는 고정관념에서 벗어나 sub-module이나 parallel module, sub network를 기반으로 한 앙상블이나 knowledge transfer 등 간접적인 지식 전달을 목표로 하는 approach도 제안되었다. 가장 놀라웠던 점은 이번 글을 작성하면서 느꼈지만, 단순한 하나의 approach도 어떻게 생각하냐에 따라 굉장히 많은 솔루션으로 탄생할 수 있다는 점이었다. 어쩌다보니 급하게 continual learning을 공부하면서 얼렁뚱땅 정리를 하는 형태가 되었다. 나는 원래 continual learning을 하는 사람이 아니라서 이 글이 얼마나 제대로 된 정리가 되었을지는 모르겠다.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Sep 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/continual</link>
        <guid isPermaLink="true">http://localhost:4000/blog/continual</guid>
        
        <category>Continual learning</category>
        
        <category>Adaptation</category>
        
        <category>Deep learning</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>MERU(Hyperbolic Image-Text Representations) 논문 리뷰</title>
        <description>&lt;h2 id=&quot;들어가며&quot;&gt;들어가며…&lt;/h2&gt;

&lt;p&gt;논문을 소개하기 전, &lt;strong&gt;CLIP&lt;/strong&gt;과 &lt;strong&gt;ALIGN&lt;/strong&gt;과 같은 기존 &lt;strong&gt;Vision-Language Modeling&lt;/strong&gt;의 문제점을 짚는 것이 우선이다. 만약 두 논문에 대한 사전 지식이 없다면 &lt;strong&gt;MERU&lt;/strong&gt; 라는 이름을 가지는 이 모델이 문제시하고자 했던 유클리디안 space(모든 datapoint에 대해 동일한 거리 기준을 삼는 것)에 대한 이해를 하기 힘들기 때문에 적어도 본인 블로그의 CLIP 논문에 대한 내용을 짚고 넘어오는 것이 좋다(&lt;a href=&quot;https://6unoyunr.github.io/blog/clip&quot;&gt;참고 링크&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;유클리디안 기하학과 비유클리디안 기하학은 다른 거리 기준을 가진다. 예컨데 유클리디안(이를 보통 평평한 평면에 비유하기도 함) 공간에서는 삼각형의 세 내각의 합이 $180^\circ$ 인 것이 당연하지만, Spherical 공간에서는 성립하지 않는 것을 볼 수 있다. 마찬가지로 Modality가 놓인 공간이 얼마나 휘어있느냐(이를 하이퍼볼릭에서는 &lt;strong&gt;Curvature&lt;/strong&gt;라는 값으로 정의함)에 따라 각 임베딩 간의 관계성이 다르게 성립한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/d90d3b8f-bdd8-4d2a-a47c-5c8e9ca3032e&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;논문에서는 다행(?)인지 모르겠지만 수식을 방대하게 풀어놓는 형태로 우리를 괴롭히지는 않는다. 다만 단순하게 요약해서 ‘이미지/텍스트’ 간의 관계성을 설명하는 기하학은 &lt;strong&gt;Euclidean&lt;/strong&gt;이 아닌 &lt;strong&gt;non-Euclidean&lt;/strong&gt;이 보다 적합하다는 것. 그렇다면 대체 어떤 이유에서 기존 VLM과 다르게 다른 기하학을 도입하고자 하였는지 살펴보도록 하자.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;텍스트와-이미지의-계층적-관계&quot;&gt;텍스트와 이미지의 계층적 관계&lt;/h2&gt;

&lt;p&gt;예컨데 다음과 같은 이미지가 있다고 생각해보자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b05276b9-b2c1-4185-af32-f8666e5077d4&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;길거리에 나가 $n$명의 사람들에게 “이 그림을 한 문장으로 요약해주시거나, 느낀 점을 말씀해주세요.” 라고 부탁해보자. 도를 아십니까로 착각하여 내치고 지나가지 않는 이상 $n$명의 사람들은 서로 다른 대답을 할 것이다. 예컨데 다음과 같은 후보군이 있을 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강아지와 고양이 사진입니다.&lt;/li&gt;
  &lt;li&gt;고양이가 강아지한테 덤비고 있네요.&lt;/li&gt;
  &lt;li&gt;너무 귀여운 사진이네요.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사람이 이미지와 텍스트를 인지하는 과정은 말하지 않더라도 자연스럽다. 이미지와 텍스트와의 관계성을 고려할 때 우리는 내재된 ‘계층적 관계’를 이해할 수 있다는 것이다. 위에서 예시로 든 첫번째 문장과 두번째 문장을 비교해보자. ‘강아지와 고양이 사진입니다.’라는 설명은 단순히 이미지에 속한 semantic한 정보만을 고려한다. 말 그대로 이미지에 ‘강아지’와 ‘고양이’라는 객체가 포함되어 있다는 정보만 줄 뿐 두 객체의 관계성에 대해서는 설명하지 않는다. 그럼에도 불구하고 ‘강아지와 고양이 사진입니다.’는 이미지를 잘 설명하는 텍스트에 해당되고, 우리가 설계하고자 하는 이미지/텍스트 간의 관계성 메트릭 공간에서는 서로 가까운 거리를 유지해야할 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/8b3eb811-8bdc-4987-8145-318ec5f50cba&quot; width=&quot;300&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/0860a54c-0650-4490-a295-60ed9e7b7e78&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;‘고양이가 강아지한테 덤비고 있네요.’라는 문장은 단순히 객체만을 묘사하지 않고 두 객체 간의 관계에 대한 정의를 내리고 있다. 실제로 이미지만 보고서 고양이가 강아지한테 덤비고 있다는 사실 여부까지는 확인할 수 없지만, 적어도 ‘고양이가 강아지한테 덤비고 있네요.’라는 문장이 해당 이미지를 묘사할 수 있는 합리적인 결과물이라고 인지할 수 있다. 마찬가지로 우리가 설계하고자 하는 이미지/텍스트 간의 관계성 메트릭 공간에서는 서로 가까운 거리를 유지해야할 것이다. 위의 그림 상에서 초록색으로 표시된 부분이 곧 그 관계성을 표현하는 부분이라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;‘그래서 하고픈 말이 뭔데?’&lt;/strong&gt;라고 물어볼 당신을 위해 마지막 문장을 예시로 들면서 언어의 계층적 구성에 대해 언급하고자 한다. ‘너무 귀여운 사진이네요’라는 표현 자체는 큰 문제가 없다. 그나마 문제시될 만한 점은 고양이랑 강아지를 그다지 귀여워하지 않는 사람도 있다는 것인데, 그런 사소한 취향 차이는 무시하고 고양이랑 강아지는 무조건 귀여운 존재라는 가정 하에 각 표현력이 가지는 정보를 계층적으로 구성하면 다음과 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/848fe1f8-2fde-4f89-b828-21eec848cc88&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;텍스트가 묘사하는 내용이 보다 이미지에 특화될수록(이미지에 잘 부합할수록), 그만큼 이미지에서 디테일하게 볼 semantic한 정보에 대한 공간력의 크기 또한 달라지게 되는 것이다. 이걸 단순히 유클리디안 공간에서 표현하게 되면 이미지를 묘사하는 모든 문장들과의 관계성이 cosine similarity(벡터의 각도)에 대해서만 정의되기 때문에 계층적으로 구성된 임베딩을 전혀 고려하지 않게 된다.&lt;/p&gt;

&lt;p&gt;왜냐하면 기존 VLM이 baseline으로 가져가는 contrastive learning objective에는 거리 메트릭이 &lt;strong&gt;오직 embedding 유사성에만 의존&lt;/strong&gt;하기 때문이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;하이퍼볼릭-공간에-대한-짧은-recap&quot;&gt;하이퍼볼릭 공간에 대한 짧은 Recap&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4fd532de-fb99-4340-a6b0-a7ae74517f01&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;사실 비유클리디안 모달리티를 표현하기 위한 하이퍼볼릭 geometry에 대한 연구는 이전에도 꾸준히 진행되었다. 계층적 임베딩을 학습하기 위해 정의한 푸앙카레 disk(혹은 확장한 ball)에 대해 정의한 paper(&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf&quot;&gt;참고 링크&lt;/a&gt;)을 먼저 살펴볼 수 있다. 푸앙카레 곡면이 가지는 특징은 우리가 매핑하고자 하는 쌍곡면(하이퍼볼릭) 공간의 모든 점을 해당 곡면 상으로 projection이 가능하다는 점이다. 사영할 때의 관계성만 정의된다면(각각의 점을 Analytic한 위상 함수의 $X, Y$ 위상 각각의 집합이라고 보면 된다) 임베딩을 푸앙카레 곡면에서 이해할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/da04296a-ae96-48b5-ac4f-3190581cfb0e&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;쌍곡면 공간의 이점은 곧 비유클리디안 모달리티의 대표적 형태인 ‘그래프’의 왜곡과 관련된다(&lt;a href=&quot;https://chumji.tistory.com/3&quot;&gt;도움될만한 링크&lt;/a&gt;). 이에 대한 내용은 &lt;a href=&quot;https://arxiv.org/abs/2011.14211&quot;&gt;‘Curvature Regularization to Prevent Distortion in Graph Embedding’&lt;/a&gt;에 잘 정리되어있는데, 간단하게 보자면 위의 그림에서 &lt;strong&gt;A&lt;/strong&gt;와 같은 노드/엣지 관계를 가진  데이터를 고려해보자. 단순히 파란색으로 표현된 노드와 빨간색으로 표시된 노드에 대한 node classification 이외에도, 각 노드들의 연결성(어떤 노드와 엣지로 관련을 가지는지)에 따라 각각의 모달리티는 특정 표현자(embedding)를 가지면서 학습될 것이다. 기존 그래프 구조의 학습법의 경우 proximity preserving(인접한 노드일수록, 임베딩 유사성을 높게 가져간다)라는 목적 함수를 가지기 때문에 인접한 노드 간의 유사성 매핑과는 별개로 동떨어진 노드 간의 representation에 대해서는 아무런 고려를 하지 못한다(노드가 넘어갈수록 diffusion process와 같은 random walk를 생각하면 된다). 결론적으로 모든 노드들의 확률 그래프는 central limit theorem에 의해 가우시안 분포를 따르게 되어, 얼추 &lt;strong&gt;B1&lt;/strong&gt;에서 보이는 것과 같이 둥그스럼한 형태의 임베딩 space를 구성하게 된다. 노드 간의 유사성을 고려한 mapping 방법 자체는 크게 잘못되지는 않았으나 기존에는 멀었던 노드들의 관계성이나 그래프 자체의 구조를 전혀 고려하지 못한다는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;B1에서의 mapping법(Proximity Preserving을 목적으로 학습된 경우)와 B2에서의 mapping 법에 대해 distance를 기준으로 비교하면, &lt;strong&gt;Geodesic distance&lt;/strong&gt;와 &lt;strong&gt;Shortest path distance&lt;/strong&gt;는 두 경우 모두 유의미한 양의 상관관계를 가지지만(그림 &lt;strong&gt;C&lt;/strong&gt;), &lt;strong&gt;Euclidean distance&lt;/strong&gt;에서는(그림 &lt;strong&gt;D&lt;/strong&gt;) 경향성이 무너지는 것을 확인할 수 있다. 하지만 Oracle Embedding으로 가정한 매핑 방법이 완벽하게 존재할 수는 없고, 임베딩 공간을 objective function으로 수렴시키고자 하는 딥러닝 메소드에서는 이런 학습법을 찾을 수 없는 것이 당연하다. 바로 이러한 &lt;strong&gt;학습법의 제약을 위상에 대한 제약으로 바꾸어 학습하고자 하는 것이 하이퍼볼릭 임베딩 리만 학습법이다&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;riemannian-manifolds리만-다양체&quot;&gt;Riemannian manifolds(리만 다양체)&lt;/h2&gt;

&lt;p&gt;부드러운 다양체는 일종의 부드러운 천을 생각하면 생각하기 편하다. 부드러운 천이 평평한 바닥에 놓여있다면 이는 곧 유클리디안 space에서의 리만 다양체가 될 것이고, 그렇지 않다면 일반화된 space 상에서의 다양체가 된다.  다양체라고 하니까 기분이 좀 이상하네 걍 manifold로 쓰는게 나을듯.&lt;/p&gt;

&lt;p&gt;암튼 Smooth surface에 대한 Manifold를 생각해보면 결국 2차원의 sheet가 이리저리 얽혀있다고 보면 되고, 우리가 특정 함수를 해석학적으로 읽을 때의 느낌과 비슷하게 smooth surface 또한 Locally Euclidean(아주 작은 양수 $\epsilon$에 대해서 manifold를 확장시키면 해당 공간은 로컬 좌표계에서 유클리디안 모달리티를 가질 수 밖에 없다. $d\vec{x}*d\vec{y}$ 느낌으로다가)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/b90aa4e4-3e1c-4bf9-b75e-49d05aab2d8a&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그렇다면 Riemannian manifold를 정의하기 위해서는 딱 두 가지만 있으면 되는데 그게 바로 위에서 언급한 smooth manifold $\mathcal{M}$ 그리고 manifold 상에서 거리를 잴 수 있는 metric $g$이다. 고로 이 메트릭을 Euclidean inner product로 정의해버리면 그 Riemannian manifold는 곧 유클리디안 space로 수렴하는 것. 바로 &lt;strong&gt;기존 방식인 CLIP 학습법&lt;/strong&gt;이 &lt;strong&gt;manifold는 implicit&lt;/strong&gt;하게 가져가면서 metric $g$(이미지 임베딩과 텍스트 임베딩 간의 거리)를 &lt;strong&gt;Euclidean inner product(&lt;/strong&gt;코사인 유사도&lt;strong&gt;)&lt;/strong&gt;로 정의해버린 것과 같다.&lt;/p&gt;

&lt;p&gt;하지만 제목에서부터 알 수 있듯이 이 논문의 목적은 하이퍼볼릭 space이고 하이퍼볼릭 space의 특징은 manifold가 ‘constant negative curvature’를 가진다는 점이다. 앞서 이런저런 소개를 통해 여러 모델링을 언급했지만 MERU 페이퍼에서는 차원을 하나 확장시켜서 해당 위상의 부분 위상으로서 정의되는 로렌츠 모델(Lorentz model)을 기반으로 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;로렌츠-모델에-대한-하이퍼볼릭-공간적-정의&quot;&gt;로렌츠 모델에 대한 하이퍼볼릭 공간적 정의&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/406d264e-c7b4-4c42-8e11-a1867fa7993c&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;MERU는 로렌츠 모델을 기반으로 한다. 로렌츠 모델을 머리 속에 그릴 때는 고등학교 때 배웠던(요즘도 배우나..?) 이차 곡선 중 쌍곡선에 대한 그래프를 $x$축을 기준으로 회전시켰다고 보면 된다. 이때의 $x$축이 additional axis을 의미하게 되며, 해당 공간 상의 텐서를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;permute&lt;/code&gt; 시키게 되면 위에서 보는 것과 같이 $\mathbb{R}^{3}$ 공간에서의 두 smooth manifold 중 upper half를 볼 수 있게 된다($z &amp;lt; 0$인 부분은 무시). 이때 hyperboloid의 중심이 되는 축(대칭축이라고도 부른다)인 $z$를 기존 수학 및 물리학에서 정의하는 것과 같이 시간에 대한 차원(time dimension)으로 정의할 수 있고, 이를 제외한 나머지 $n$개의 차원은 공간에 대한 차원(space dimension)으로 정의할 수 있다. 고로 위상의 모든 벡터는 $\rm{x}_\text{space} \in \mathbb{R}^n$이고 $x_\text{time} \in \mathbb{R}$인 좌표 $[\rm{x}_\text{space}, \it{x}_\text{time}]$로 벡터를 표현할 수 있다. 모든 수식에 서 로마 문자($\rm{x}, \rm{y}$)로 표현된 친구들은 2차원 이상의 좌표계를 가지는 벡터를 의미하고 이텔릭($x, y$)로 표현된 친구들은 1차원의 스칼라 값을 의미한다. Riemannian manifold를 구성하기 위해 필요한 smooth sheet는 마련되었고, 남은 건 거리 메트릭이다. 거리 메트릭은 Euclidean inner product인 $\left&amp;lt; \cdot, \cdot \right&amp;gt;$에 대해 다음과 같이 정의된다.&lt;/p&gt;

&lt;p&gt;[
    \left&amp;lt; \rm{x}, \rm{y} \right&amp;gt;_\mathcal{L} = \left&amp;lt; \rm{x}_\text{space},\rm{y}_\text{space} \right&amp;gt; - x_\text{time} y_\text{time}
]&lt;/p&gt;

&lt;p&gt;마찬가지로 로렌츠 norm은 inner product에 대해 $\parallel \rm{x} \parallel_\mathcal{L} = \sqrt{\vert \left&amp;lt; x, x \right&amp;gt;_\mathcal{L} \vert}$ 로 유도된다. 고등학교 수학을 열심히 들었다면 쌍곡선의 특성에 대해서 잘 알고 있겠지만, 이차곡선의 방정식을 전미분하여 curvature를 구할 수 있고 &lt;strong&gt;해당 curvature에 해당되는 음의 scalar value $-c$&lt;/strong&gt;에 대해 로렌츠 모델 상의 모든 벡터 조건(constraints)을 다음과 같이 줄 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    &amp;amp;\mathcal{L}^n { \rm{x} \in \mathbb{R}^{n+1}: \left&amp;lt; x, x \right&amp;gt;_\mathcal{L} = -1/c,c &amp;gt; 0 } \newline
    &amp;amp;x_\text{time} = \sqrt{1/c + \parallel \rm{x}_\text{space} \parallel^2}
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;이제 로렌츠 모델링을 통한 리만 다양체를 구성하는 과정이 거의 마무리되었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;로렌츠-모델링에서의-거리-메트릭&quot;&gt;로렌츠 모델링에서의 거리 메트릭&lt;/h2&gt;

&lt;h3 id=&quot;geodesics&quot;&gt;Geodesics.&lt;/h3&gt;

&lt;p&gt;Geodesic은 다양체 내부의 두 점에 대해 가장 짧은 path를 의미한다.&lt;/p&gt;

&lt;p&gt;질량이 없는 빛이 중력 가속도의 영향을 받아 휜다는 상대성 이론의 설명에서 휘어진 시공간을 통해 빛이 진행하는 얘기를 들어본 적이 있을 것이다. 우주 공간을 하나의 리만 다양체로 생각하고 만약 이 다양체가 여러 천체의 상호작용에 의해 휘어진 천과 같이 구성되어 있다면 직진하는 성질을 가진 빛은 사실 직진하는 성질을 가지는 것이 아니라 목표 지점까지 최단 루트로 가는 것과 같다. 바로 이렇게 빛이 그리는 궤도를 Geodesics라고 이해해볼 수 있다. 로렌츠 모델에서의 Geodesics는 hyperboloid와 두 벡터 및 원점이 그리는 평면의 intersection curve에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/4438c1a3-dca0-4a7e-a356-5b54f20ff99c&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이를 유도해서 풀면&lt;/p&gt;

&lt;p&gt;[
    d_\mathcal{L}(\rm{x}, \rm{y}) = \sqrt{1/c} \cdot \cosh^{-1}(-c \left&amp;lt; \rm{x}, \rm{y} \right&amp;gt;_\mathcal{L})
]&lt;/p&gt;

&lt;h3 id=&quot;tangent-space&quot;&gt;Tangent space.&lt;/h3&gt;

&lt;p&gt;로렌츠 공간의 spacial vector $z$에 대해 tangent space는 해당 벡터와 Lorentzian inner product(앞에서 정의했던거)가 $0$이 되는 모든 모든 벡터가 모인 공간(span)이다. 해당 공간은 Euclidean space가 된다.&lt;/p&gt;

&lt;p&gt;[
    \mathcal{T}_{\rm{z}} = { \rm{v} \in \mathbb{R}^{n+1}: \left&amp;lt; z, v\right&amp;gt;_\mathcal{L} = 0 } 
]&lt;/p&gt;

&lt;p&gt;따라서 Ambient space(특정 차원의 공간을 포함하는 그 이상의 모든 공간을 의미한다)의 모든 벡터 $\rm{u}$는 orthogonal projection을 통해 저차원 공간인 Tangent space로 사영할 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \rm{v} = \text{proj}_{\rm{z}}(\rm{u}) = \rm{u} + \it{c}\rm{z} \left&amp;lt;\rm{z},\rm{u} \right&amp;gt;_\mathcal{L}
]&lt;/p&gt;

&lt;h3 id=&quot;exponential-and-logarithm-maps&quot;&gt;Exponential and logarithm maps.&lt;/h3&gt;

&lt;p&gt;Exponential map은 tangent space의 벡터들을 manifold 상으로 올려주는 역할을 한다. 위에서 언급한 projection의 역과정은 아니고, ambient space가 아닌 공간에 대한 sub-space에서 공간 차원에 대한 벡터를 통해 함수 및 역함수를 구하는 과정이라고 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    &amp;amp;\text{expm}_{\rm{z}} : \mathcal{T}_{\rm{z}}\mathcal{L}^n  \rightarrow \mathcal{L}^n \newline
    \rm{x} = \text{expm}_{\rm{z}}(\rm{v}) &amp;amp;= \cosh(\sqrt{c} \parallel v \parallel_\mathcal{L})\rm{z} + \frac{\sinh (\sqrt{c} \parallel v \parallel_\mathcal{L})}{\sqrt{c}\parallel v \parallel_\mathcal{L}}\rm{v}
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;뒤에서 보면 알겠지만 실제 MERU에서는 space component만 써먹는 단순화 작업을 통해 하이퍼볼릭 코사인 텀 하나를 날려버린다. 하이퍼볼릭 함수는 모두 역함수가 있어서 반대로  manifold 상에서 tangent 공간으로 내리는 공식도 가능하다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    &amp;amp;\text{logm}_{\rm{z}} : \mathcal{L}^n  \rightarrow \mathcal{T}_{\rm{z}}\mathcal{L}^n \newline
    \rm{v} = \text{logm}_{\rm{z}}(\rm{x}) &amp;amp;= \frac{\cosh^{-1}(-c\left&amp;lt;\rm{z},\rm{x} \right&amp;gt;_\mathcal{L})}{\sqrt{(c\left&amp;lt; \rm{z}, \rm{x} \right&amp;gt;_\mathcal{L})^2-1}} \text{proj}_{\rm{z}}(\rm{x})
    \end{aligned}
]&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;방법론&quot;&gt;방법론&lt;/h2&gt;

&lt;p&gt;모델링은 CLIP과 같이 이미지와 텍스트가 독립적인 인코더를 가지는 형태가 된다.  그런데 이제 여기서 그칠게 아니고 실질적으로 이 논문이 제시한 문제점을 해결하기 위해서는 다음과 같은 방법론을 적용해야한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;임베딩을 Euclidean space가 아니라 Lorentz space로 바꿀 방법&lt;/li&gt;
  &lt;li&gt;CLIP과 같이 이미지의 semantic 정보와 텍스트의 semantic 정보를 함께 학습할 방법&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/a712812b-bbdc-4aa0-8aa0-f644cbcc0b31&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;lifting-embeddings&quot;&gt;Lifting embeddings&lt;/h3&gt;

&lt;p&gt;임베딩을 올리는 방법은 다음과 같다. $n$차원으로 나온 이미지 및 텍스트 임베딩이 있다고 해보자. 기존 CLIP과 동일한 프레임워크라면 두 임베딩은 같은 형태의 텐서가 될 것이다. 로렌츠 모델은 설명했던 바와 같이 추가된 축(시간축)으로 확장시키는 작업이 필요하기 때문에, 우선은 임시로 원점에서의 tangent space(Euclidean)에 대한 벡터를 구한 뒤 이를 hyperboloid로 올리는 공식($\text{expm}$)을 적용하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;expand-dims&quot;&gt;Expand dims&lt;/h3&gt;

&lt;p&gt;[
    \rm{v} = (\rm{v}_\text{enc}, 0) \in \mathbb{R}^{n+1}
]&lt;/p&gt;

&lt;h3 id=&quot;onto-hyperboloid&quot;&gt;Onto &lt;strong&gt;Hyperboloid&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;여기는 잘보면 우리가 지금 올려놓은 tangent space가 $\rm{z} = 0$에 기반하므로 하이퍼볼릭 코사인이 없어져도 된다.&lt;/p&gt;

&lt;p&gt;[
    \rm{x}_\text{space} = \frac{\sinh (\sqrt{c} \parallel v \parallel_\mathcal{L})}{\sqrt{c}\parallel v \parallel_\mathcal{L}} \rm{v}_\text{space}
]&lt;/p&gt;

&lt;p&gt;그리고 &lt;strong&gt;시간에 대한 스칼라&lt;/strong&gt;는 &lt;strong&gt;쌍곡면의 곡면에 대한 constraints&lt;/strong&gt;로 구할 수 있다.&lt;/p&gt;

&lt;p&gt;[
    \begin{aligned}
    &amp;amp;\mathcal{L}^n { \rm{x} \in \mathbb{R}^{n+1}: \left&amp;lt; x, x \right&amp;gt;_\mathcal{L} = -1/c,c &amp;gt; 0 } \newline
    &amp;amp;x_\text{time} = \sqrt{1/c + \parallel \rm{x}_\text{space} \parallel^2}
    \end{aligned}
]&lt;/p&gt;

&lt;p&gt;즉 기존의 CLIP embedding을 하이퍼볼릭 embedding으로 바꾸는 작업 끝.&lt;/p&gt;

&lt;h3 id=&quot;numerical-issue&quot;&gt;Numerical Issue&lt;/h3&gt;

&lt;p&gt;다만 문제가 되는 점은 기존 임베딩 벡터를 하이퍼볼릭으로 올리면서 exponential 계산이 추가되는데, CLIP 기반의 weight 초기화가 진행된 경우 유클리디안 공간 벡터의 norm은 대략 $\sqrt{n}$을 가지게 된다. 그 말인 즉슨 이걸 exponential하게 올리면 수치적으로 $e^{\sqrt{n}}$가 초기값이 되어 학습이 불안정해지기 때문에 이를 해결할 방법이 필요하다. 그래서 이걸 해결하기 위해 scaling 스칼라인 $\alpha$를 각각 이미지 및 텍스트 임베딩 output에 적용하여 이러한 문제를 해결한다. 알파는 학습 가능한 파라미터.&lt;/p&gt;

&lt;h3 id=&quot;entailment-loss&quot;&gt;Entailment loss&lt;/h3&gt;

&lt;p&gt;Entailment loss는 CLIP에서 임베딩 관계성을 implicit하게 학습하기 위해 사용되는 contrastive loss에 추가적으로 하이퍼볼릭 공간의 위상 특성을 고려하여 이미지-텍스트 관계를 더해주는 역할이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/df979d42-5ef4-471f-afd0-aea6e47d9355&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그림을 보면 알겠지만 각도에 대한 내용이기 때문에 로렌츠 모델을 위에서 내려다본 구조를 생각하면 되고, 예를 들어 특정 텍스트가 존재할 때 해당 텍스트가 반영하는 모든 이미지 임베딩의 공간을 aperture로 가정하면(원뿔 형태), 만약 이미지가 이 내부에 들어와있다면 굳이 끌어들일 필요가 없지만(이는 아마도 각도가 too much align되면 embedding collapse가 발생하여 representation이 subfold된다고 생각한 것으로 예상) 외부에 있다면 각도를 줄여주는 loss를 통해 aperture 내부로 임베딩을 끌어들이는 것이다. 서로 다른 이미지/텍스트를 인코딩한 임베딩 간의 거리 조절은 contrastive learning에서 주로 담당하고 있고, entailment loss는 하이퍼볼릭의 curvature를 고려하여 계층적 구조를 탄탄히 하려는 목적인 듯하다. &lt;strong&gt;모델링이 생각보다 너무 심플해서 놀랐던 부분&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;[
    \mathcal{L}_\text{entail}(\rm{x}, \rm{y}) = \max (0, ext(\rm{x},\rm{y})-\text{aper}(\rm{x}))
]&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;h3 id=&quot;imagetext-retrieval&quot;&gt;Image/Text retrieval&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/9457b696-2f9e-435f-92cb-960b91309b57&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;전반적으로 retrieval 성능을 CLIP에 비해 끌어올린 것을 볼 수 있는데, 확실히 유클리디안에서 비유클리디안으로의 공간 확장이 가지는 장점이 가장 잘 드러날 수 있는 실험 결과가 아닐까 생각된다.&lt;/p&gt;

&lt;h3 id=&quot;zero-shot-image-classification&quot;&gt;Zero-shot image classification&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/8e0f987d-6331-473b-be8e-49a517ff1fd8&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Representation 학습에는 어떤 도움이 되는지. Retrieval 결과가 본인들 모델링에 대한 근본적 main contribution을 보여주는 실험이었다면 이건 비교적 sub contribution을 보여주는 결과라고 본다. 그래서 그런지 SOTA를 찍을 필요는 없고 성능 향상의 가능성만 간단하게 보여주고 넘어가는 듯.&lt;/p&gt;

&lt;h3 id=&quot;ablations&quot;&gt;Ablations&lt;/h3&gt;

&lt;p&gt;이런저런 ablation도 많이 진행했다. Qualitative results도 많다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/2c7e4230-5f27-4fe8-b9b8-130bb0d60738&quot; width=&quot;400&quot; /&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/ecd93705-8f0d-4671-a04e-3ed81228b246&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;좌측은 임베딩 길이에 따라 성능 본 것. 길이랑 무관하게 모두 성능이 좋게 나왔다. 우측은 논문에서 제시한 방법론들을 하나씩 빼고 한 것. Curvature를 고정하는 건 원래 푸앙카레 기법에서 사용한 것인데 그렇게 하면 놀랍게도 Large model에서 성능이 개판이 되는 걸 볼 수 있다. Contrastive learning 시에 Large model에서 로렌츠 norm을 사용하지 않으면 심지어 수렴이 되지 않고 발산을 하는데, 이는 아마도 트랜스포머 백본과의 수렴 속도 차이 때문의 문제로 생각된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;discussion-and-conclusion&quot;&gt;Discussion and conclusion&lt;/h2&gt;

&lt;p&gt;CLIP의 retrieval 및 linear probing/fine tuning 등 representation 자체를 모달리티 robust하게 만들고자 여러 연구가 나오고 있는 중인 것 같다. 그와 동시에 이미지와 텍스트 간의 semantic한 정보들을 보다 풍부하게 제공할 수 있는 학습 objective나 constraints를 제시하는 것이 앞으로의 multimodal task에서 가장 challenging한 부분이 되지 않을까 싶다.&lt;/p&gt;

&lt;p&gt;MERU에서 제시한 로렌츠 모델링에 contribution이 있는 것은 아니고 기존 CLIP representation을 확장시킬 수 있는 방향을 제시한 것이 contribution로 보인다. Hyperbolic Riemannian manifold를 위한 모듈을 기존 framework 끝부분에 사용하고, 새로운 manifold에서 제시할 수 있는 추가적인 objective를 제시한 방향도 paper contribution에 적합하다고 생각했다. 기존 인코딩 방식을 바꾸지 않으면서 projection하는 방법만 제시했기 때문에 비슷한 형태의 embedding을 추출하는 인코더 기반 네트워크들에 대해 hierarchy embedding에 대한 추가 실험 및 연구가 진행되면 해당 방법론에 대한 정당성이 조금씩 확립될 것으로 보인다.&lt;/p&gt;
</description>
        <pubDate>Wed, 12 Jul 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/meru</link>
        <guid isPermaLink="true">http://localhost:4000/blog/meru</guid>
        
        <category>VLM</category>
        
        <category>CLIP</category>
        
        <category>Multimodal</category>
        
        <category>Hyperbolic space</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>GLIDE(Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models) 논문 및 코드 리뷰</title>
        <description>&lt;h1 id=&quot;들어가며&quot;&gt;들어가며&lt;/h1&gt;

&lt;p&gt;최근 &lt;a href=&quot;https://6unoyunr.github.io/blog/DDPMproof&quot;&gt;DDPM 수식 조지기 게시글&lt;/a&gt;과 더불어 &lt;a href=&quot;https://6unoyunr.github.io/blog/ddim&quot;&gt;DDIM&lt;/a&gt; 등등 여러 diffusion model 관련 논문들을 리뷰했었다. 그 중 diffusion에 condition을 추가하는 논문인 &lt;a href=&quot;https://6unoyunr.github.io/blog/diffusionpapers&quot;&gt;classifier guided/free 논문&lt;/a&gt;과 최근 가장 핫한 conditioning paper인 &lt;a href=&quot;https://6unoyunr.github.io/blog/controlnet&quot;&gt;ControlNet&lt;/a&gt; 또한 다뤘었다. 이번에 소개할 논문인 &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;&gt;GLIDE&lt;/a&gt; 또한 conditional diffusion에 관련된 논문이며, 기존에 소개했던 classifier 관련 논문들은 label이 존재하는 discretized category에만 적용될 수 있었던 방법이라면 GLIDE는 &lt;strong&gt;text description&lt;/strong&gt;을  어떻게 하면 diffusion sampling에 효과적으로 &lt;strong&gt;조건으로써 사용&lt;/strong&gt;할 수 있는지 논의한 페이퍼이다.&lt;/p&gt;

&lt;p&gt;DALLE-2가 출시되고 난 후 부랴부랴 diffusion에 대해 알아보고 그제서야 GLIDE가 관련 논문으로 눈에 들어왔으나 그 때 당시 diffusion에 대한 지식이 전무하기도 했고 구글에 검색해봐도 그다지 &lt;strong&gt;도움이 되는 리뷰글&lt;/strong&gt;이 없어 고생했었다. Diffusion에 대해 공부를 시작한 지 어느덧 거의 1년이 되어가는데, GLIDE는 사실 조금은 철 지난 논문이긴 하지만 어떻게 &lt;strong&gt;디퓨전 모델&lt;/strong&gt;이 GAN과 같은 기존 generative method를 넘어설 수 있었는지 그 &lt;u&gt;흐름을 볼 수 있는 과정 중간에 있는&lt;/u&gt; 좋은 페이퍼라고 생각한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;diffusion-model의-성장&quot;&gt;Diffusion model의 성장&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Diffusion model&lt;/strong&gt;의 가장 기본이 되는 SMLD와 DDPM는 score based generative model로 여러 관련 연구들을 파생하며 디퓨전 모델링의 가능성을 딥러닝 생성 모델 소사이어티에 널리 알려지기 시작하였다. 그러한 연구들 중 &lt;strong&gt;GLIDE&lt;/strong&gt;는 diffusion도 GAN에서의 연구 방향처럼 언젠가는 다양한 task에 적용이 될 수 있을 것이라고 생각하고, 가장 대표적인 생성 관련 멀티모달 연구인 &lt;u&gt;T2I(Text to Image synthesis)&lt;/u&gt; 분야를 파고들기 시작했다.&lt;/p&gt;

&lt;h3 id=&quot;clip-연구와-t2i&quot;&gt;CLIP 연구와 T2I&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;T2I 연구&lt;/strong&gt;는 기존 SOTA 및 우수한 성능을 가지는 generative model을 활용하여 활발히 진행되었다. 대표적으로 이를 가속화할 수 있었던 대표주자가 transformer 구조와 이를 기반으로 한 Image/Text corresponding model인 CLIP이다. CLIP에 대한 글은 포스팅한 내용을 보면 보다 이해하기 빠를 것이다(&lt;a href=&quot;https://6unoyunr.github.io/blog/clip&quot;&gt;참고 링크&lt;/a&gt;). 이를 간단하게 설명하자면,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698004-59d9e19f-d74a-4b51-a774-5d5616fad369.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;이미지&lt;/strong&gt;와 해당 이미지를 잘 설명하는 &lt;strong&gt;caption&lt;/strong&gt;(text prompt)가 pair로 존재하는 대량의 데이터셋에서, 이미지와 함께 positive pair가 되는 text prompt 각각의 embedding 유사도를 높이게끔 학습하며 그와 동시에 negative pair가 되는 나머지 prompt 각각의 embedding과의 유사도는 낮아지게끔 학습한다. &lt;strong&gt;Contrastive learning&lt;/strong&gt;을 사용하여 기존 classification task에서의 discrete label(one hot encoding label)에서 벗어나, 다양한 &lt;u&gt;텍스트 형태와 이미지를 연관&lt;/u&gt;지을 수 있는 학습 구조를 소개한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;embedding-similarity-into-classifier&quot;&gt;Embedding similarity into classifier&lt;/h3&gt;

&lt;p&gt;이러한 구조적 변경을 통한 사용 가능한 텍스트의 확장은 시사하는 바가 굉장히 컸다. 예컨데 &lt;u&gt;사용될 수 있는 텍스트가 많아진다는 것&lt;/u&gt;은 네트워크의 크기를 부담없이 크게 가져가면서 dataset domain $p(x)$를 확장할 수 있다는 뜻이 되고, semantic understanding의 이해가 여타의 연구들에 &lt;strong&gt;snowball effect&lt;/strong&gt;를 가져올 수 있는 딥러닝 연구 특성상 이전과는 비교할 수 없을 정도로 &lt;u&gt;multimodal에 대한 연구 및 성능이 급증할 것&lt;/u&gt;임을 암시하였다. GLIDE에서는 classifier를 사용한 디퓨전 모델의 조건화와 CLIP을 사용한 이미지와 텍스트 간의 유사도를 기반으로 한 understanding 두 연구를 통해 아이디어를 확립하기 시작했다.&lt;/p&gt;

&lt;h3 id=&quot;guided-diffusion-models&quot;&gt;Guided diffusion models&lt;/h3&gt;

&lt;p&gt;디퓨전 모델 조건화의 대표격 연구인 &lt;a href=&quot;https://arxiv.org/abs/2105.05233&quot;&gt;Diffusion beat GANs&lt;/a&gt; 그리고 &lt;a href=&quot;https://arxiv.org/abs/2207.12598&quot;&gt;Classifier free diffusion&lt;/a&gt;는 기존 GAN 및 autoregressive modeling에서만 가능했던 다양성/퀄리티 간의 trade-off(low temperature sampling)를 디퓨전 모델에 접목할 수 있게 하였고, 디퓨전 생성 모델의 controllability 및 latent manipulation을 용이하게 할 수 있었다.&lt;/p&gt;

&lt;p&gt;[
\log p_\phi(y \vert x_t) \approx \log p_\phi(y \vert x_t) \vert_{x_t = \mu}+(x_t - \mu)\nabla_{x_t} \log p_\phi (y \vert x_t) \vert_{x_t = \mu} = (x_t - \mu)g+C_1
]&lt;/p&gt;

&lt;p&gt;평균에 가까운 point에서 상대적으로 &lt;strong&gt;curvature&lt;/strong&gt;이 작다고 가정할 수 있는 classifier의 score를 &lt;u&gt;테일러 1차 근사&lt;/u&gt;를 통해 gradient guidance를 주는 방식인 classifier guidance는 디퓨전 모델로 하여금 샘플 퀄리티를 높일 수 있는 효과적인 방법으로 제시되었다.&lt;/p&gt;

&lt;p&gt;[
\tilde{\epsilon}(z_\lambda, c) = (1+w)\epsilon_\theta(z_\lambda, c) - w\epsilon_\theta(z_\lambda)
]&lt;/p&gt;

&lt;p&gt;그에 대응하여 나온 연구인 classifier free 연구는 classifier guidance를 위해 classifier에 의존하는 것은 classifier를 모든 scale의 noise에 대해 학습해야하기 때문에 &lt;u&gt;pipeline을 복잡하게&lt;/u&gt; 만들고, 무엇보다 classifier에 의한 gradient 학습은 결국 FID나 IS 메트릭 상 직접적인 목적 함수로 작용하기 때문에 실제 샘플링 성능을 높이는데 방법론 자체가 주된 역할을 수행하지 않는다는 비판 속에서 등장하였다.&lt;/p&gt;

&lt;h3 id=&quot;clip-guidance-and-classifier-free-guidance&quot;&gt;CLIP guidance and classifier-free guidance&lt;/h3&gt;

&lt;p&gt;저자는 이러한 두 경향(classifier를 사용하자/classifier를 사용하지 말자)에 대해 &lt;u&gt;모두 실험이 가능한 프레임워크&lt;/u&gt;를 만들고, 실제로 각 방법이 &lt;strong&gt;text to image sample quality&lt;/strong&gt;에 어떤 영향을 미치는지 관찰하였다.&lt;/p&gt;

&lt;p&gt;다만 text description에 대해서는 단일 task에 대한 classifier를 사용할 수 없기 때문에 이를 보완하고자 CLIP score(유사성)을 사용하는 방법을 고안하였고, classifier-free guidance는 classifier의 의존성이 불필요하기 때문에 기존 ADM(Diffusion beat GANs)구조와 해당 논문에서 사용된 conditioning 방법을 사용하게 된다. 해당 내용은 뒤에서 오피셜 코드와 함께 자세히 리뷰할 예정이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;diffusion-model에-대한-간략한-소개&quot;&gt;Diffusion model에 대한 간략한 소개&lt;/h1&gt;

&lt;h3 id=&quot;ddpm-as-score-estimator&quot;&gt;DDPM as Score estimator&lt;/h3&gt;

&lt;p&gt;GLIDE는 이해하고 싶지만 아직 diffusion에 대해서 다뤄본 적이 없는 사람들에게는 갑자기 이상한 입실론이 나오기 시작하면 골이 땡기기 시작한다. 본인 게시글 중 &lt;a href=&quot;https://6unoyunr.github.io/blog/DDPM&quot;&gt;DDPM 소개&lt;/a&gt; 및 &lt;a href=&quot;https://6unoyunr.github.io/blog/scoresde&quot;&gt;Score based generative modeling 이해하기&lt;/a&gt;를 읽고 오면 좋지만, 간단하게 소개하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;일반적으로 샘플링을 하기 위해 intractable solution을 풀기 위해 가정해야하는 variational inference의 기초가 바로 샘플링이 용이한 prior $p_\theta(z)$를 잘 설정하는 것이다. 기존 GAN에서는 단순 샘플링으로 해결하거나 VAE에서는 auto-encoder를 함께 학습하면서 KL divergence 정규화를 하는 방법을 쓰게 되는데, 이걸 diffusion model에서는 이름에서 알 수 있듯이 &lt;u&gt;‘diffusion process’&lt;/u&gt;로 해결한다.&lt;/p&gt;

&lt;p&gt;[
q(x_t \vert x_{t-1}) := \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I) 
]&lt;/p&gt;

&lt;p&gt;마치 향수가 공기 중에서 점차 확산해가는 운동을 Brownian motion으로 정의하는데, 해당 운동을 설명하는 방정식이 바로 stochastic differential equation이고, 이 SDE의 solution이 바로 위에서 보이는 diffusion process가 된다. 이렇게 작은 gaussian noise를 점차 더해가다보면, 충분한 시간 $t$가 지난 후에는 $x_T$가 가우시안 노이즈가 되어있는 것이다.&lt;/p&gt;

&lt;p&gt;가우시안 노이즈를 이번에는 reverse process(역과정)를 통해 데이터를 만들고 싶다고 생각해보자. 하지만 확률 분포가 정의된 정방향과는 다르게 역방향인 $q(x_{t-1} \vert x_t)$ 는 tractable하지 않기 때문에, 이를 예측하는 parametric function $p_\theta(x_{t-1} \vert x_t)$를 딥러닝을 통해 해결하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;이를 수식화하여 정리한 것이 다음과 같은 diffusion process(DDPM)의 학습 목적함수인 simplified loss이며,&lt;/p&gt;

&lt;p&gt;[
L_\text{simple} := \mathbb{E}_{x_0 \sim q(x_0), \epsilon \sim \mathcal{N}(0, I)} \left( \parallel \epsilon - \epsilon_\theta(x_t, t) \parallel_2^2 \right)
]&lt;/p&gt;

&lt;p&gt;이는 곧 score matching 수식화를 통해 DDPM network가 예측하고자 하는 epsilon은 score function의 normalized 버전임을 알 수 있다(증명 생략).&lt;/p&gt;

&lt;p&gt;[
\begin{aligned}
&amp;amp;x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}}_t} \epsilon_\theta(x_t, t)\right)+\sigma_tz \newline
\rightarrow~~&amp;amp;x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_i + \beta_i s_{\theta^\ast}(x_i, i)) + \sqrt{\beta_i}z_i
\end{aligned}
]&lt;/p&gt;

&lt;p&gt;이런 베이스라인에서 variance까지 학습하고자 한 논문이 &lt;u&gt;improved DDPM 논문&lt;/u&gt;이다. 위의 베이스 식에 이런저런 condition을 넣을 수 있는데, 이러한 조건화 중 diffusion model을 보다 고차원 이미지에 대해 성능을 높일 수 있었던 방법이 downsampled input $x$를 채널 단위로 concatenate하여 조건화를 시킨 &lt;u&gt;superresolution diffusion model&lt;/u&gt;이다. 이 내용은 뒤이어 코드 리뷰와 함께 확인해볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
p_{\theta}(y_{t-1} \vert y_t,~x) 
]&lt;/p&gt;

&lt;h3 id=&quot;guided-diffusion&quot;&gt;Guided diffusion&lt;/h3&gt;

&lt;p&gt;Noise가 더해진 각 process 단계에서의 sample에 대한 classifier $p_\phi(y \vert x_t)$를 사용하여 생성 과정에서 gradient를 틀어주는 방법($s$가 guidance의 power를 결정)이다.&lt;/p&gt;

&lt;p&gt;[
\hat{\mu}_\theta(x_t \vert y) := \mu_\theta(x_t \vert y) + s \cdot \Sigma_\theta(x_t \vert y)\nabla_{x_t} \log p_\phi (y \vert x_t) 
]&lt;/p&gt;

&lt;h3 id=&quot;classifier-free-guidance&quot;&gt;Classifier-free guidance&lt;/h3&gt;

&lt;p&gt;앞서 소개한 방법은 noise sample에 대해 classifier를 학습시켜야 한다는 번거로움이 있다. 만약 classification에 대한 implicit classifier $p^i(y \vert x_t)$을 가정하면, 해당 implicit classifier는 다음 비례식을 가진다(Bayes’ rule).&lt;/p&gt;

&lt;p&gt;[
p^i(y \vert x_t) \propto \frac{p(x_t \vert y)}{p(x_t)}
]&lt;/p&gt;

&lt;p&gt;log likelihood에 대한 gradient(score)를 표현하면 다음과 같이 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;[
\nabla_{x_t} \log p^i(y \vert x_t) \propto \nabla_{x_t} \log p(x_t \vert y) - \nabla_{x_t} \log p(x_t) \propto \epsilon^\ast(x_t \vert y) - \epsilon^\ast(x_t)
]&lt;/p&gt;

&lt;p&gt;고로, 네트워크를 classifier conditioned sample 그리고 unconditional sample에 대해 모두 학습하면 다음과 같은 extrapolated direction을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;[
\hat{\epsilon}_\theta(x_t \vert y) = \epsilon_\theta(x_t \vert \emptyset) + s \cdot (\epsilon_\theta(x_t \vert y) - \epsilon_\theta(x_t \vert \emptyset))
]&lt;/p&gt;

&lt;h3 id=&quot;clip-guidance&quot;&gt;CLIP guidance&lt;/h3&gt;

&lt;p&gt;앞서 설명한 내용을 사용하여 이 논문에서 사용한 CLIP guidance에 대해 살펴보면 다음과 같다. CLIP은 image encoder $f(x)$를 통해 추출한 이미지 임베딩과 text encoder $g(c)$를 통해 추출한 텍스트 임베딩 간의 similarity를 내적을 통해 계산한다. 정확히 말하자면 cosine similarity에 해당된다.&lt;/p&gt;

&lt;p&gt;[
f(x) \cdot g(c)
]&lt;/p&gt;

&lt;p&gt;만약 image와 text 사이에 유사도가 높다면 inner product 역시 큰 값을 가지게 된다. CLIP은 이러한 방식으로 캡션과 이미지 사이의 유사도를 측정할 수 있기 때문에, 해당 value를 일종의 classification probability로 바꿔 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;[
\hat{\mu}_\theta(x_t \vert y) := \mu_\theta(x_t \vert y) + s \cdot \Sigma_\theta(x_t \vert y)\nabla_{x_t} (f(x_t) \cdot g(c))
]&lt;/p&gt;

&lt;p&gt;기존 classifier guidance 논문에서 했던 방법처럼 CLIP 또한 noised image $x_t$에 대해 학습을 했고, 이를 통해 reverse process를 하는 과정에서 보다 정확한 gradient를 구할 수 있게 된다. 이를 페이퍼에서는 ‘Noised CLIP’이라 부른다.&lt;/p&gt;

&lt;p&gt;기존에 커뮤니티에서 CLIP을 사용한 diffusion의 text 조건화에 대해 실험한 경우에는 &lt;u&gt;굳이 noised CLIP 없이도 fine tuning이 가능하다고 주장&lt;/u&gt;했었지만, 그대신 data augmentation이나 perceptual loss와 같은 &lt;strong&gt;추가적인 메트릭&lt;/strong&gt;이 필요하다. CIFAR-10C와 같은 corruption dataset이 domain shift problem과 같은 task에서 사용되는 것을 보면, noised input이 classifier로 하여금 out of distribution domain의 이미지에 해당되고, 이는 곧 제대로 된 classification에 방해가 되기 때문이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698044-d3810eb0-0921-4997-80c4-6a3e8a866020.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;related-works&quot;&gt;Related works&lt;/h1&gt;

&lt;h3 id=&quot;text-conditional-image-generation&quot;&gt;Text conditional image generation&lt;/h3&gt;

&lt;p&gt;Text를 조건부로 image를 생성하는 task는 기존에도 수없이 존재해왔다. 예컨데 기존의 가장 SOTA였던 GAN을 아키텍쳐 베이스로 잡은 수많은 방법들이 captioning dataset을 기반으로 제안되었고, 비교적 가장 해당 논문과 시기적으로 가까운 DALLE의 경우 Vector Quantized learning을 베이스라인으로 삼았다. 기존 연구들은 diffusion baseline을 사용하지 않았지만, 디퓨전 모델이 발전하기 시작하면서 text conditioning 연구를 접목하려는 시도가 점차 증가하기 시작했다. 사실상 DALLE의 아이디어를 많이 참고했다고 볼 수 있는 vector quantized diffusion 연구가 diffusion based T2I 연구의 초창기에 해당된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698010-5d52e1aa-0efe-4246-8660-b3abd2e76c7b.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;diffusion-model-based-generation-tasks&quot;&gt;Diffusion model based generation tasks&lt;/h3&gt;

&lt;p&gt;Diffusion 관련하여 text to image 이외에도 다른 형태의 task 또한 발전하기 시작했다.  &lt;a href=&quot;https://sde-image-editing.github.io/&quot;&gt;SDEdit&lt;/a&gt;과 같은 논문에서는 diffusion process인 SDE를 통해 단순히 inpainting과 같은 task 말고도 rough sketch(stroke) to image와 같은 conditioned task를 해결할 수 있음을 보였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698012-e5a084b7-69c8-4e16-9938-8b93cee78de5.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;또한 &lt;a href=&quot;https://arxiv.org/pdf/2111.05826.pdf&quot;&gt;Palette&lt;/a&gt; 논문에서는 image to image translation task 각각을 목적으로 diffusion model을 학습시켰을 때 모두 성공적으로 task를 수행할 수 있음을 보였다.&lt;/p&gt;

&lt;h3 id=&quot;use-clip-guidance-into-image-generationgan&quot;&gt;Use CLIP guidance into image generation(GAN)&lt;/h3&gt;

&lt;p&gt;GAN 모델을 기반으로 한 CLIP guided image generation 또한 활발하게 연구되기 시작했다. 예컨데 이전에 리뷰했던 논문들을 포함하여 StyleCLIP(StyleGAN + CLIP), BigSleep(BigGAN + CLIP) 그리고 StyleGAN-NADA와 같은 내용이 바로 이에 해당된다. GAN과 CLIP guidance를 어떻게 엮어서 사용할 수 있었는지에 대한 자세한 내용은 해당 게시글에 정리되어있다(&lt;a href=&quot;https://6unoyunr.github.io/blog/styleclip+styleganada&quot;&gt;참고 링크&lt;/a&gt;). &lt;a href=&quot;https://arxiv.org/pdf/2111.13792.pdf&quot;&gt;LAFITE paper&lt;/a&gt;에서는 CLIP text embedding에 조건화를 위해 사전 정의된 노이즈로 perturb한 CLIP image embedding을 사용하여 GAN model을 학습하는 방식으로, 고퀄리티의 image/text pair dataset이 없더라도(language free) text to image 조건화가 가능하다는 연구를 진행한 바 있다.&lt;/p&gt;

&lt;h3 id=&quot;use-clip-guidance-into-image-generationdiffusion&quot;&gt;Use CLIP guidance into image generation(Diffusion)&lt;/h3&gt;

&lt;p&gt;그러나 물론 diffusion에도 CLIP guidance를 접목하고자 한 시도가 GLIDE 논문이 처음은 아니었다. Crowson이 커뮤니티에서 공개한 &lt;a href=&quot;https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj&quot;&gt;코랩 코드&lt;/a&gt;라던지, DDIM reconstruction process 과정에서 CLIP loss를 따라가게끔 diffusion model을 fine tuning한 &lt;a href=&quot;https://arxiv.org/pdf/2110.02711.pdf&quot;&gt;DiffusionCLIP&lt;/a&gt; 연구가 진행되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698017-8e20d77b-42a5-4cbb-b307-022372c776e3.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;text-based-image-editing&quot;&gt;Text based image editing&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.10951.pdf&quot;&gt;Paint by Word&lt;/a&gt;나 &lt;a href=&quot;https://arxiv.org/pdf/2111.14818.pdf&quot;&gt;Diffusion based image editing&lt;/a&gt;  논문에서는 CLIP을 사용하여 GAN이나 Diffusion model로 하여금 이미지를 원하는 prompt에 맞춰 변환을 할 수 있도록 학습사는 방법을 소개하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;training-방법&quot;&gt;Training 방법&lt;/h1&gt;

&lt;p&gt;GLIDE는 diffusion을 사용한 T2I를 다룬 논문이므로 학습법을 이해하는 것이 매우 중요하다. 가장 메인이 되는 실험에서는 일반적인 크기의 $64 \times 64$ image에 대해 대용량(3.5B)의 diffusion  model(+text condition)을 학습시키고, 이에 추가로 대용량(1.5B)의 upsampling diffusion model($64 \times 64 \rightarrow 256 \times 256$)을 학습하였다. 또한 CLIP guidance를 위해 앞서 말했던 것처럼 $64 \times 64$의 image에 대해 ViT-L CLIP 모델을 noised input과 함께 학습하였다.&lt;/p&gt;

&lt;h3 id=&quot;training-text-conditional-diffusion-models&quot;&gt;Training text conditional diffusion models&lt;/h3&gt;

&lt;p&gt;이 논문에서는 Diffusion beat GANs 논문에서 밝힌 ADM model을 사용하였다. 해당 구조를 서칭하는 과정은 diffusion 관련 논문들을 정리한 게시글(&lt;a href=&quot;https://6unoyunr.github.io/blog/diffusionpapers&quot;&gt;참고 링크&lt;/a&gt;)에 있지만 결론부터 가져오자면,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각 resolution마다 2개의 residual block(BigGAN)을 가지며, width도 resolution에 맞게 조정됨&lt;/li&gt;
  &lt;li&gt;Attention head마다 $64$의 channel 수를 가지는데, resolution $32, 16, 8$에 모두 attention layer가 있음&lt;/li&gt;
  &lt;li&gt;BigGAN residual block을 upsampling, downsampling할 때 사용하며 AdaGN이 들어가서 timestep과 class embedding을 넣어줌&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이렇게 된다. 이때 기존 ADM에서는 학습 과정에 class embedding과 time embedding을 attention으로 넣어주는 과정을 통해 class conditioning을 진행했는데, GLIDE에서는 해당 부분을 transformer로부터 나오는 text embedding으로 대체하였다. 이 부분은 논문에서 설명이 충분치 않기 때문에 github official code와 함께 보도록 해보자.&lt;/p&gt;

&lt;h3 id=&quot;transformer를-통해-text-embedding-뽑아내기&quot;&gt;Transformer를 통해 text embedding 뽑아내기&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positional_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padding_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_ln&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_ln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xf_proj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# NLC -&amp;gt; NCL
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_proj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Text2ImUNet&lt;/code&gt; 클래스에 속한 메소드 중 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_text_emb&lt;/code&gt;에 대한 내용이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_in&lt;/code&gt;이 transformer에 들어가는 prompt input을 tokenize 및 임베딩화 + positional encoding으로 바꾸게 되고 모든 transformer 모듈 연산이 끝난 후 실제로 conditioning에 사용되는 것은 output의 가장 마지막 token에 대한 embedding(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_out[:, -1]&lt;/code&gt;)를 dimension에 맞게끔 projection한 결과가 된다. Output의 가장 마지막 token이 가지는 의미는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_ctx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Transformer tokenizer는 인코딩 과정에서 BPE를 사용하게 되는데, 이때 시퀀스를 같은 길이로 맞춰주면서 가장 뒷부분에 시퀀스의 마지막을 의미하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;EOS&amp;gt;&lt;/code&gt;를 추가해준다. 따라서 ViT에서 가장 앞부분에 class token을 추가한 뒤 encoder를 통과시켜 해당 token의 feature map을 class에 대한 정보로 사용했던 것처럼, Text embedding 또한 transformer를 통과한 뒤 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;EOS&amp;gt;&lt;/code&gt; 토큰에 남은 나머지 시퀀스에 대한 attention 정보를 사용하면 이는 곧 전체 text를 요약한 feature로 해석할 수 있다.  즉, class embedding 대신 conditioning에 사용될 수 있게 된다.&lt;/p&gt;

&lt;h3 id=&quot;unet-module-conditioning-하기&quot;&gt;UNet module conditioning 하기&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestep_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xf_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text_outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_text_emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xf_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xf_proj&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xf_out&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실제 모델 포워딩(UNet)에서 사용하는 과정을 보게 되면, 위와 같은 방식으로 추출한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_proj&lt;/code&gt;와 feature dimension과 token index의 위치는 permute한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_out&lt;/code&gt;를 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_proj&lt;/code&gt;는 시간에 대한 정보인 timestep embedding과 더해지고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_out&lt;/code&gt;는 그대로 모듈에 들어가게 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;middle_block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xf_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;module이라고 되어있는 부분은 상속된 상위 클래스인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNet&lt;/code&gt; 에서 확인할 수 있듯이 TimestepBlock 혹은 AttentionBlock으로 이어지게 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TimestepEmbedSequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TimestepBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TimestepBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AttentionBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;레이어의 속성에 따라 사용되는 text condition이 서로 다른 것을 볼 수 있는데, 예컨데 ResBlock과 같이 TimestepBlock을 상속 클래스로 하는 녀석들은 time+text embedding(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xf_proj&lt;/code&gt;) 정보를 받아서 group normalization할 때 사용하고(기존 ADM과 똑같다),&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_scale_shift_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_rest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emb_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_rest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb_out&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;AttentionBlock을 클래스로 하는 녀석들은 모든 text token information을 key/value로 cross-attention에 사용하여 query의 주체가 되는 noised image &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;가 text 정보랑 잘 엮일 수 있게 연산된단.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spatial&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;qkv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_kv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proj_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spatial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 구조 설명에서 말했던 바와 같이 resolution $32, 16, 8$에 모두 attention layer가 있다고 했기 때문에 각각의 attention map에 맞게 projection되어(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.qkv(self.norm(x).view(b, c, -1))&lt;/code&gt;) key/value 역할을 수행한다.&lt;/p&gt;

&lt;h3 id=&quot;training-dataset-and-architecture&quot;&gt;Training dataset and architecture&lt;/h3&gt;

&lt;p&gt;Dataset은 DALL-E에서 사용한 것과 동일한 데이터셋으로 학습하였다. 다만 Diffusion beat GAN 논문에서는 width를 샘플링 효율 대비 연산량 때문에 크게 증가시키지 않았지만 이 논문에서는 보다 풍부한 text 정보를 담기 위해 과감하게 $512$ 채널로 증가시켜버린다. Transformer의 경우 채널 수가 $2048$인  $24$개의 residual block을 사용하였고, 이 두 개(UNet + Transformer) 파라미터를 총 합한 것이 3.5B가 된다.&lt;/p&gt;

&lt;p&gt;이에 추가로 upsampling diffusion model은 마찬가지로 같은 conditioning을 사용하는데, 이미지 생성 UNet보다는 적은 규모의 transformer($2048 \rightarrow 1024$)를 사용하게 된다. 이외의 training detail은 논문에 있기 때문에 따로 언급은 하지 않겠다.&lt;/p&gt;

&lt;h3 id=&quot;classifier-free-guidance-1&quot;&gt;Classifier free guidance&lt;/h3&gt;

&lt;p&gt;위의 구조는 transformer output에 대해 text conditioning만 해줄 뿐 따로 CLIP guidance나 classifier free 방법이 들어간 것은 아니다. 사전 학습이 모두 끝난 뒤에, $20\%$의 비율에 해당되는 text token sequence를 empty sequence(NULL = $\emptyset$)로 바꾸어 text에 대해 unconditional representation도 함께 학습시킨다(fine-tuning). 이러한 방법을 통해 기존 classifier free guidance 논문에서처럼 네트워크는 unconditional diffusion model $p_\theta(z)$ 그리고 conditional model $p_\theta(z, c)$ 두 확률분포를 모두 가지게 된다. 다만 여기서 $c$가 class embedding에서 text embedding으로 바뀌었다고 생각하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;image-inpainting&quot;&gt;Image inpainting&lt;/h3&gt;

&lt;p&gt;앞서 related에서 설명했듯이 &lt;a href=&quot;https://arxiv.org/pdf/2111.05826.pdf&quot;&gt;Palette&lt;/a&gt; 논문에서는 image to image translation task 각각을 목적으로 학습했다고 하였다. 이외의 논문들은 직접 diffusion model을 해당 task를 목적으로 학습시키지 않았는데, 이렇게 단순히 다른 목적으로 학습된 diffusion forward로 perturbed input을 넣어 noise로 만든 다음 복구하는 작업을 취하게 되면 perturbed edge 부분이 불연속적이고 artifact가 발생한다는 문제가 있다.&lt;/p&gt;

&lt;p&gt;따라서 이 논문에서는 Palette 논문과 유사하게 inpainting에 대해 학습시키게 된다.  Impaint에 사용된 UNet 구조를 보면 포워딩이 다음과 같이 진행된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;x(마스킹 안된 원래의 이미지)에 x를 고대로 복사한 RGB에 masking을 적용한 이미지를 concat한다. 여기에 추가로 inpaint mask를 붙인 총 $7$개의 channel input이 사용된다고 생각하면 된다. (RGB(원래 이미지) + RGB(가려진 이미지) + Mask).&lt;/p&gt;

&lt;p&gt;Input의 channel의 갯수가 바뀌기 때문에 이에 따라 받아들이는 module의 channel 수도 달라지게 되는데, fine-tuning이기 때문에 원래 $3$개의 채널은 사전 학습된 녀석을 가져오고 나머지 channel은 $0$으로 초기화했다고 생각하면 된다. 이에 상응하는 upsampling model은 fine tuning 과정에서 low-resolution 이미지의 전체(원본 RGB)를 넣어주되, high resolution image는 masking된 부분을 제외하고 넣어주었다(아래 코드 참고).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			      &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low_res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;upsampled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interpolate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;low_res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bilinear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;align_corners&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inpaint_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upsampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;즉, 기존 upsampling module이 $256 \times 256$  크기의 noised $x_t$와 $64 \times 64$  GT를 bicubic으로 upsampling한 녀석을 concatenate해서 프로세스를 진행했다면, inpainting의 경우 두 concatenate 사이에 inpainting된 high resolution GT를 mask와 함께 조건부로 넣어준다.&lt;/p&gt;

&lt;h3 id=&quot;noised-clip-models&quot;&gt;Noised CLIP models&lt;/h3&gt;

&lt;p&gt;Diffusion beat GANs 논문에서는 보다 정확한 classifier guidance를 위해 classifier를 각 noise input에 대해 학습했던 것과 같이, GLIDE에서는 CLIP guidance를 위해 CLIP model을 noised image $x_t$에 대해 학습하게 된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결과&quot;&gt;결과&lt;/h1&gt;

&lt;h3 id=&quot;clip-guidance-vs-classifier-free-guidance&quot;&gt;CLIP guidance vs. Classifier-free guidance&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698021-b1c810ca-e900-404c-bf79-f4ae880b3c1b.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;정성적으로 나온 결과는 위의 그림과 같다. 보다 디테일이 잘 살아있고, artifact나 부자연스러운 부분이 CF Guide가 더 적은 것을 확인할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698028-34cbeb76-9949-45e8-b0fe-4b39afb04e0f.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;메트릭 평가에서도 대체로 Classifier-free guidance가 더 좋다는 결과가 나온다. Precision/Recall을 보게 되면 Precision 파트에서 CLIP guidance는 꼬랑지가 휘는 걸 볼 수 있는데, 앞서 리뷰했던 논문에서 밝힌 것과 같이 Recall을 희생하는 만큼 Precision이 오르는 것이 정상적인 trade-off 경향성이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698031-bf332432-d596-4cfe-99e5-5cf16734508d.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결국 precision이 제대로 증가하지 않았다는 것은 그만큼 $P_r$에 $P_g$가 포함되지 않는다는 소리며, 이는 곧 다양한 text prompt를 커버할 수 없는 샘플링이라고 할 수 있다. IS/FID 역시 Classifier free guidance 기준으로 더 넓은 범위를 커버하는 것을 볼 수 있으며, 가장 인상깊은 결과는 CLIP score(유사도)에 따른 FID 경향성이 CLIP으로 직접 guidance를 주는 것보다 classifier free로 가는게 더 효과적인 것을 알 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698035-f6822431-04d9-42e2-9ebf-7a695732f510.png&quot; width=&quot;500&quot; /&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235698039-b32dd194-cb2a-4f15-8cbc-d2f9cb5b5ea0.png&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;사람이 평가한 Elo score 상으로도 Classifier free guidance가 상당히 높은 점수를 획득하였으며, 사실적 이미지(photorealism)와 캡션 유사도(Caption similarity) 측면에서 DALL-E 보다도 좋은 성능을 보였다. DALL-E에 reranking(CLIP 점수를 이용하여 좋은 샘플을 우선시하여 picking하는 것)을 적용하고 GLIDE는 reranking을 적용하지 않고 단순 비교를 한 결과이며 $\%$는 승률을 의미한다. 마지막 row는 GLIDE의 output에 DALL-E의 d-VAE를 적용한 구조가 된다.&lt;/p&gt;
</description>
        <pubDate>Tue, 02 May 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/glide</link>
        <guid isPermaLink="true">http://localhost:4000/blog/glide</guid>
        
        <category>Diffusion model</category>
        
        <category>Text to Image</category>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        
        <category>paper review</category>
        
      </item>
    
      <item>
        <title>ControlNet 논문 이해하기 및 사용해보기</title>
        <description>&lt;h1 id=&quot;들어가며&quot;&gt;들어가며&lt;/h1&gt;
&lt;p&gt;ControlNet의 논문 제목 풀네임은 ‘Adding conditional control to text-to-image diffusion models’이다. 이른바 &lt;u&gt;ControlNet&lt;/u&gt;이라고 불리는 이번 연구는 사전 학습된 large diffusion model을 어떻게 하면 &lt;strong&gt;input condition&lt;/strong&gt;에 맞게 &lt;u&gt;효율적인 knowledge transfer&lt;/u&gt;이 가능할지에 대해 논의한 페이퍼이다.  Diffusion model이라는 말이 들어갔지만 기존에 리뷰했던 디퓨전 베이스 페이퍼와는 완전히 다른 방향의 연구에 해당된다. 오히려 최근 LLM(Large Language Model)을 파라미터 효율적으로 학습하는 연구 방향인 Parameter efficient fine tuning과 연결짓는 편이 더 합리적이다. 실제로 코드를 받아서 실험해보았을 때 저자들이 제시한 ControlNet 구조를 학습시키는 과정은 서버용 GPU가 아닌 &lt;u&gt;개인 GPU로도 충분히 학습 가능&lt;/u&gt;하며, 가장 눈에 띄는 장점은 ControlNet은 어떠한 input condition에 대해서도 학습이 가능하다는 점이다. 방법론으로 들어가게 되면 ControlNet의 가장 메인 포인트라고 할 수 있는 ‘zero convolution’이 등장하는데, 과연 어떠한 방식으로 input condition을 자유롭게 조정할 수 있게 되었는지 차근차근 살펴보도록 하자.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;input-condition-in-diffusion-models&quot;&gt;Input condition in diffusion models&lt;/h1&gt;

&lt;p&gt;Input condition을 diffusion model에 주는 방식은 사실 이미 존재했었다. 아직 본인 블로그에서는 요즘 가장 핫한 stable diffusion의 근간이 되는 연구인 &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;&gt;latent diffusion 논문&lt;/a&gt;을 따로 다루지는 않았지만 간단하게 소개하자면,&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462245-c6711e21-c0b8-435f-80e9-09fea31ea502.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;예컨데 이미지를 &lt;u&gt;유의미한 semantic 정보만 유지&lt;/u&gt;하고 이미지 생성에 크게 필요하지 않은 high frequency feature를 거르는 vector quantized encoder/decoder를 학습한 상태로 생각하자(즉, 이미지 $x$를 작은 크기의 resolution을 가지는 latent image로 축소한다고 생각하면 된다). 이렇게 축소된 latent를 diffusion process를 통해 복구하는 과정을 학습하는 것이 우리가 일반적으로 이해하고 있는 &lt;strong&gt;DDPM&lt;/strong&gt; 혹은 &lt;strong&gt;DDIM&lt;/strong&gt;의 학습 및 샘플링 프로세스이다.&lt;/p&gt;

&lt;p&gt;우리가 기존에 살펴본 내용 중에서 attention pooling에 시간 정보와 class label 정보를 projection embedding으로 넣어주는 방법론이 있었다(&lt;a href=&quot;https://6unoyunr.github.io/blog/diffusionpapers&quot;&gt;diffusion process conditioning 논문 리뷰글&lt;/a&gt;). 이를 확장시켜 생각하면, 만약 특정 목적을 가지고 condition을 임베딩으로 사영시킬 수 있는 task specific encoder $\tau_\theta$만 있다면, 각 디퓨전 모델 학습 시에 $\tau_\theta$를 통해 추출된 condition vector를 attention layer를 통해 조건화해줄 수 있다. 예컨데 만약 다음과 같은 이미지와 텍스트 description 쌍이 있다고 생각하면(출처 : BLIP 논문),&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Description : The car is driving past a small old building&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462200-a917f64d-6bd3-4c05-9f05-29ef14451152.png&quot; width=&quot;400&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;CLIP의 text encoder와 같은 &lt;u&gt;임의의 텍스트 인코더&lt;/u&gt;를 통해 추출한 embedding을 이미지 생성 시(reverse process)에 조건부로 넣어주게 되면 해당 디퓨전 모델은 샘플링 과정에서 prior에 prompt 조건만 추가해주게 되면 text to image task를 수행할 수 있게 되는 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462194-52d7d549-92c8-4f76-a8eb-a31384addbbd.png&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;단순히 prompt를 통해 위와 같은 고퀄리티의 이미지를 만들 뿐만 아니라, &lt;u&gt;다양한 모달리티에 대한 학습된 encoder&lt;/u&gt;만 있다면 attention pooling 조건화를 통해 diffusion process를 학습시킬 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;naive-conditioning의-단점&quot;&gt;Naive conditioning의 단점&lt;/h1&gt;

&lt;p&gt;이러한 방법들이 가지는 문제점은 상당히 명확하다.&lt;/p&gt;

&lt;p&gt;첫번째로는 diffusion model이 특정 condition에 맞게 학습되려면 그만큼 score network가 &lt;u&gt;해당 condition을 이미지 생성에 잘 반영&lt;/u&gt;해야하는데, 이를 달성하기 위한 &lt;strong&gt;학습 데이터&lt;/strong&gt;가 상당히 &lt;u&gt;많이 필요하다는 것&lt;/u&gt;이다. 예컨데 Vision-Language(VL) task는 멀티모달에서 활발히 연구가 되었기 때문에 CLIP, ALIGN과 같은 대량의 데이터셋이 구축되었지만 다른 모달리티(pose to image, semantic to image 등등)은 그렇지가 않다는 것이다. 실제로 LAION-5B와 같이 stable diffusion의 학습 base가 된 데이터셋에 비해서 object shape나 pose 같이 특정 목적성을 가진 데이터셋은 여러 가지 한계점 때문에 대량으로 구축하기 힘들기 때문이다. 대략 &lt;u&gt;수만배 정도 차이&lt;/u&gt;가 난다.&lt;/p&gt;

&lt;p&gt;두번째로, 이미지 생성이나 manipulation 같은 processing 과정이 대량의 데이터를 통해 솔루션을 획득하는 과정은 굉장히 리소스가 많이 든다는 점이다. 첫번째 문제였던 데이터 갯수의 차이를 극복하더라도 &lt;u&gt;사전 학습된 네트워크를 학습하는 것은 장벽&lt;/u&gt;으로 작용하게 된다.&lt;/p&gt;

&lt;p&gt;마지막으로 processing 과정은 problem 정의에 있어 그 형태의 boundary를 예측할 수 없을 정도로 다양하고, 더욱이 발전할 수 있다. 즉 한계가 없는 문제를 해결하는데 있어 greedy한 선택만 취하게 된다면(디퓨전 프로세스를 제한하거나 attention activation을 바꾸는 것) 이는 결국 고차원의 이해가 필요한 작업들(depth, pose 등등)에는 최적화가 힘들다는 것을 의미한다.  말이 조금 복잡하게 표현된 것 같은데 이를 latent diffusion의 방법론을 통해 다시 한 번 언급하자면, latent diffusion process는 사전 학습된 task specific encoder의 embedding output에 conditioning을 의존하게 되므로(embedding을 단순히 샘플링 부분에 넣어주는 과정을 통해 constraints를 줌) 보다 다양한 task에 대한 학습 과정에서 최적의 선택이 아닐 수 밖에 없다는 것이다. 고로 &lt;u&gt;end-to-end 학습을 할 수 있는 방법을 강구&lt;/u&gt;해야한다. 아래의 그림과 같이 기존 방식은 conditioning part와 실제 디퓨전 모델 학습이 end-to-end가 아닌 분리된 형태를 가진다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462202-abcf9761-fbf6-41cd-b966-3d012c7433e9.png&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;controlnet-end-to-end-neural-network&quot;&gt;ControlNet, end-to-end neural network&lt;/h1&gt;

&lt;p&gt;따라서 논문이 문제로 삼은 기존 conditioning의 한계점을 극복하기 위해 저자는 새로운 &lt;u&gt;transfer learning 구조를 제안&lt;/u&gt;하였다.  ControlNet을 간단하게 묘사하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;diffusion model의 parameter를 복사하여 새로운 학습 프레임워크를 원래 parameter와 병렬로 구성한다. 이를 각각 “trainable(학습 가능한)  copy”와 “locked(학습 불가능한) copy”라고 부른다.&lt;/li&gt;
  &lt;li&gt;Locked copy는 기존 network의 성능인 이미지 생성에 필요한 representation을 유지하고 있다고 생각할 수 있다.&lt;/li&gt;
  &lt;li&gt;Trainable copy는 conditional control을 위해 여러 task-specific dataset에 대해 학습되는 프레임워크다.&lt;/li&gt;
  &lt;li&gt;Locked copy와 Trainable copy는 zero convolution을 통해 서로 연결된다. Zero convolution 또한 학습 가능한 레이어에 속한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;대충만 쭉 묘사했는데 사실 이 부분은 그림을 보면 이해가 쉽다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462204-48bf3f2f-bc94-423d-a3f7-a6f4a900382a.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;$x$가 들어가서 $y$가 나오는 구조는 diffusion process에 접목시키게 되면 특정 시점의 noised latent vector $z_{t}$가 input으로 들어가서 다음 시점의 noised latent vector $z_{t-1}$를 예측하는 것과 같다. 회색으로 된 neural network는 원래의 diffusion model로 파라미터가 고정된 채 변하지 않게끔 하면 사전 학습된 디퓨전 모델의 &lt;u&gt;이미지를 만드는 성능을 해치지 않고&lt;/u&gt; 가만히 놔둘 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462207-4ecbc531-2eae-4da1-a9be-b2dcfc6cca9c.png&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;좌측의 얼어있는 친구는 가만 놔두고 우측의 불타는 친구만 condition에 대해 학습한다고 생각하면 된다. Trainable copy이므로 fine-tuning 과정인데 원래의 parameter를 최대한 손상시키기 않겠다는 의도가 보이는 학습 구조가 된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p&gt;그렇다면 구체적으로 어떻게 해당 학습이 효과적으로 conditioning을 할 수 있는지 수식적으로 살펴보도록 하자. 예컨데 conditioning을 하는 neural network block은 흔히 우리가 알고있는 resnet에서의 bottleneck block이나 transformer의 multi-head attention block을 생각하면 된다.&lt;/p&gt;

&lt;p&gt;2D(이미지와 같은 형태)의 feature를 예시로 들어보자. 만약 feature map $x \in \mathbb{R}^{h \times w \times c}$가 정의되어 있다면, neural network block $\mathcal{F}_\Theta(\cdot)$는 블록에 포함되는 parameter $\Theta$를 통해 input feature map $x$를 transform하게 된다.&lt;/p&gt;

&lt;p&gt;[
y = \mathcal{F}_\Theta(x)
]&lt;/p&gt;

&lt;p&gt;바로 이 과정이 앞서 그림에서 봤던 (a)에 해당된다. 이제부터 해당 parameter $\Theta$는 잠궈놓을 것이다(학습하지 않을 것). 그리고 이를 똑같이 복사한 trainable parameter $\Theta_c$는 잠궈놓은 친구와는 다르게 input condition $c$를 input으로 받아 학습에 사용될 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462208-12e86232-15ef-4f0f-b15d-f4d66b69869a.png&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;참고로 더해지는 부분에 대해서는 네트워크가 &lt;u&gt;activation을 저장해놓을 필요가 없기 때문에&lt;/u&gt; 학습 시에 메모리를 $2$배로 가질 필요성도 없어진다. Backpropagation을 통해 계산된 gradient는 학습 가능한 모델에 대해서만 optimization을 진행할 것이기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;zero-convolution&quot;&gt;Zero convolution&lt;/h3&gt;

&lt;p&gt;이때 더해질 때 바로바로 이 논문에서 가장 중요한 녀석인 zero convolution이라는 개념이 사용되는데, 각 neural block의 앞/뒤로 하나씩 붙는다고 생각하면 된다. 앞/뒤에 붙는 녀석들을 각각 $\mathcal{Z}_{\Theta_1}(\cdot), \mathcal{Z}_{\Theta_2}(\cdot)$라고 해보자. 물론 zero-convolution은 feature map의 크기를 변화시키면 안되기 때문에 $1\times 1$ 크기를 가지는 convolution이며 weight와 bias 모두 zero로 초기화된 상태로 학습이 시작된다.&lt;/p&gt;

&lt;p&gt;위의 그림대로 원래의 output $y$에 conditioning 함수를 거친 output을 더하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;[
y_c = \mathcal{F}_\Theta(x) + \mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x + \mathcal{Z}_{\Theta_1}(c)))
]&lt;/p&gt;

&lt;p&gt;여기에서 대체 왜 weight 및 bias가 $0$으로 초기화된 ‘Zero convolution’이 사용되었는지 이유가 등장한다. Zero-convolution은 weight 및 bias가 모두 $0$이므로, input에 상관없이 처음엔 모두 $0$을 output으로 내뱉는다.&lt;/p&gt;

&lt;p&gt;[
\begin{cases}
\mathcal{Z}_{\Theta_1}(c) = 0 \newline
\mathcal{F}_{\Theta_c}(x+\mathcal{Z}_{\Theta_1}(c)) = \mathcal{F}_{\Theta_c}(x) = \mathcal{F}_{\Theta}(x) \newline
\mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x + \mathcal{Z}_{\Theta_1}(c))) = \mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x)) = 0
\end{cases}
]&lt;/p&gt;

&lt;p&gt;즉 처음에는 $y_c = y$로 시작하게 된다. 해당 내용이 암시하는 것은 training이 시작되는 당시에는 ControlNet 구조에 의한 input/output 관계가 사전 학습된 diffusion의 input/output과 전혀 차이가 없다는 것이고, 이로 인해 optimization이 진행되기 전까지는 neural network 깊이가 증가함에 따라 영향을 끼치지 않는다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;gradient-flow-in-zero-convolution&quot;&gt;Gradient flow in zero convolution&lt;/h3&gt;

&lt;p&gt;$1 \times 1$ convolution 구조를 가지는 zero convolution에 대한 연산 과정에 local gradient를 유도할 수 있다. 예컨데 input feature map $I \in \mathbb{R}^{h \times w \times c}$가 있을때 forward pass는&lt;/p&gt;

&lt;p&gt;[
\mathcal{Z}(I,; \{W, B\})_{p, i} = B_i + \sum_{j}^c I_{p, i}W_{i, j}
]&lt;/p&gt;

&lt;p&gt;이처럼 표현되고, zero convolution은 최적화 전까지는 $W = 0, B = 0$이기 때문에 $I_{p, i}$가 $0$이 아닌 모든 point에 대해서&lt;/p&gt;

&lt;p&gt;[
\begin{cases}
\frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial B_i} = 1\newline
\frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial I_{p, i}} = \sum_{j}^cW_{i,j} = 0 \newline
\frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial W_{i, j}} = I_{p, i} \neq 0
\end{cases}
]&lt;/p&gt;

&lt;p&gt;위와 같이 정리된다. Input에 대한 gradient는 $0$으로 만들지만 weight나 bias에 대한 gradient는 $0$이 아니기 때문에 학습이 가능하다. 왜냐하면 first step만 지나게 되면 Hadamard product 기호인 $\odot$에 대해&lt;/p&gt;

&lt;p&gt;[
W^\ast = W-\beta_\text{lr} \cdot \frac{\partial \mathcal{L}}{\partial \mathcal{Z}(I; \{W, B\})} \odot \frac{\partial \mathcal{Z}(I; {W, B})}{\partial W} \neq 0
]&lt;/p&gt;

&lt;p&gt;$0$이 아닌 weight를 만들기 때문에 바로 다음 step에서는&lt;/p&gt;

&lt;p&gt;[
\frac{\partial \mathcal{Z}(I; \{W^\ast, B\})_{p, i}}{\partial I_{p, i}} = \sum_{j}^cW^\ast_{i,j} \neq 0
]&lt;/p&gt;

&lt;p&gt;학습이 잘된다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;stable-diffusion--controlnet&quot;&gt;Stable diffusion + ControlNet&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462212-cafb40f1-f222-4560-9491-52d370dd512f.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위에서 설명한 구조를 기존 stable diffusion에 구현한 구조는 위와 같다. Loss는 기존 diffusion algorithm에 task specific condition $c_f$만 추가된 형태가 된다.&lt;/p&gt;

&lt;p&gt;[
\mathcal{L} = \mathbb{E}_{z_0, t, c_t, c_f, \epsilon \sim \mathcal{N}(0, 1)}\left( \parallel \epsilon - \epsilon_\theta(z_t, t, c_t, c_f) \parallel_2^2 \right)
]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;결과&quot;&gt;결과&lt;/h1&gt;

&lt;h3 id=&quot;canny-edge&quot;&gt;Canny Edge&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462216-b7291a87-32f6-4ac2-983f-b7193936fd35.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;hough-line&quot;&gt;Hough Line&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462217-4d045efa-eddb-4450-9ec2-92a3ad0cb070.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;scribble&quot;&gt;Scribble&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462218-4986d394-2c30-4e23-b3fd-18f65c34bcb9.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;hed-edge&quot;&gt;HED edge&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462222-f9f9d56d-ee9f-4e1c-af07-9fb29cd10880.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;pose&quot;&gt;Pose&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462224-f1f5b4d8-579f-473e-b033-aad1b955a387.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;segmentation&quot;&gt;Segmentation&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462226-5209cf47-c377-400e-add5-07738a63644b.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;depth&quot;&gt;Depth&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462230-9f60b1a4-d1d7-41c3-a5d8-a3b74f07af57.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;cartoon-line-drawing&quot;&gt;Cartoon line drawing&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462232-8dbc6b0e-ad70-41f6-aa4b-fb2d91c40f60.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;official-code로-직접-실행해보기&quot;&gt;Official Code로 직접 실행해보기&lt;/h1&gt;

&lt;p&gt;현재 official code는 &lt;a href=&quot;https://github.com/lllyasviel/ControlNet.git&quot;&gt;깃허브 소스&lt;/a&gt;로 제공되고 있다. 엥간하면 로컬 서버에서 돌아가기는 하는데 안정적으로 돌릴라면 서버에서 돌리는게 좋다. 여기다가 실행법은 올리겠지만 원본 페이지에 들어가서 ⭐ 한번씩 눌러주면 좋을 것 같다. 다음 repository를 클론 후&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/lllyasviel/ControlNet.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Conda 가상 환경을 설치해준다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;ControlNet
conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; environment.yaml
conda activate control
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그런 뒤 사용하고자 하는 모델과 stable diffusion을 &lt;a href=&quot;https://huggingface.co/lllyasviel/ControlNet&quot;&gt;Hugging Face Page&lt;/a&gt;로부터 다운받으면 된다. 다운받는 위치는 ControlNet/models에 stable diffusion ckpt를 넣고 detector를 ControlNet/annotator/ckpts에 넣으면 된다.&lt;/p&gt;

&lt;h3 id=&quot;detector모두-다운받는-코드controlnet-레포지에서-실행&quot;&gt;Detector(모두) 다운받는 코드(ControlNet 레포지에서 실행)&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ./annotator/ckpts
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;models모두-다운받는-코드controlnet-레포지에서-실행&quot;&gt;Models(모두) 다운받는 코드(ControlNet 레포지에서 실행)&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ./models
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth
curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;마찬가지로 굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;데모-버전-api-실행하기&quot;&gt;데모 버전 API 실행하기&lt;/h3&gt;

&lt;p&gt;원하는 모델을 실행하는 코드는 간단하게&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python gradio_어쩌구2어쩌구.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;를 실행하면 되는데, 만약 서버컴에서 이걸 실행하고 로컬에서 접속하고 싶다면 코드를 살짝만 바꿔주면 된다. 예컨데 모든 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gradio_어쩌구2저쩌구.py&lt;/code&gt; 파일 코드를 보게 되면 가장 마지막 줄에&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;launch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0.0.0.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;요 친구가 있는데 이걸 다음과 같이 바꿔주면 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;launch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0.0.0.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;share&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;본인은 대충&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradio_scribble2image_interactive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이걸 실행해보겠다. 제대로 실행되면 다음처럼 나온다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462235-7dd62696-6f57-4ae8-a0ee-640a2b1cf4e8.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;대강 public URL은 72시간 동안 유효하다는 뜻, 본인은 연세 vpn으로 서버컴에 접속한 상태지만 노트북으로 들어가보겠다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462236-0d8e2563-80b2-45a8-9b61-46aa33e51c64.png&quot; width=&quot;900&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;다음과 같은 화면이 뜬다. 실제로 잘 되는지 확인해보자.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462237-2678713e-f6ad-441a-aed9-0debfa373a47.png&quot; width=&quot;700&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;비루한 그림실력.. 힘내라 ControlNet&lt;/p&gt;

&lt;p&gt;Run 버튼을 누르자 DDIM sampler가 동작하기 시작한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462239-0268dd2a-2e93-45fa-aff1-f9261b635f62.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/79881119/235462241-f63bdb2f-ddd6-437f-9aa0-e27094bbe81c.png&quot; width=&quot;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;그림을 못그려도 인생 살기 큰 문제 없다는 긍정적인 희망이 생기는 논문이었다… 암튼 이렇게 하면 된다. 넉넉잡아 10기가 이상의 GPU면 다 돌아가는 듯하다.&lt;/p&gt;
</description>
        <pubDate>Mon, 01 May 2023 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/blog/controlnet</link>
        <guid isPermaLink="true">http://localhost:4000/blog/controlnet</guid>
        
        <category>Diffusion model</category>
        
        <category>Generative model</category>
        
        <category>Controllable diffusion</category>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        
        <category>paper review</category>
        
      </item>
    
  </channel>
</rss>