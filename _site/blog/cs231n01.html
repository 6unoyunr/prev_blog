<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | cs231n 내용 요약 (1) - Image classification</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | cs231n 내용 요약 (1) - Image classification"><meta name="og:description" content="Lecture summary"><meta name="og:image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"><meta name="og:image:alt" content="Welcome to my blog | cs231n 내용 요약 (1) - Image classification"><meta name="og:url" content="http://localhost:4000/blog/cs231n01"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | cs231n 내용 요약 (1) - Image classification"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | cs231n 내용 요약 (1) - Image classification"><meta name="twitter:description" content="Lecture summary"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Search Engine --><meta name="description" content="Lecture summary"><meta name="image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | cs231n 내용 요약 (1) - Image classification"><meta name="author" content="JY"/><meta itemprop="description" content="Lecture summary"><meta itemprop="image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a> <!--<hr class="navbar-divider"> <a class="navbar-item"> Report an issue </a> --></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { const gitcus_theme = localStorage.theme === 'dark' ? 'dark' : 'light'; function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: gitcus_theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; changeGiscusTheme(); /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/cs231n01" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">cs231n 내용 요약 (1) - Image classification</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Lecture summary</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>November 02, 2022</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span> <span class="tag is-link">cs231n</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 11 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="what-is-image-classification">What is image classification?</h1><p><u>이미지 분류</u>(Image classification)는 컴퓨터비전(computer vision)으로 해결하고자 하는 여러 가지 task(challenge) 중 가장 기본이라고 볼 수 있다. 기존 머신러닝이 해결하기 힘들었던 데이터 수에 따른 성능 수렴을 해결했던 딥러닝 방식은 이미지 분류 대회에서 AlexNet이라는 네트워크가 우승하면서 시작되었다. 대부분의 사람들은 AI라고 하면 그 시작을 알파고로 기억해주는 사람도 많고, 물론 알파고도 RL 분야에서는 상당히 세상의 이목을 집중시켰던 중요한 이벤트긴 하지만 본인에게는 <strong>AlexNet</strong>이 조금 더 인상깊게 다가온다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211229357-29c7e82a-eaa7-4b7e-81ba-81841ed2ec33.png" width="500" /></p><p>다시 image classification으로 돌아와서, 컴퓨터비전에서 해결하고자 하는 이미지 분류는 간단하게 input으로 특정 이미지를 주면 고정된 카테고리 분류들 중 하나에 매칭하는 것이다. 위의 그림은 CIFAR-10 dataset의 예시로, 총 10가지의 클래스로 구분되며 각 이미지를 잘 나타낼 수 있는 label(dog, cat 등등)로 지표화가 된 상태이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211229816-ec1c4c9a-3d7e-4ea7-9f1c-d9af75bf2c66.png" width="500" /></p><p>이러한 이미지를 인간이 보았을 때는 누가 봐도 <u>고양이</u>지만 컴퓨터의 입장은 좀 다르다. 모든 이미지는 컴퓨팅 환경에서 양자화되고, 3차원 메모리의 형태로 각 픽셀 위치에 따른 RGB value로 매핑된다. 컴퓨터가 처음 고양이 이미지를 받아들였을 때 이 숫자들의 나열을 통해 곧바로 ‘고양이’라고 추측할 수 있는 가능성은 10% 뿐이다. 따라서 우리는 컴퓨터로 하여금 <u>최대한 다양한 고양이 이미지들의 숫자 배열 사이에서 규칙을 찾아내게 하고 싶은 것</u>이다. Image classification의 개요에 대해 간단하게 짚고 넘어왔다.</p><hr /><h1 id="what-is-challenging">What is challenging?</h1><p>더 깊게 들어가기 전에 우선 텐서(Tensor)에 대한 개념을 언급하는게 좋을 것 같다. Matrix(행렬) 개념을 일반화한 형태인 Tensor(텐서)는 숫자 혹은 데이터의 배열이다. Matrix에서의 rank 개념은 행렬이 가지는 independent vector의 개수이며, 이와 유사하게 Tensor도 rank 개념은 해당 <u>텐서가 가지는 차원 수</u>를 의미한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211230352-c1d20e34-770e-4854-9ee2-e2e4aac8ff5c.png" width="500" /></p><p>만약 어떤 Tensor가 묶음으로 표현될 수 있는 dimension을 $N$개 가지고 있다면, 그 Tensor는 $N$차원의 rank를 가지는 Tensor가 된다. 예를 들어 이미지는 RGB의 채널을 가지는 $H \times W$ 크기의 matrix 모음이기 때문에 다음과 같이 표현할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211230627-b36d86ae-f5ae-471f-82c5-3d409ee87f25.png" width="500" /></p><p>고양이 이미지의 각 픽셀은 특정 위치에서의 색을 의미하며(보다 구체적으로는 3차원 공간에서 2차원 카메라로 rendering된 색상) 다양한 색이 빛의 삼원색인 RGB의 조합으로 표현이 가능하다. 디지털 카메라 환경에서 RGB 값은 $0 \sim 255$의 값으로 양자화되며(8-bit unsigned), 이는 <u>computing situation</u>에서도 동일하게 적용된다. 따라서 고양이 이미지를 $H \times W$의 spatial dimension을 가지는 matrix를 R, G, B 세 묶음으로 가지고 있는 Tensor로 표현할 수 있다. <br /> 앞으로의 모든 게시글에서 Image의 resolution은 이미지의 <u>spatial dimension</u>을 의미하고, Image의 channel은 이미지의 <u>RGB</u> 축을 의미한다고 생각하면 된다. 결론은 지금 다루고 있는 이미지 분류 task에서 input으로 사용되는 image의 데이터 형태는 3차원의 Tensor로 해석할 수 있다는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211231096-d6f43001-9449-4c98-8602-511ac5ec87d2.png" width="800" /></p><p>하지만 이렇게 데이터로 표현된 이미지에는 <u>큰 문제가</u> 있다. 만약 특정 피사체를 찍는 각도가 바뀐다면(Viewpoint variation), 분명 같은 이미지임에도 불구하고 데이터 상으로는 크게 상이한 결과가 출력될 것이다. 이미지 분류를 위해 사용되는 image의 데이터 형태를 컴퓨터가 받았을 때, 동일한 object임에도 <u>보는 각도에 따른 value 차이가 생겨서</u> 이를 일반화할 수 있는 예측 알고리즘을 찾기 힘들다는 것이다. <br /> 마찬가지로 조도 환경(Illumination conditions)에 따른 데이터 차이도 문제가 된다. 단순히 밤에서 낮으로 바뀐다거나, 아니면 광원의 위치가 바뀌어서 우리가 보는 각도에서의 전경과 후경의 색감에 차이가 생기게 되면 위의 경우와 마찬가지로 같은 장면에 대해 잘못된 예측 알고리즘을 적용할 수 있다. <br /> 이를 제외하고도 scale variation(동일한 사물이지만, 카메라가 가까워지고 멀어짐에 따라 물체의 크기가 다양해질 수 있고 이로 인해 잘못된 예측을 할 수 있음), Deformation(예를 들어 고양이는 <u>액체이므로</u> dynamic한 pose를 보여주는데, 이러한 사물 변형에 대해 robust한 모델링이 힘들다는 것), 물체와 배경 색이 유사해서(보호색) 분류에 차질이 생긴다던지 전경이 후경을 가려서 물체의 구분을 방해한다던지 그리고 같은 class의 물체에 대해서도 다양한 모습이 존재할 수 있다는 문제점 등등 <u>image를 통한 machine learning에 제약이 많다는 것</u>을 알 수 있다. 이런 모든 데이터의 가능성에 대해서 네트워크를 구성한다는 것은 불가능에 가까우며, 여기서 바로 딥러닝의 근간이 될 <u>data-driven algorithm</u>을 해결책으로 사용하게 된다.</p><hr /><h1 id="data-driven-algorithm">Data-driven algorithm</h1><p>데이터 기반 알고리즘은 <u>귀납적 추리</u>에 가깝다. 최단 경로 찾기 문제나 이진 탐색과 같이 어느 정도 한정된 자원을 가정하고 시작하는 알고리즘과는 다르게 image classification과 같은 task에서 해결하고자 하는 문제는 위에서 설명한 다양한 <u>challenging situation</u>에 무관한 예측이 가능한 알고리즘이다. 단순히 ‘의자’라는 객체만 하더라도 세상에는 정말 다양한 종류의 의자가 있으며, 심지어 의자의 고정관념을 깨는 예술품들이 등장할 수도 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211232116-3bb0a20c-90ef-4d53-92b8-60d0d0f519aa.png" width="600" /></p><p>솔직히 말하자면 사람도 오른쪽 그림을 보고 의자라고 말하긴 조금 힘들 것 같은데, 아무튼 여러 조도 환경, 카메라의 회전이나 위치 등등 모든 상황에 대처할 수 있는 알고리즘을 구성하고자 하는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211232302-e4a7ba44-0aab-4294-abcb-886ea40e56d6.png" width="600" /></p><p>적게는 수천장부터, 많게는 수백/억의 데이터를 통해 이를 구현하고자 한다. 최대한의 데이터를 통한 최적화 과정으로, <strong>관측할 수 없는</strong> 세상의 모든 <u>객체 데이터</u>에 적용할 수 있는 알고리즘을 찾는 과정이다. 앞으로 언급할 대부분의 딥러닝 알고리즘은 바로 이러한 방법을 사용하게 된다.</p><hr /><h1 id="image-classification-pipeline">Image classification pipeline</h1><p>이미지 분류를 앞서 언급한 개념들을 토대로 진행하는데, 총 <u>3개의 구성 요소</u> 혹은 <u>단계</u>로 구분할 수 있다. 그 중 첫번째는 <strong>입력 데이터</strong>이다. $K$개의 라벨(class의 개수)에 각각 매칭되어있는 $N$개의 이미지를 data-driven 최적화 과정에 사용한다. 이를 ‘Training data’라고 부른다. Supervision(지도) 학습에서는 training data가 이처럼 라벨링이 되어있어야한다. <br /> 두번째로는 학습 단계인 <strong>learning</strong>이다. 이 단계에서는 최적화 과정에서 사용할 목적 함수 혹은 알고리즘을 정의하게 된다. 첫번째 구성 요소였던 training data를 활용하여 최대한 좋은 성능을 내고자 하는 것이 두번째 단계가 된다. <br /> 마지막 세번째는 <strong>evaluation</strong>이다. 최적화 과정에서 실제로 학습 과정 중인 네트워크가 얼마나 좋은 성능을 내는지 확인하는 과정이 필요하고, 이 과정에서 사용될 수 있는 데이터가 필요하다. Training data와 마찬가지로 성능을 확인할 수 있어야하므로 데이터에는 지표화가 되어있어야 하며, 이를 validation/test data라고 부른다. 알고리즘이 예측한 결과에 대응되는 지표는 ground truth라고 부른다.</p><hr /><h1 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h1><p>지금 소개하고자 하는 classifier는 딥러닝에서 사용하는 neural network 구조는 아니다. 상당히 비효율적인 알고리즘을 소개할 것인데, 이를 언급하는 이유는 <u>data-driven algorithm</u>이랑 <u>gradient descent algorithm</u>이랑 혼동하지 않는 것이 중요하기 때문이다. 두 알고리즘은 머신러닝과 딥러닝의 관계와 비슷하게 말할 수 있다. data-driven algorithm을 활용한 함수 최적화 방식에 gradient descent algorithm이 있을 수 있지만 두 개념은 <u>서로 다른 개념</u>이기 때문에 꼭 NN형태의 딥러닝이 아니더라도 data-driven approach를 사용할 수 있다. 예를 들어 gradient descent 방식이나 WGAN-GP(Wessertein-GAN with gradient penalty) 방식 등등 현재 딥러닝에서 활용 및 언급되는 모든 최적화 알고리즘은 이전에도 이미 존재했었고, AI 분야가 아닌 통신, 수학 등등에서 주로 사용되던 것이었다. <br /> Nearest neighbor classifier는 추론을 위해 모든 데이터가 필요하다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211233597-11a527e0-72e3-49b4-a8cc-1093508f674f.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/211233625-49c6f4f6-22a9-4059-8028-0a25888c70d8.png" width="400" /></p><p>왼쪽 그림은 CIFAR-10의 몇몇 샘플을 보여주는 그림이고, 오른쪽 그림은 test image와 비교했을때, training image와 가장 유사하다고 판단한 그림 10장을 보여준 모습이다. 가장 가까운 이웃을 찾는 알고리즘 이름에 맞게 유사함의 정도를 측정하는 메트릭을 정하기만 하면 training image와 test image 사이의 거리를 비교함으로써 nearest neighbor이 매핑된 클래스가 곧 test image에 대해 예측하고자 하는 클래스가 된다는 것. 흔히 거리를 계산하는 방법은 다음과 같다. Distance와 norm의 개념은 ground truth가 있는 텐서, 행렬 혹은 벡터 간 연산에서 주로 활용되는 편인데, order $p$에 따라 다르게 표현이 가능하다. Norm과 distance의 차이는 norm은 벡터의 크기를 표현하고, distance는 두 벡터의 차이에 대한 크기라고 생각하면 된다. Norm으로 나타낸 벡터의 크기는 <u>원점에서 벡터 좌표까지의 거리</u> 혹은 <u>magnitude</u>라 부른다.</p><p>[ L_p = \left( \sum_i^n \vert x_i \vert^p \right)^{\frac{1}{p}} <br /> ]</p><p>Order 값을 $p$로 가지는 norm을 $L_p$로 표현하며, 식은 위에서 보는 바와 같다. 위의 식은 $x$를 $n$차원의 벡터라고 가정하고 작성하였다. 주로 사용되는 norm은 $L_1$ norm 그리고 $L_2$ norm이다. 위의 식에 $p = 1$을 대입하면 다음과 같다.</p><p>[ \begin{aligned} L_1 =&amp; \left( \sum_i^n \vert x_i \vert \right) \newline =&amp; \vert x_1 \vert + \vert x_2 \vert + \vert x_3 \vert + \cdots + \vert x_n \vert \end{aligned} <br /> ] L1 norm은 Taxicab norm 혹은 Manhattan norm이라고도 한다. 식을 보면 알 수 있듯이 각 요소의 절댓값에 대한 합으로 정의된다. 이번에는 $p = 2$를 대입해보도록 하자.</p><p>[ \begin{aligned} L_2 =&amp; \sqrt{\sum_i^n x_i^2 } \newline =&amp; \sqrt{x_1^2 + x_2^2 + x_3^2 + \cdots + x_n^2 } \end{aligned} <br /> ]</p><p>아마 일반적인 사람들이 알고 있는 원점에서의 벡터 좌표까지 거리를 구하는 공식일 것이다. 유클리드 공간에서의 벡터 크기를 측정하는 공식이기 때문에 Euclidean norm이라 불린다. 그리고 벡터 연산을 통해 inner product 형태로 바꿔볼 수도 있다.</p><p>[ \begin{aligned} L_2 =&amp; \sqrt{\sum_i^n x_i^2 } \newline =&amp; \sqrt{x \cdot x} \newline =&amp; \sqrt{x^\top x} \newline =&amp; \sqrt{x_1^2 + x_2^2 + x_3^2 + \cdots + x_n^2 } \end{aligned} <br /> ]</p><p><u>Distance</u>는 벡터 차원을 확장해서 생각해보면, 서로 다른 두 텐서의 <u>차이에 대한 norm</u>을 구하는 것과 같다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211241290-4282638c-9493-4854-896a-da54eac738da.png" width="600" /></p><p>다시 원래 설명하던 내용으로 돌아와서, nearest neighbor 방식은 위와 같은 거리 공식(metric)을 기반으로 test image와 training image 사이의 거리를 구한다. 위는 $L_1$ distance 예시이고, $L_2$ distance을 적용해서 구할 수도 있다. 이렇게 구한 distance 기준으로 각 training image와의 최소 거리를 구한 뒤, 가장 거리가 가까운 이미지의 라벨을 기준으로 test image의 라벨을 예측하게 된다. <br /> $L_1$, $L_2$ distance 어떤 걸 사용하는지에 따른 장단점이 다른데, 일반적으로 제곱으로 계산되는 $L_2$ distance가 $L_1$에 비해 vector 거리에 대해 더 unforgiving한 면이 있다. 거리 공식 자체가 제곱이다보니, 직선 형태로 그려지는 $L_1$ distance보다 1보다 큰 값의 오차에 대해 quadratic하기 때문이다. Unforgiving이라는 말은 같은 정도의 오차가 발생했을때 penalty를 더 많이 준다는 의미와 같다. <br /> Nearest neighbor 방법은 매우 단순하면서 간단하지만, 가장 가까운 거리의 image만을 판별 기준으로 삼는다면 정확도가 많이 떨어질 수도 있다. Image 벡터 사이의 거리만 비교한다면 그냥 <u>육안상 색감이 비슷하지만, 서로 다른 object에 속하는 두 이미지</u>를 같은 class로 결정할 수 있기 때문이다. 이를 방지하고자, 보다 많은 표본을 통해 예측하려고 한다. 바로 이 방법이 뒤이어 바로 설명할 KNN($K$-nearest neighbor)이고, 이 방법에서는 거리 비교 후 <u>가장 가까운 거리의 image를 포함한 $k$개의 sample</u>을 통해 예측하게 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211242160-3f49e1f0-aa21-4000-97bd-bd8706b15c01.png" width="800" /></p><p>그림에서 볼 수 있듯이 만약 $k$가 적다면 가장 가까운 거리의 샘플을 기반으로 예측이 되므로 경계선이 끊어지거나 prediction map이 불연속인 형태를 가지게 된다. 이와는 다르게 근처 5개의 샘플을 기반으로 주를 이루는 class로 예측을 할 경우 이러한 문제가 어느 정도는 해소되는 것을 확인할 수 있다. 하지만 $K$ 값이 무작정 크면 성능이 오히려 악화되는 문제가 생기는데, 이는 사실 직관적으로 관찰 가능한 사실이지만 이후에 다시 언급하도록 하겠다.</p><hr /><h1 id="hyperparameter-조정">Hyperparameter 조정</h1><p>흔히 네트워크를 학습하는 과정에서 우리가 manually 조정할 수 있는 파라미터를 hyperparameter라고 부른다. 네트워크의 weight와 같은 parameter 개념과는 다르게 하이퍼파라미터는 최적값을 찾기 위해 실험하는 입장에서 직접 grid search를 해야한다. 그렇다면 KNN에서의 $K$를 정하는 것과 $L_1,~L_2$ norm 중 어떤 것이 더 좋은 성능을 보이는지 알기 위해서는, 직접 해당 조건에 따라 실험을 진행한 후, 가장 좋은 성능을 보이는 값을 적용하는 것이다. <br /> 여기서 중요한 점은 training set, test set 이외에 validation set이다. 성능을 평가하는 과정에서 파라미터 조정에는 training dataset이 쓰이지는 않지만 hyperparameter 조정에는 test dataset과 같은 metric의 기준이 필요하다. KNN의 경우에는 training dataset이 일종의 <u>이미지 사전</u> 역할을 하고 test dataset이 <u>찾고 싶은 단어</u>가 되는데, 여기서 test dataset으로 하이퍼파라미터 조정을 하게 되면 일반화된 데이터에 대한 hyperparameter value가 아닌 특정 test dataset에 overfitting된 value가 된다. <br /> 딥러닝을 먼저 공부한 사람 중에서 헷갈리는 사람이 있을까 싶어 다시 언급하자면 지금 설명하는 방법은 neural network 알고리즘이 아닌 KNN 알고리즘이다. 따라서 일반적으로 DNN에서 언급하는 training set에 대한 overfitting과는 다르게 KNN에서는 overfitting이 test set에 대해서 정의되어야 맞다. 결론적으로 두 경우 모두 해결법으로 validation dataset을 통한 모니터링이지만, 두 알고리즘에서의 overfitting이 맥락으로는 비슷해도 서로 다른 것이라는 걸 짚고 넘어가고 싶었다. <br /> 따라서 기존에 분류되던 training data/test data 이외에 validation data가 필요하고, 일반화를 위해 <u>training dataset</u>을 여러 fold로 나눈 뒤 이 중 하나의 fold를 <u>validation dataset</u>으로 사용한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211243687-c6c31a8a-6528-462c-9a5b-384167f73e34.png" width="800" /></p><p>이렇게 되면 학습 과정에서 성능을 평가할 때 여러 fold에 대해서 평균을 구해볼 수 있으므로, hyperparameter인 $K$를 조정하는 과정에서 공평한 비교가 가능해진다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211243813-41facc7a-ed22-48f3-993e-dacfdcbb35dc.png" width="600" /></p><p>이렇게 validation과 training을 여러 fold들에 대해서 cross-validation을 함으로써 얻어진 그래프는 위와 같다. Fold가 5개이므로 각각 validation에 사용될 때의 accuracy가 총 5개나 나올 것이고, 이를 통해 평균 정확도를 구하면 변인에 대해 일반화된 성능을 확인할 수 있다. 이러한 방식은 training data가 현저히 작을 경우 사용할 수 있는 아주 좋은 방법이다. <br /> 결과를 보게 되면 <u>거리 비교가 많아질 때</u>($k$가 커질 때) 그만큼 표본은 많아져서 좋지만, 우리가 학습시킨 데이터가 test data에 대해서 <u>얼만큼 대표적인지 알 수 없기 때문에</u> 성능이 무작정 좋아진다고 할 수 없다. 학습시킨 데이터 기준으로 test data와 <u>유사한데 다른 label을 가진 sample들</u>이 많으면 오히려 독이 될 수 있다는 것이다. 이는 필연적으로 $k$가 계속하여 증가하면 확률적으로 생길 수 있는 일이기 때문에(관측 가능한 데이터는 한정적이므로) 성능이 최대가 되는 지점은 중간인 $k = 7$에 존재한다.</p><hr /><h1 id="한계점">한계점</h1><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211244279-ccbaf27a-a9a5-41b4-ab07-3185fed018b2.png" width="800" /></p><p>우리는 이 4개의 이미지를 보고 서로 다르다고 생각하지 않을 것이다. 분명 약간씩의 왜곡이나 다른 점은 있지만, <u>다 동일한 사람의 이미지를 기반으로 생성</u>된 결과라는 걸 알 수 있다. 그러나 KNN의 경우에는 불가능하다. 2차원 이미지의 경우 이미지 데이터가 옆으로 몇 픽셀만 이동하더라도 <u>같은 사진임에도 불구하고</u> distance가 매우 클 수 있다. 특히나 $L_2$ distance의 경우 모든 픽셀 값들의 차이가 제곱합으로 계산되기 때문에 오차가 더 심하게 나타난다. 그렇기 때문에 KNN은 이미지와 같은 고차원인 데이터에 적용하기가 힘들고, 그렇기 때문에 <strong>neural network</strong>를 활용하는 방법을 고안해낸다. 이 부분부터 다음에 다룰 내용이다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/cs231n00"><p id="prev_title"> ❮❮ cs231n 내용 요약 (0) - What is deep learning?</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/cs231n02"><p id="next_title"> ❯❯ cs231n 내용 요약 (2) - Linear classification</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="gitcus" id="gitcus_container"> <script src="https://giscus.app/client.js" data-repo="6unoyunr/comments" data-repo-id="R_kgDOODmwzQ" data-category="General" data-category-id="DIC_kwDOODmwzc4Cn5wD" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="ko" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
