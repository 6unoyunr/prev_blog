<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기"><meta name="og:description" content="Controllable stable diffusion"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/235468213-09aaeaec-4cd4-4503-8e67-6b279557a6d5.png"><meta name="og:image:alt" content="Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기"><meta name="og:url" content="http://localhost:4000/blog/controlnet"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기"><meta name="twitter:description" content="Controllable stable diffusion"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/235468213-09aaeaec-4cd4-4503-8e67-6b279557a6d5.png"> <!-- Search Engine --><meta name="description" content="Controllable stable diffusion"><meta name="image" content="https://user-images.githubusercontent.com/79881119/235468213-09aaeaec-4cd4-4503-8e67-6b279557a6d5.png"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | ControlNet 논문 이해하기 및 사용해보기"><meta name="author" content="JY"/><meta itemprop="description" content="Controllable stable diffusion"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/235468213-09aaeaec-4cd4-4503-8e67-6b279557a6d5.png"><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBDTZMG942"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GBDTZMG942'); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep_learning1" class="navbar-item has-text-grey-light "> DEEP LEARNING THEORY </a> <a href="http://localhost:4000/category/deep_learning2" class="navbar-item has-text-grey-light "> PAPERS & TECHS </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github_blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/controlnet" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">ControlNet 논문 이해하기 및 사용해보기</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/235468213-09aaeaec-4cd4-4503-8e67-6b279557a6d5.png" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Controllable stable diffusion</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>May 01, 2023</b> by <a href="https://6unoyunr.github.io/mycard" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Diffusion model</span> <span class="tag is-link">Generative model</span> <span class="tag is-link">Controllable diffusion</span> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 10 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="들어가며">들어가며</h1><p>ControlNet의 논문 제목 풀네임은 ‘Adding conditional control to text-to-image diffusion models’이다. 이른바 <u>ControlNet</u>이라고 불리는 이번 연구는 사전 학습된 large diffusion model을 어떻게 하면 <strong>input condition</strong>에 맞게 <u>효율적인 knowledge transfer</u>이 가능할지에 대해 논의한 페이퍼이다. Diffusion model이라는 말이 들어갔지만 기존에 리뷰했던 디퓨전 베이스 페이퍼와는 완전히 다른 방향의 연구에 해당된다. 오히려 최근 LLM(Large Language Model)을 파라미터 효율적으로 학습하는 연구 방향인 Parameter efficient fine tuning과 연결짓는 편이 더 합리적이다. 실제로 코드를 받아서 실험해보았을 때 저자들이 제시한 ControlNet 구조를 학습시키는 과정은 서버용 GPU가 아닌 <u>개인 GPU로도 충분히 학습 가능</u>하며, 가장 눈에 띄는 장점은 ControlNet은 어떠한 input condition에 대해서도 학습이 가능하다는 점이다. 방법론으로 들어가게 되면 ControlNet의 가장 메인 포인트라고 할 수 있는 ‘zero convolution’이 등장하는데, 과연 어떠한 방식으로 input condition을 자유롭게 조정할 수 있게 되었는지 차근차근 살펴보도록 하자.</p><hr /><h1 id="input-condition-in-diffusion-models">Input condition in diffusion models</h1><p>Input condition을 diffusion model에 주는 방식은 사실 이미 존재했었다. 아직 본인 블로그에서는 요즘 가장 핫한 stable diffusion의 근간이 되는 연구인 <a href="https://arxiv.org/abs/2112.10752">latent diffusion 논문</a>을 따로 다루지는 않았지만 간단하게 소개하자면,</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462245-c6711e21-c0b8-435f-80e9-09fea31ea502.png" width="700" /></p><p>예컨데 이미지를 <u>유의미한 semantic 정보만 유지</u>하고 이미지 생성에 크게 필요하지 않은 high frequency feature를 거르는 vector quantized encoder/decoder를 학습한 상태로 생각하자(즉, 이미지 $x$를 작은 크기의 resolution을 가지는 latent image로 축소한다고 생각하면 된다). 이렇게 축소된 latent를 diffusion process를 통해 복구하는 과정을 학습하는 것이 우리가 일반적으로 이해하고 있는 <strong>DDPM</strong> 혹은 <strong>DDIM</strong>의 학습 및 샘플링 프로세스이다.</p><p>우리가 기존에 살펴본 내용 중에서 attention pooling에 시간 정보와 class label 정보를 projection embedding으로 넣어주는 방법론이 있었다(<a href="https://6unoyunr.github.io/blog/diffusionpapers">diffusion process conditioning 논문 리뷰글</a>). 이를 확장시켜 생각하면, 만약 특정 목적을 가지고 condition을 임베딩으로 사영시킬 수 있는 task specific encoder $\tau_\theta$만 있다면, 각 디퓨전 모델 학습 시에 $\tau_\theta$를 통해 추출된 condition vector를 attention layer를 통해 조건화해줄 수 있다. 예컨데 만약 다음과 같은 이미지와 텍스트 description 쌍이 있다고 생각하면(출처 : BLIP 논문),</p><blockquote><p>Description : The car is driving past a small old building</p></blockquote><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462200-a917f64d-6bd3-4c05-9f05-29ef14451152.png" width="400" /></p><p>CLIP의 text encoder와 같은 <u>임의의 텍스트 인코더</u>를 통해 추출한 embedding을 이미지 생성 시(reverse process)에 조건부로 넣어주게 되면 해당 디퓨전 모델은 샘플링 과정에서 prior에 prompt 조건만 추가해주게 되면 text to image task를 수행할 수 있게 되는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462194-52d7d549-92c8-4f76-a8eb-a31384addbbd.png" width="800" /></p><p>단순히 prompt를 통해 위와 같은 고퀄리티의 이미지를 만들 뿐만 아니라, <u>다양한 모달리티에 대한 학습된 encoder</u>만 있다면 attention pooling 조건화를 통해 diffusion process를 학습시킬 수 있다.</p><h1 id="naive-conditioning의-단점">Naive conditioning의 단점</h1><p>이러한 방법들이 가지는 문제점은 상당히 명확하다.</p><p>첫번째로는 diffusion model이 특정 condition에 맞게 학습되려면 그만큼 score network가 <u>해당 condition을 이미지 생성에 잘 반영</u>해야하는데, 이를 달성하기 위한 <strong>학습 데이터</strong>가 상당히 <u>많이 필요하다는 것</u>이다. 예컨데 Vision-Language(VL) task는 멀티모달에서 활발히 연구가 되었기 때문에 CLIP, ALIGN과 같은 대량의 데이터셋이 구축되었지만 다른 모달리티(pose to image, semantic to image 등등)은 그렇지가 않다는 것이다. 실제로 LAION-5B와 같이 stable diffusion의 학습 base가 된 데이터셋에 비해서 object shape나 pose 같이 특정 목적성을 가진 데이터셋은 여러 가지 한계점 때문에 대량으로 구축하기 힘들기 때문이다. 대략 <u>수만배 정도 차이</u>가 난다.</p><p>두번째로, 이미지 생성이나 manipulation 같은 processing 과정이 대량의 데이터를 통해 솔루션을 획득하는 과정은 굉장히 리소스가 많이 든다는 점이다. 첫번째 문제였던 데이터 갯수의 차이를 극복하더라도 <u>사전 학습된 네트워크를 학습하는 것은 장벽</u>으로 작용하게 된다.</p><p>마지막으로 processing 과정은 problem 정의에 있어 그 형태의 boundary를 예측할 수 없을 정도로 다양하고, 더욱이 발전할 수 있다. 즉 한계가 없는 문제를 해결하는데 있어 greedy한 선택만 취하게 된다면(디퓨전 프로세스를 제한하거나 attention activation을 바꾸는 것) 이는 결국 고차원의 이해가 필요한 작업들(depth, pose 등등)에는 최적화가 힘들다는 것을 의미한다. 말이 조금 복잡하게 표현된 것 같은데 이를 latent diffusion의 방법론을 통해 다시 한 번 언급하자면, latent diffusion process는 사전 학습된 task specific encoder의 embedding output에 conditioning을 의존하게 되므로(embedding을 단순히 샘플링 부분에 넣어주는 과정을 통해 constraints를 줌) 보다 다양한 task에 대한 학습 과정에서 최적의 선택이 아닐 수 밖에 없다는 것이다. 고로 <u>end-to-end 학습을 할 수 있는 방법을 강구</u>해야한다. 아래의 그림과 같이 기존 방식은 conditioning part와 실제 디퓨전 모델 학습이 end-to-end가 아닌 분리된 형태를 가진다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462202-abcf9761-fbf6-41cd-b966-3d012c7433e9.png" width="1000" /></p><hr /><h1 id="controlnet-end-to-end-neural-network">ControlNet, end-to-end neural network</h1><p>따라서 논문이 문제로 삼은 기존 conditioning의 한계점을 극복하기 위해 저자는 새로운 <u>transfer learning 구조를 제안</u>하였다. ControlNet을 간단하게 묘사하면 다음과 같다.</p><ul><li>diffusion model의 parameter를 복사하여 새로운 학습 프레임워크를 원래 parameter와 병렬로 구성한다. 이를 각각 “trainable(학습 가능한) copy”와 “locked(학습 불가능한) copy”라고 부른다.</li><li>Locked copy는 기존 network의 성능인 이미지 생성에 필요한 representation을 유지하고 있다고 생각할 수 있다.</li><li>Trainable copy는 conditional control을 위해 여러 task-specific dataset에 대해 학습되는 프레임워크다.</li><li>Locked copy와 Trainable copy는 zero convolution을 통해 서로 연결된다. Zero convolution 또한 학습 가능한 레이어에 속한다.</li></ul><p>대충만 쭉 묘사했는데 사실 이 부분은 그림을 보면 이해가 쉽다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462204-48bf3f2f-bc94-423d-a3f7-a6f4a900382a.png" width="700" /></p><p>$x$가 들어가서 $y$가 나오는 구조는 diffusion process에 접목시키게 되면 특정 시점의 noised latent vector $z_{t}$가 input으로 들어가서 다음 시점의 noised latent vector $z_{t-1}$를 예측하는 것과 같다. 회색으로 된 neural network는 원래의 diffusion model로 파라미터가 고정된 채 변하지 않게끔 하면 사전 학습된 디퓨전 모델의 <u>이미지를 만드는 성능을 해치지 않고</u> 가만히 놔둘 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462207-4ecbc531-2eae-4da1-a9be-b2dcfc6cca9c.png" width="500" /></p><p>좌측의 얼어있는 친구는 가만 놔두고 우측의 불타는 친구만 condition에 대해 학습한다고 생각하면 된다. Trainable copy이므로 fine-tuning 과정인데 원래의 parameter를 최대한 손상시키기 않겠다는 의도가 보이는 학습 구조가 된다.</p><hr /><h1 id="method">Method</h1><p>그렇다면 구체적으로 어떻게 해당 학습이 효과적으로 conditioning을 할 수 있는지 수식적으로 살펴보도록 하자. 예컨데 conditioning을 하는 neural network block은 흔히 우리가 알고있는 resnet에서의 bottleneck block이나 transformer의 multi-head attention block을 생각하면 된다.</p><p>2D(이미지와 같은 형태)의 feature를 예시로 들어보자. 만약 feature map $x \in \mathbb{R}^{h \times w \times c}$가 정의되어 있다면, neural network block $\mathcal{F}_\Theta(\cdot)$는 블록에 포함되는 parameter $\Theta$를 통해 input feature map $x$를 transform하게 된다.</p><p>[ y = \mathcal{F}_\Theta(x) ]</p><p>바로 이 과정이 앞서 그림에서 봤던 (a)에 해당된다. 이제부터 해당 parameter $\Theta$는 잠궈놓을 것이다(학습하지 않을 것). 그리고 이를 똑같이 복사한 trainable parameter $\Theta_c$는 잠궈놓은 친구와는 다르게 input condition $c$를 input으로 받아 학습에 사용될 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462208-12e86232-15ef-4f0f-b15d-f4d66b69869a.png" width="500" /></p><p>참고로 더해지는 부분에 대해서는 네트워크가 <u>activation을 저장해놓을 필요가 없기 때문에</u> 학습 시에 메모리를 $2$배로 가질 필요성도 없어진다. Backpropagation을 통해 계산된 gradient는 학습 가능한 모델에 대해서만 optimization을 진행할 것이기 때문이다.</p><h3 id="zero-convolution">Zero convolution</h3><p>이때 더해질 때 바로바로 이 논문에서 가장 중요한 녀석인 zero convolution이라는 개념이 사용되는데, 각 neural block의 앞/뒤로 하나씩 붙는다고 생각하면 된다. 앞/뒤에 붙는 녀석들을 각각 $\mathcal{Z}_{\Theta_1}(\cdot), \mathcal{Z}_{\Theta_2}(\cdot)$라고 해보자. 물론 zero-convolution은 feature map의 크기를 변화시키면 안되기 때문에 $1\times 1$ 크기를 가지는 convolution이며 weight와 bias 모두 zero로 초기화된 상태로 학습이 시작된다.</p><p>위의 그림대로 원래의 output $y$에 conditioning 함수를 거친 output을 더하면 다음과 같다.</p><p>[ y_c = \mathcal{F}_\Theta(x) + \mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x + \mathcal{Z}_{\Theta_1}(c))) ]</p><p>여기에서 대체 왜 weight 및 bias가 $0$으로 초기화된 ‘Zero convolution’이 사용되었는지 이유가 등장한다. Zero-convolution은 weight 및 bias가 모두 $0$이므로, input에 상관없이 처음엔 모두 $0$을 output으로 내뱉는다.</p><p>[ \begin{cases} \mathcal{Z}_{\Theta_1}(c) = 0 \newline \mathcal{F}_{\Theta_c}(x+\mathcal{Z}_{\Theta_1}(c)) = \mathcal{F}_{\Theta_c}(x) = \mathcal{F}_{\Theta}(x) \newline \mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x + \mathcal{Z}_{\Theta_1}(c))) = \mathcal{Z}_{\Theta_2}(\mathcal{F}_{\Theta_c}(x)) = 0 \end{cases} ]</p><p>즉 처음에는 $y_c = y$로 시작하게 된다. 해당 내용이 암시하는 것은 training이 시작되는 당시에는 ControlNet 구조에 의한 input/output 관계가 사전 학습된 diffusion의 input/output과 전혀 차이가 없다는 것이고, 이로 인해 optimization이 진행되기 전까지는 neural network 깊이가 증가함에 따라 영향을 끼치지 않는다는 것을 알 수 있다.</p><h3 id="gradient-flow-in-zero-convolution">Gradient flow in zero convolution</h3><p>$1 \times 1$ convolution 구조를 가지는 zero convolution에 대한 연산 과정에 local gradient를 유도할 수 있다. 예컨데 input feature map $I \in \mathbb{R}^{h \times w \times c}$가 있을때 forward pass는</p><p>[ \mathcal{Z}(I,; \{W, B\})_{p, i} = B_i + \sum_{j}^c I_{p, i}W_{i, j} ]</p><p>이처럼 표현되고, zero convolution은 최적화 전까지는 $W = 0, B = 0$이기 때문에 $I_{p, i}$가 $0$이 아닌 모든 point에 대해서</p><p>[ \begin{cases} \frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial B_i} = 1\newline \frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial I_{p, i}} = \sum_{j}^cW_{i,j} = 0 \newline \frac{\partial \mathcal{Z}(I; \{W, B\})_{p, i}}{\partial W_{i, j}} = I_{p, i} \neq 0 \end{cases} ]</p><p>위와 같이 정리된다. Input에 대한 gradient는 $0$으로 만들지만 weight나 bias에 대한 gradient는 $0$이 아니기 때문에 학습이 가능하다. 왜냐하면 first step만 지나게 되면 Hadamard product 기호인 $\odot$에 대해</p><p>[ W^\ast = W-\beta_\text{lr} \cdot \frac{\partial \mathcal{L}}{\partial \mathcal{Z}(I; \{W, B\})} \odot \frac{\partial \mathcal{Z}(I; {W, B})}{\partial W} \neq 0 ]</p><p>$0$이 아닌 weight를 만들기 때문에 바로 다음 step에서는</p><p>[ \frac{\partial \mathcal{Z}(I; \{W^\ast, B\})_{p, i}}{\partial I_{p, i}} = \sum_{j}^cW^\ast_{i,j} \neq 0 ]</p><p>학습이 잘된다.</p><hr /><h1 id="stable-diffusion--controlnet">Stable diffusion + ControlNet</h1><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462212-cafb40f1-f222-4560-9491-52d370dd512f.png" width="" /></p><p>위에서 설명한 구조를 기존 stable diffusion에 구현한 구조는 위와 같다. Loss는 기존 diffusion algorithm에 task specific condition $c_f$만 추가된 형태가 된다.</p><p>[ \mathcal{L} = \mathbb{E}_{z_0, t, c_t, c_f, \epsilon \sim \mathcal{N}(0, 1)}\left( \parallel \epsilon - \epsilon_\theta(z_t, t, c_t, c_f) \parallel_2^2 \right) ]</p><hr /><h1 id="결과">결과</h1><h3 id="canny-edge">Canny Edge</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462216-b7291a87-32f6-4ac2-983f-b7193936fd35.png" width="" /></p><h3 id="hough-line">Hough Line</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462217-4d045efa-eddb-4450-9ec2-92a3ad0cb070.png" width="" /></p><h3 id="scribble">Scribble</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462218-4986d394-2c30-4e23-b3fd-18f65c34bcb9.png" width="" /></p><h3 id="hed-edge">HED edge</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462222-f9f9d56d-ee9f-4e1c-af07-9fb29cd10880.png" width="" /></p><h3 id="pose">Pose</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462224-f1f5b4d8-579f-473e-b033-aad1b955a387.png" width="" /></p><h3 id="segmentation">Segmentation</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462226-5209cf47-c377-400e-add5-07738a63644b.png" width="" /></p><h3 id="depth">Depth</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462230-9f60b1a4-d1d7-41c3-a5d8-a3b74f07af57.png" width="" /></p><h3 id="cartoon-line-drawing">Cartoon line drawing</h3><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462232-8dbc6b0e-ad70-41f6-aa4b-fb2d91c40f60.png" width="" /></p><hr /><h1 id="official-code로-직접-실행해보기">Official Code로 직접 실행해보기</h1><p>현재 official code는 <a href="https://github.com/lllyasviel/ControlNet.git">깃허브 소스</a>로 제공되고 있다. 엥간하면 로컬 서버에서 돌아가기는 하는데 안정적으로 돌릴라면 서버에서 돌리는게 좋다. 여기다가 실행법은 올리겠지만 원본 페이지에 들어가서 ⭐ 한번씩 눌러주면 좋을 것 같다. 다음 repository를 클론 후</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/lllyasviel/ControlNet.git
</code></pre></div></div><p>Conda 가상 환경을 설치해준다.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>ControlNet
conda <span class="nb">env </span>create <span class="nt">-f</span> environment.yaml
conda activate control
</code></pre></div></div><p>그런 뒤 사용하고자 하는 모델과 stable diffusion을 <a href="https://huggingface.co/lllyasviel/ControlNet">Hugging Face Page</a>로부터 다운받으면 된다. 다운받는 위치는 ControlNet/models에 stable diffusion ckpt를 넣고 detector를 ControlNet/annotator/ckpts에 넣으면 된다.</p><h3 id="detector모두-다운받는-코드controlnet-레포지에서-실행">Detector(모두) 다운받는 코드(ControlNet 레포지에서 실행)</h3><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ./annotator/ckpts
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth
</code></pre></div></div><p>굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.</p><h3 id="models모두-다운받는-코드controlnet-레포지에서-실행">Models(모두) 다운받는 코드(ControlNet 레포지에서 실행)</h3><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ./models
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth
curl <span class="nt">-LO</span> https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth
</code></pre></div></div><p>마찬가지로 굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.</p><h3 id="데모-버전-api-실행하기">데모 버전 API 실행하기</h3><p>원하는 모델을 실행하는 코드는 간단하게</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python gradio_어쩌구2어쩌구.py
</code></pre></div></div><p>를 실행하면 되는데, 만약 서버컴에서 이걸 실행하고 로컬에서 접속하고 싶다면 코드를 살짝만 바꿔주면 된다. 예컨데 모든 <code class="language-plaintext highlighter-rouge">gradio_어쩌구2저쩌구.py</code> 파일 코드를 보게 되면 가장 마지막 줄에</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block</span><span class="p">.</span><span class="n">launch</span><span class="p">(</span><span class="n">server_name</span><span class="o">=</span><span class="s">'0.0.0.0'</span><span class="p">)</span>
</code></pre></div></div><p>요 친구가 있는데 이걸 다음과 같이 바꿔주면 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block</span><span class="p">.</span><span class="n">launch</span><span class="p">(</span><span class="n">server_name</span><span class="o">=</span><span class="s">'0.0.0.0'</span><span class="p">,</span> <span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div><p>본인은 대충</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">gradio_scribble2image_interactive</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div><p>이걸 실행해보겠다. 제대로 실행되면 다음처럼 나온다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462235-7dd62696-6f57-4ae8-a0ee-640a2b1cf4e8.png" width="" /></p><p>대강 public URL은 72시간 동안 유효하다는 뜻, 본인은 연세 vpn으로 서버컴에 접속한 상태지만 노트북으로 들어가보겠다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462236-0d8e2563-80b2-45a8-9b61-46aa33e51c64.png" width="900" /></p><p>다음과 같은 화면이 뜬다. 실제로 잘 되는지 확인해보자.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462237-2678713e-f6ad-441a-aed9-0debfa373a47.png" width="700" /></p><p>비루한 그림실력.. 힘내라 ControlNet</p><p>Run 버튼을 누르자 DDIM sampler가 동작하기 시작한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462239-0268dd2a-2e93-45fa-aff1-f9261b635f62.png" width="" /></p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/235462241-f63bdb2f-ddd6-437f-9aa0-e27094bbe81c.png" width="" /></p><p>그림을 못그려도 인생 살기 큰 문제 없다는 긍정적인 희망이 생기는 논문이었다… 암튼 이렇게 하면 된다. 넉넉잡아 10기가 이상의 GPU면 다 돌아가는 듯하다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/diffusionpapers"><p id="prev_title"> ❮❮ Improved DDPM + Diffusion beats GAN + Classifier free diffusion guidance 논문 리뷰</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/glide"><p id="next_title"> ❯❯ GLIDE(Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models) 논문 및 코드 리뷰</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/deep_learning2">PAPERS & TECHS</a></li><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github_blog">GITHUB BLOG</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
