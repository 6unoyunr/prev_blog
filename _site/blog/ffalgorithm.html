<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)"><meta name="og:description" content="Forward-forward algorithm"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/221447149-7ed12c39-a466-4f34-82d8-cecd13e304ef.gif"><meta name="og:image:alt" content="Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)"><meta name="og:url" content="http://localhost:4000/blog/ffalgorithm"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)"><meta name="twitter:description" content="Forward-forward algorithm"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/221447149-7ed12c39-a466-4f34-82d8-cecd13e304ef.gif"> <!-- Search Engine --><meta name="description" content="Forward-forward algorithm"><meta name="image" content="https://user-images.githubusercontent.com/79881119/221447149-7ed12c39-a466-4f34-82d8-cecd13e304ef.gif"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)"><meta name="author" content="JY"/><meta itemprop="description" content="Forward-forward algorithm"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/221447149-7ed12c39-a466-4f34-82d8-cecd13e304ef.gif"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a> <!--<hr class="navbar-divider"> <a class="navbar-item"> Report an issue </a> --></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/ffalgorithm" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/221447149-7ed12c39-a466-4f34-82d8-cecd13e304ef.gif" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Forward-forward algorithm</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>February 22, 2023</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Deep learning</span> <span class="tag is-link">FF algorithm</span> <span class="tag is-link">Methodology</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 29 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="들어가며">들어가며…</h1><p>제목이 너무 어그로성이 짙었는데, 논문에서는 backpropagation을 <u>완전히 대체하는 알고리즘을 소개한 것은 아니고</u> 딥러닝의 새로운 연구 방향을 잡아준 것과 같다.</p><p>이 논문에서는 neural network를 학습하는 <u>기존 방법들</u>로부터 벗어나 새로운 학습법을 소개한다. 새롭게 제시된 방법인 <u>FF(forward-forward)</u> 알고리즘은 뒤에서 보다 디테일하게 언급되겠지만 Supervised learning과 unsupervised learning의 몇몇 간단한 task에 잘 적용되는 것을 볼 수 있고, 저자는 이를 통해 FF 알고리즘이 기존의 foward/backward 알고리즘과 더불어 더 많은 연구가 진행될 수 있을 것이라고 전망한다. 아마 딥러닝을 하던 사람들은 가장 기초부터 배울 때 backpropagation이라는 개념을 필수로 배울 수 밖에 없으며, 본인이 블로그에 작성한 글 중 신경망 학습을 위해 제시된 backpropagation이라는 개념을 perceptron의 역사와 함께 소개하는 내용이 있었다(<a href="https://junia3.github.io/blog/cs231n04">참고 링크</a>). <br /> 기존 backpropagation 방법은 forward pass를 통해 <u>오차를 계산한 뒤</u>(supervision이 있다고 가정하면) backward pass 시 chain rule을 통해 각 parameter를 learning rate에 따라 업데이트했다면, forward forwad algorithm(FF)은 한 번의 <u>positive pass(real data에 대한 stage)</u>와 한 번의 <u>negative pass(네트워크 자체에서 생성되는 data에 대한 stage)</u>로 구성된다.</p><hr /><h1 id="논문에서-제시한-backpropagation의-근본적인-문제점">논문에서 제시한 backpropagation의 근본적인 문제점</h1><p>사실상 딥러닝은 큰 갯수의 parameter를 가진 model을 stochastic gradient 방법을 통해 대량의 데이터셋에 fitting하는 과정이었다. 그리고 gradient는 <u>backpropagation</u>을 통해 연산하게 되었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220493453-18b6ebe8-cc5e-4316-932b-3ae8006f52db.png" width="400" /></p><p>인간의 뇌가 작동하는 구조를 모방한 것이 신경망이었다. 시냅스가 서로 연결되어있으며 이전 신호의 magnitude에 따라 activation이 일어나는 방식으로 구성한 <u>다층 신경망 구조</u>는 실생활의 이런 저런 문제들을 해결할 수 있는 힘이 있었다. 이러한 발전이 있기 위해서는(다층 신경망을 학습시키기 위해서는) <u>backpropagation 알고리즘</u>이 필수적이었으며, 여기서 발생하는 의문점은 다음과 같았다.</p><ul><li>인간의 뇌가 실제로 학습할 때 backpropagation 방법을 사용하는가?</li><li>만약 인간의 뇌에서도 학습할 때 backpropagation과 같은 방법을 사용하지 않는다면, 시냅스 간의 연결 사이에 가중치를 조절하기 위한 메커니즘이 따로 존재하는가?</li></ul><h2 id="피아제의-인지발달이론">피아제의 인지발달이론</h2><p>잠시 논문을 소개하기 전에 심리학 이론에 대한 설명을 하고 넘어가도록 하겠다. 직접적으로 이 논문과 관련이 있을지는 모르겠지만, 논문을 읽으며 근본적으로 backpropagation에 의문을 가진 과정 자체가 인간이 어떤 <u>정보를 학습하는 메커니즘과의 차이</u> 때문이라고 생각했다. 실제로 우리 뇌의 cortex(피질)를 생각해보면 backpropagation은 뉴런으로 구현할 수 있음이 증명되었지만 우리가 실생활에서 학습하는 방식과는 차이가 있다. 예를 들어 <u>어떤 아이</u>가 난생 처음으로 강아지를 본다고 생각해보자(<a href="https://www.simplypsychology.org/what-is-accommodation-and-assimilation.html">참고 링크</a>). Piaget(피아제)의 <u>인지발달이론</u>을 인용하자면, 인간에게 있어서 accommodation과 assimilation 과정이 반복되면서 인지 적응이 진행된다고 설명한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220494883-88e9a0a7-8bbf-4baf-8784-cc07bac1da9f.png" width="400" /></p><p>아이의 보호자는 아이에게 강아지의 생김새에 대한 묘사를 해주거나, 그림책을 기반으로 ‘강아지’라는 존재에 대한 특징을 입력받는다. 아이는 기존에 강아지에 대한 어떠한 지식도 없었기 때문에 ‘강아지’라는 존재는 <u>본인의 인식 체계 속</u>에서 dissimilar한 존재다(낯선 존재). 따라서 강아지에 대한 <u>정보가 주어진 순간</u>에는 해당 지식에 대한 <u>불안정한 체계</u>가 잡히게 되고, 아이는 강아지에 대한 새로운 특징이나 정보를 입력할 때마다 ‘강아지’에 대한 인식 체계를 확립하며 이를 안정화하는 단계에 이른다. 이런 상황에서 길을 걷다가 실제로 <u>강아지</u>를 만났을 경우를 생각해보자. 아이는 본인이 안정화시킨(확립한) 강아지에 대한 특징을 토대로 목격한 대상을 강아지라고 판단하게 된다. 그런데 갑자기 강아지가 <u>예상치 못한 행동</u>을 하는 경우를 생각해보자. 강아지가 <u>'짖고', '물고', '핥고'</u>하는 특징들은 아이가 기존에 경험해보지 못했기 때문에 본인이 확립한 ‘강아지’라는 <u>인식 체계에 disequillibrium을 주는 특징</u>들이다. 이러한 혼란스러운 상황에서 아이는 부모나 정보를 제공해줄 수 있는 사람을 통해 ‘강아지가 맞다’라는 확답을 듣게 되면, disequillibrium 상태에 있었던 <u>지식 체계가 강화</u>된다(reinforcement). 이를 동화(assimilatioon) 과정이라고 부른다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220498798-01bddac3-658f-49cc-99c0-453f3da78280.png" width="400" /></p><p>Accomodation은 조금 다르게, 처음 보는 존재를 분류할 때 본인이 인식하고 있는 특징과 <u>다른 점들을 통해 새로운 정보 체계를 확립</u>하는 과정이다. 예를 들어 길을 가다가 고양이를 본 경우를 생각해보자. 고양이는 강아지와 다르게 ‘야옹’하는 소리를 내고, ‘나무에 올라가거나’ 등등 여러 다른 특징들을 보여준다. 기존에 강아지가 본인이 알고 있는 특징들과 다른 모습들을 보여준 경우에도 아이는 같은 <u>disequillibrium 과정</u>을 거쳤고, 이런 상황에서 정답을 제공해줄 수 있는 사람을 통해 <u>정보 체계를 강화</u>했던 것과 비슷한 방식으로 아이는 정보를 제공해줄 수 있는 사람에게 강아지가 맞는지 묻게 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220499468-238695a2-b377-40e4-8998-5a2eb773d618.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/220499618-eb68d8de-a747-4e63-ba1a-d8d83a788940.png" width="400" /></p><p>그러나 이번엔 강아지가 아니라 본인이 기존에 알지 못했던 정보인 ‘고양이’라는 대답을 듣게 되고, 이를 통해 본인이 <u>기존에 알고 있는 강아지에 대한 특징</u>에 새로운 존재인 <u>고양이의 특징</u>을 접목시켜 새로운 정보에 대해 적응하는 과정을 겪는다. 이러한 과정을 <u>accomodation</u>(적응)이라고 부른다.</p><h2 id="그래서-인간의-학습-과정은">그래서 인간의 학습 과정은?</h2><p>피아제의 인지 발달 이론에 대해 굳이 짚고 넘어 온 이유는 인간은 본인이 <u>알지 못했던 사실</u>이나 <u>새로운 사실</u>을 받아들이는 과정에서 본인이 가진 인식 체계(일종의 뉴런 weight)가 도출한 잘못된 결과에 대해 오차를 계산한 뒤 이를 <u>다시 적용시키는 과정</u>이 <u>explicit하게 존재하지 않는다</u>는 사실이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220500694-89a1e6d3-ad86-4420-9744-62ff8c24eac4.png" width="400" /></p><p>시각 정보를 처리하는 visual cortex가 연결된 구조는 top-down 형태로, 시각 정보를 받아들이는 <u>가장 바깥쪽 부분</u>부터 차례로 정보를 처리하게끔 되어있다. 만약 backpropagation이 진행되는 구조는 이와는 반대로 <u>가장 안쪽 cortex부터 망막까지 이어지는 시신경 세포들까지</u>의 bottom-up mirror 구조를 가져야하는데, 실제로는 그렇게 되지 않는다는 것이다. 오히려 우리가 보는 시각 정보는 연속적인 프레임을 가진 일종의 동영상이며, <u>잘못된 판단에 대한 ground truth가 주어졌을 경우</u>(강아지라고 했는데 사실은 고양이였을 경우) 이전에 관찰한 시각 정보에 대한 <u>nerve signal</u> 오차를 계산해서 역방향으로 정보를 학습하는 것이 아니라, 우리가 <u>지금 보고 있는 이미지에 대해</u> 정보 체계를 수정하게 된다. 즉 backpropagation 구조라기 보다는 시각 정보를 통해 신경 activity가 발생하는 내부에서 <u>하나의 루프를 생성</u>하고, 이 과정으로 <u>정보 체계를 바꿔가는 것</u>으로 볼 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220513070-eb6e8bb0-f122-4fe9-9da6-8dac3584a4d9.png" width="800" /></p><p>그리고 만약 <u>우리가 학습하는 정보에 대해서도</u> backpropagation이 진행된다면 형광등이 빠른 속도로 점멸하는 것처럼 우리의 <u>인식 체계에도 주기적인 time-out</u>이 필요하다. 딥러닝에서 하는 일종의 <u>online-learning</u>과 비슷한데, 우리가 일상생활을 유지하면서 그와 동시에 backpropagation이 가능하기 위해서는 뇌의 각 단계에서의 sensory processing 결과를 저장할 pipeline이 필요하고, 이를 오차에 맞춰 수정한 뒤 원래의 인식 체계에 적용할 수 있어야 한다. 하지만 <u>pipeline의 뒤쪽에 있는 정보</u>가 backpropagation을 통해 <u>earlier stage</u>(보다 input에 가까운 위치)에 영향을 끼치지 위해서는 <u>실시간으로 인식을 진행하는</u> 우리의 학습 과정과는 차이가 있어야 한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220514216-e547f003-4f09-4cf9-867a-3857f6f95a41.png" width="800" /></p><p>Backpropagation은 또한 forward pass 과정에서의 <u>모든 연산 결과</u>를 알아야한다는 것이다. Chain-rule에 의해 각 노드에서의 <u>local gradient를 계산</u>하기 위해서는 노드에서의 input을 알아야하고, 이는 곧 이전 노드들의 output을 모두 알아야 가능하기 때문이다. 그렇기에 <u>forward pass 과정이 black box</u>라 가정하면(어떤 연산이 진행되는지 전혀 모른다고 생각하면), 미분 가능한 모델이 확립된 상황이 아니라면 <u>backpropagation이 진행될 수 없는 것</u>을 알 수 있다. 이를 바꿔 설명하자면 만약 인간의 인식 체계가 backpropagation을 적용하기 위해서는 시신경을 포함하여 판단을 내리는 모든 구조에 대해 <u>differentiable closed form</u>으로 알고 있다는 전제가 필요하다. 이러한 문제들을 forward-forward algorithm에서는 고려할 필요가 없다.</p><p>또다른 방법으로는 강화학습을 생각해볼 수 있다. Forward process에 대한 정보가 부재할 경우에는 단순히 neural activity에 대한 weight의 일부에 random한 변화를 가해주고, 변화에 따라 바뀌는 <u>결과값에 대한 보상</u>을 해주면 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220514711-1707850b-19dc-45ac-b9dc-55275351196a.png" width="400" /></p><p>하지만 특정 parameter의 변화가 <u>다른 parameter의 변화에 종속</u>할 수 있는 기존의 backpropagation과는 달리, 강화학습의 경우에 <u>variance(경우의 수)가 너무 크기 때문</u>에 각 parameter의 변화가 output에 미치는 영향을 제대로 확인할 수가 없다. 이러한 문제를 학습 과정에서 생기는 noise라 하는데, 이를 완화하기 위해서는 변화가 가해지는 parameter의 개수에 반비례하게 learning rate를 구성하는 방법이 있다. 결국 <u>parameter의 개수가 증가할수록</u> 학습 속도는 이에 반비례해서 <u>계속 감소</u>하게 되며, 대용량의 네트워크를 학습시킬 수 있는 backpropagation 알고리즘에 비해 <u>학습 속도 측면</u>에서 불리하게 작용한다.</p><p>이 논문에서는 <u>ReLU나 softmax</u>와 같이 closed form으로 구할 수 있는 non-linearity를 포함하지 않는 네트워크도 학습할 수 있는 forward-forward algorithm(FF)을 제안한다. FF의 가장 큰 장점은 backpropagation 방법에서는 forward pass에 대한 <u>레이어 연산이 불명확한 경우</u>에는 <u>학습이 불가능</u>하다는 점을 해결할 수 있다는 사실이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220517775-fe18eb62-c188-4ba5-99cb-6478931496e6.png" width="1000" /></p><p>또한 <u>연속된 데이터</u>가 주어졌을 때 다음과 같이 neural network의 output에 대한 <u>error를 통해 parameter를 업데이트</u>하는 과정에서 pipelining을 멈출 필요가 없다는 점도 장점이 될 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220519115-745feb79-b4f3-4631-82ac-fbcdc1d680a7.png" width="400" /></p><p>하지만 논문에서 밝히는 것은 backpropagation보다는 forward-forward algorithm이 <u>속도가 더 느리고</u> 실험한 몇몇의 toy problem 이외에는 아직 일반화가 힘들다는 문제가 있기 때문에 FF 알고리즘이 backpropagation을 완전히 대체하기는 힘들다고 밝힌다. 그렇기 때문에 여전히 대용량의 데이터셋을 기반으로 하는 딥러닝은 <u>backpropagation을 계속 사용할 것</u>이라고 한다.</p><hr /><h1 id="forward-forward-algorithm이란">Forward forward algorithm이란?</h1><p>FF는 <u>볼츠만 머신</u>이나 <u>noise contrastive estimation(NCE)</u>에서 말하는 greedy multi-layer learning procedure라고 볼 수 있다. Greedy 알고리즘이라 불리는 이유는 다음과 같다. 어떠한 <u>문제(task)</u>를 해결하려면 그에 맞는 <u>해결책(solution)</u>이 필요하고 이를 우리는 <u>알고리즘</u>이라고 부른다. 실생활에서 우리가 접하는 것과 같이 <u>복잡한 문제를 컴퓨팅 환경에서 해결</u>하는 상황에서 단번에 최적의 해를 구할 수 없는 것이 일반적이다. 따라서 복잡한 문제들을 여러 sub-task로 분류하여 해결해가는 형태의 <u>dynamic programming</u>을 활용하기도 하지만, 복잡한 문제를 타개할 마땅한 sub-task 조차도 정의하기 어려운 상황이 있다면 그럴 땐 <u>직면한 상황을 해결하면서</u> 최종 task의 solution에 근접해가는데, 이를 <u>greedy algorithm</u>이라 부른다. 볼츠만 머신이나 NCE 그리고 신경망 구조도 결국 layer-wise greedy algorithm이라고 할 수 있다. 예컨데 학습 과정에서 <u>서로 다른 위치</u>의 레이어는 <u>input 정보</u>가 다르기 때문에(distribution) 이들을 통해 얻을 수 있는 representation(feature) 또한 달라지게 되고, 결국 각 layer는 <u>각자가 직면한 상황</u>에서 <u>독립적인 solution을 구하는</u> task로 귀결된다. Greedy MLP algorithm은 각 레이어가 학습되는 단계에서는 각자가 내릴 수 있는 최선의 해결책을 내놓아야 한다는 점이고, 이는 모든 레이어가 <u>독립적으로 학습된다는 assumption</u>이 포함된다.</p><p>FF algorithm이 제시하는 방법은 다음과 같다. <u>서로 반대되는 objective</u>를 가지는 두 data가 <u>각각 forward pass</u>되면서 backpropagation을 대체한다는 것이다. 이러한 두 forward pass를 각각 ‘positive pass’ 그리고 ‘negative pass’라고 한다. Positive pass는 <u>real data</u>에 적용되고, 각 <u>hidden layer의 weight</u>로 하여금 <u>'goodness'</u>를 증가시키게끔 작동한다. 그와는 반대로 negative pass는 <u>negative data</u>에 적용되며 각 hidden layer에 대해 <u>'goodness'</u>를 감소시키는 방향으로 작용한다. 각각의 pass에 대한 goodness는 서로 같은 식이지만 부호가 반대라고 생각하면 되는데, positive pass의 경우에는 goodness가 neural activities의 squared sum이 되고, negative pass의 경우에는 goodness가 neural activities의 negative squared sum이 된다. 이 논문에서 제시한 goodness란 greedy 알고리즘에서 ‘최적의 선택’을 의미하며, 굳이 이 논문에서 주장한 <u>squared sum</u>이 아니라 다른 형태가 될 수 있다고 말한다. 본인이 생각하기에는 이 부분이 아마도 앞으로 FF algorithm을 활용한 다양한 딥러닝 연구의 기준이 되지 않을까 생각해본다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220526904-ba8ddae0-6cc4-466a-bda7-535ccd91b2d8.png" width="700" /></p><p>위의 그림을 토대로 보면, 각 레이어는 each pass에 따라 real data(positive input)에 대해서 negative input을 기준으로 특정 threshold 만큼 높이는 것이 목적이 된다. 기존의 backpropagation은 <u>output node에서만</u> objective function을 가졌다면, FF algorithm에서는 <u>각 레이어마다</u> objective function을 가진다고 생각할 수 있다. 각 레이어에서의 objective function은 <u>positive input</u>과 <u>negative input</u>을 이진 분류하는 classifier와 같기 때문에 특정 레이어의 노드 index $j$에 대해 positive sample일 확률을 logistic $\sigma$를 통해 다음과 같이 표현할 수 있다.</p><p>[ p(\text{positive}) = \sigma \left( \sum_j y_j^2 - \theta \right) <br /> ]</p><p>식에서의 $y_j$는 <u>layer normalization 이전의 activity</u>에 해당되고, $\theta$는 threshold다. 식에서 확인할 수 있는 것은 저자가 목적함수로 삼은 goodness가 <u>데이터의 이진 분류</u>를 위한 logistic의 input으로 사용되기 때문에 likelihood(squared sum)를 최대화하는 방향으로 <u>positive pass</u>의 goodness를, negative likelihood(negative squared sum)를 최대화하는 방향으로 <u>negative pass</u>의 goodness를 설정했다고 해석할 수 있다.</p><hr /><h1 id="learning-mlp-with-greedy-algorithm">Learning MLP with greedy algorithm</h1><p>위의 방법을 그대로 생각해보면 <u>단일 hidden layer</u>에 대해서는 각 데이터에 대한 goodness를 구하는 방식이 명확하고, 학습할 때의 objective 또한 마찬가지인 것을 볼 수 있다. 하지만 만약 <u>첫번째 layer의 output</u>이 그대로 <u>두번째 layer의 input으로</u> 사용된다면 이미 output의 squared sum을 통해 positive/negative 구분이 가능하게끔 학습되었기 때문에 첫번째 layer의 output vector의 크기를 비교하는 것만으로도 두번째 layer에서는 goodness를 판별할 수 있게 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220530960-9729f287-48b7-46a3-a55f-a74838df93b7.png" width="700" /></p><p>쉽게 말하자면 <u>볼츠만 머신과 같은 greedy 알고리즘</u>에서 추구하고자 하는 것은 각 레이어마다 <u>input이 다르기 때문에</u> 그에 맞게 <u>서로 다른 representation</u>을 학습하게 되는 것인데, 이미 이전 레이어에서 학습한 결과만 있다면 goodness 판별이 어렵지 않기 때문에 이후 레이어는 새로운 feature(representation)을 <u>학습할 필요가 없게 된다</u>(일종의 identity mapping이라고 생각하면 될 것 같다).</p><p>이러한 <u>feature collapse</u> 문제를 막기 위해서 FF는 hidden layer output으로 나오는 feature vector의 길이를 다음 layer의 input으로 넣기 전에 normalize하게 된다. 그렇게 되면 길이에 대한 정보를 통해 <u>상대적인 길이만 유지</u>한 채로 input으로 들어가게 된다. 다르게 표현하자면 activity vector는 크기와 방향을 가지는데, <u>크기 정보를 필터링</u>하고 <u>방향에 대한 정보</u>만 다음 layer로 보낸다고 생각하면 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/222123454-0e336269-f646-450b-8695-c6e9fbc5ab5e.png" width="600" /></p><p>[ \begin{aligned} \hat{a^l} =&amp; \frac{a^l}{\vert \vert a^l \vert \vert_2} \newline a^l =&amp; (a_1^l,~a_2^l,~a_3^l,~a_4^l,~a_5^l) \end{aligned} ]</p><p>FF algorithm에서의 layer normalization은 activity에 대해 <u>layer mean을 빼주는 과정 없이</u> activity vector의 <u>길이로 나눠주는 작업</u>을 진행했다고 한다.</p><hr /><h1 id="ff-experiments">FF experiments</h1><p>앞서 말했던 바와 같이 이 논문의 주된 목적은 <u>FF algorithm의 feasibility를 확인</u>하는 것이기 때문에 상대적으로 적은 parameter 수를 가지는 작은 neural network에 대해 적용한 실험이 대부분이고, 저자는 <u>이후의 연구들을 통해</u> FF를 <u>large neural network에 적용</u>하는 것은 future work로 남겼다.</p><h2 id="backpropagation-baseline-for-mnist">Backpropagation baseline for MNIST</h2><p>이 논문에서 대부분의 실험은 <u>MNIST dataset</u>(숫자 손글씨)을 기반으로 한다. MNIST의 원래 구성은 $60,000$개의 training images와 $10,000$개의 test images 인데, 이 중에서 $50,000$의 training image와 $10,000$개의 validation image로 분리하여 최적의 hyper-parameter를 찾는 과정을 거친다. MNIST는 딥러닝에서 원래 사용되던 backpropagation 알고리즘을 적용한 연구에서 많이 활용되던 데이터셋이기 때문에, <u>저자가 제시한 새로운 알고리즘의 feasibility를 확인</u>하기 좋은 구조가 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220541733-4f57acde-0604-4d97-9d23-bea840547492.png" width="600" /></p><p>CNN(Convolutional neural network)와 같은 구조는 MNIST에 대해 대략 $0.6\%$의 오차를 보인다. 보통 CNN과 같은 구조는 permutation-invariant하지 않다고 하는데, permutation invariant란 <u>순열의 변화</u>가 <u>output에 영향을 미치지 않는 경우</u>를 의미한다. 즉 ReLU를 activation function으로 사용하는 MLP 구조는 permutation invariant 구조인데, 이 경우 대략 $1.4\%$의 test error를 보이고 dropout이나 label smoothing 같은 regularizer를 사용할 경우 $1.1\%$까지 성능이 올라간다. 이에 추가로 이미지의 확률 분포를 모델링하는 unsupervised learning 방법을 추가하면 더 성능이 올라가지만, 요약하자면 정규화를 고려하지 않는다면 CNN은 $0.6\%$, MLP는 $1.4\%$의 test error를 보인다.</p><h2 id="unsupervised-learning-in-ff">Unsupervised learning in FF</h2><p>FF algorithm을 길게 설명했는데, 여기서 두 가지의 의문이 나오게 된다. 첫번째는 <u>negative data를 학습하는 과정</u>이 dataset의 <u>multi-layer representation</u> 학습에 어떤 방식으로 효과를 주는가이고, 두번째는 <u>negative data를 어떻게 만들어내는가</u>이다. 저자는 첫번째 질문에 대한 대답을 하기 위해 <u>hand-crafted negative data</u>를 만들어내게 되었다.</p><p><u>Contrastive learning</u>을 supervised learning에서 활용할 때 주로 적용하는 방식은 input vector를 label에 대한 정보 없이 <u>representation vector로 바꾸는 작업</u>을 진행한 뒤에, 이를 label에 대한 softmax probability를 구하는 <u>linear transformation</u>으로 학습하는 구조를 활용한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220544597-6a6533da-3184-4d29-86bd-654c87e2d0a5.png" width="600" /></p><p>Linear transformation을 학습하는 과정은 supervision을 가지지만, <u>hidden layer 없이</u> 학습되기 때문에 <u>backpropagation</u>이 따로 필요하지 않다. FF는 바로 이러한 관점에서 <u>positive example</u>과 <u>corrupted example</u>을 활용한 representation learning을 한다고 볼 수 있다. Dataset을 corrupt하는 방법은 data augmentation과 마찬가지로 여러 가지가 있을 수 있다.</p><p>이러한 여러 augmentation 중에서 FF 알고리즘의 효과적인 학습을 위해서는 다음과 같은 조건을 충족해야한다고 한다. FF가 image에서의 object shape와 같은 long-range correlation(이미지 전반을 보게끔)을 가질 수 있게 하는 방법은 <u>negative dataset</u>이 real dataset과 <u>long range correlation</u>은 <strong>다르게</strong>, <u>short range correlation</u>은 <strong>유사하게</strong> 구성하는 것이다. 이렇게 데이터셋을 구성하게 되면 네트워크의 각 레이어는 short range correlation로는 fake/real을 <u>구분할 수 없기 때문에</u> longer range correlation에 집중하는 경향성이 생긴다. 가장 간단한 방법은 ones/zeros로 구성된 마스크를 넓은 영역으로 구성하는 것이다. 그런 뒤 서로 다른 real image를 mask와 reversed mask를 적용하여 합한 데이터셋을 구성하게 되면, <u>적은 영역에 대해서는 real dataset과 구분할 수 없는</u> negative sample을 구성할 수 있게 된다.</p><p>넓은 영역의 mask를 만드는 방법은 다음과 같은데, 먼저 random한 bit image를 기준으로 가로/세로 모두에 $(1/4,~1/2,~1/4)$의 값으로 블러링하는 과정을 계속 반복한다. 실제로 negative sample을 만드는 과정이 궁금해서 논문에서 제시한 방법을 코드로 옮겨보았다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220559410-cc6e67cf-7e3e-41b9-b6fe-d12eb2da9dda.png" width="600" /></p><h2 id="hand-crafted-negative-sample">Hand-crafted negative sample</h2><h4 id="mnist-dataset-sample-가져오기">MNIST dataset sample 가져오기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">sample1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="n">sample2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
<span class="n">sample1_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample1</span><span class="p">)</span>
<span class="n">sample2_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample2</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220557403-e28b43bf-8617-4af5-b747-d7944a567025.png" width="400" /></p><h4 id="28-times-28-크기의-random-bit-image-생성하기">$28 \times 28$ 크기의 random bit image 생성하기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">random_bit_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220557637-bf65cdf3-6e43-4139-9aae-98f691a215ae.png" width="200" /></p><h4 id="3-times-3-크기의-blur-kernel-생성하기">$3 \times 3$ 크기의 blur kernel 생성하기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">blur_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220558021-d792c287-77ad-4419-ad88-d26e659ac99e.png" width="200" /></p><h4 id="numpy-배열에-대한-2d-convolution-함수">Numpy 배열에 대한 2D convolution 함수</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define convolution operation for 2D numpy matrix
</span><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
    <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">))</span>

    <span class="c1"># zero padding
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># calculate 2d convolution
</span>    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_height</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_width</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">output</span><span class="p">[</span><span class="n">h</span><span class="p">][</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span>
                        <span class="n">image</span><span class="p">[</span><span class="n">h</span> <span class="p">:</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">3</span><span class="p">,</span> <span class="n">w</span> <span class="p">:</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel</span>
                    <span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div><h4 id="iteration-돌리면서-blur-kernel-적용하기">Iteration 돌리면서 blur kernel 적용하기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iteration</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">blurred</span> <span class="o">=</span> <span class="n">random_bit_image</span><span class="p">[:,</span> <span class="p">:]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
  <span class="n">blurred</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">blurred</span><span class="p">,</span> <span class="n">blur_kernel</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">blurred</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">blurred</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">blurred</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">blurred</span><span class="p">))</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220558492-f8ce61ec-1d0b-4a25-a876-3af816573cf4.png" width="200" /></p><h4 id="thresholding-적용하여-mask-생성하기">Thresholding 적용하여 mask 생성하기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">thresholding</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">image</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">thresholding</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220558727-932c0804-839f-4f7a-9764-f26bc1571b6d.png" width="200" /></p><h4 id="sample-섞기">Sample 섞기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mixed</span> <span class="o">=</span> <span class="n">sample1</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">+</span> <span class="n">sample2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220558989-74c7a012-ad90-4a75-8c5c-c920d4082f32.png" width="200" /></p><p>이게 맞나…? <u>논문에서 주어진 그림처럼</u> 깔끔한 mask가 잘 안나온다.</p><p>아무튼 negative sample을 잘 만들어주었고, 이를 토대로 $4$개의 layer를 가지는 MLP를 학습한 결과 test error rate로 $1.37\%$를 얻을 수 있다고 한다. Label을 prediction할 때 <u>마지막 세 layer의 normalized activity vector를 사용</u>했으며, 첫번째 hidden layer의 output을 사용했을 경우에는 performance가 악화되었다고 언급한다. 물론 fully-connected layer 말고도 local receptive field를 가지는 구조를 활용할 수도 있는데, 이때는 보다 성능이 올라서 $1.1\%$의 오차율을 보였다고 한다. 참고로 <u>CNN이랑 조금 다른 점</u>은 원래 CNN에서는 weight parameter가 sharing되는데, FF 알고리즘에서는 필터의 weight가 공유되지 않는다는 것이다. 저자들이 밝힌 local receptive field 구조는 다음과 같다.</p><ul><li>The first hidden layer used a $4 \times 4$ grid of locations with a stride of $6$, a receptive field of $10 \times 10$ pixels and $128$ channels at each location. The second hidden layer used a $3 \times 3$ grid with $220$ channels at each grid point. The receptive field was all the channels in a square of $4$ adjacent grid points in the layer below. The third hidden layer used a $2 \times 2$ grid with $512$ channels and, again, the receptive field was all the channels in a square of $4$ adjacent grid points in the layer below. This architecture has approximately 2000 hidden units per layer.</li></ul><hr /><h1 id="supervised-learning-in-ff">Supervised learning in FF</h1><p>앞서 진행한 학습은 label 없이 representation을 학습하는 방법에 대한 문제였다(Like contrastive learning method). Unsupervised learning 방법은 네트워크 크기가 크고, 학습 가능한 feature가 여러 downstream task에 활용될 수 있을 때 유용하다는 특징이 있지만 굳이 그러지 않고 작은 네트워크를 <u>원하는 task</u>에 대해 <u>fine-tuning</u> 혹은 단순 <u>fitting</u> 시켜서 사용하고 싶을 수도 있다.</p><p>FF에서는 이를 해결하는 방법이 input에 label을 추가하는 것인데, 예를 들면 <u>text 학습 시에 앞단에 prompt를 붙여주는 것</u>처럼 이미지에 <u>label에 대한 정보를 함께 주는</u> 방식이다. 앞서 unsupervised learning에서 했던 것과는 다르게 이번에는 <u>label 정보가 맞다면</u>(correct label) positive data이고 <u>다르다면</u>(incorrect label) negative data가 된다. 이번에는 <u>positive</u>와 <u>negative</u> 간의 차이가 오직 label이기 때문에, FF 과정에서 negative label이 있다면 해당 상황에서 <u>모든 feature를 무시하게끔</u> 학습해야한다. 왜냐하면 잘못된 라벨과 이미지를 매칭하는 상황 자체가 잘못된 correlation이 형성되는 과정이기 때문이다.</p><p>MNIST 이미지는 black border를 가지고 있기 때문에 CNN을 적용하기 적절한 데이터 구조가 된다. 첫번째 $10$개의 픽셀을 $N$ 만큼의 <u>label representation 중 하나로 치환</u>하게 되면 첫 hidden layer가 학습하는 형태를 간단하게 시각화할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220803989-e3b018e6-b76c-4c74-938e-d0e36ec19d85.png" width="900" /></p><p>앞서 <u>unsupervised에서 사용했던 MLP</u> 구조(4개의 ReLU를 포함한 hidden layers)를 학습시켰을 때 $1.36\%$의 test error가 나왔다고 한다. Backpropagation은 FF보다 약 $1/3$만큼의 epoch에도 비슷한 결과가 나오는 것으로 봤을 때 아직 이 논문에서 제시하는 FF 알고리즘이 완벽하지는 않다는 것을 보여주는 것 같다. <u>수렴 속도를 늘리기 위해서</u> Learning rate를 높이고 더 적은 epoch에 대해 학습하게 되면 오히려 test error 성능이 $1.46\%$로 하락했다고 한다. FF 방법을 통해 학습하고 난 후에는 test digit에 neutral label($10$개의 픽셀이 모두 $0.1$ 값을 가짐)이 추가된 input을 분류할 수 있게 된다. 이런 방법을 사용하게 되면 사실 <u>들어가는 input을 제외하고</u>는 unsupervised learning과 동일한데, 첫번째 hidden layer의 activation을 제외하고 <u>나머지 activation을 모두 사용</u>하여 학습된 softmax를 통해 classification을 진행한다. 그러나 굳이 이런 방법을 사용할 필요가 없고, 단순히 특정 라벨과 test image를 통과시킨 후, <u>activation 결과로 축적된 goodness</u>가 가장 높은 label을 prediction하는 경우를 생각해볼 수 있다. 학습 과정에서 neutral label을 hard negative label로 사용할 때는 <u>학습이 더 어려워졌다</u>고 한다.</p><h2 id="supervised-learning-mnist-sample">Supervised learning MNIST sample</h2><h4 id="mnist-sample-가져오기">MNIST sample 가져오기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">'./data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">sample1</span><span class="p">,</span> <span class="n">label1</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="n">sample2</span><span class="p">,</span> <span class="n">label2</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
<span class="n">sample1_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample1</span><span class="p">)</span>
<span class="n">sample2_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample2</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220806706-912847a9-ee8d-401e-b736-9f3b60ade2b8.png" width="400" /></p><h4 id="mnist에-label-씌우기">MNIST에 Label 씌우기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">label_on_mnist</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">overlay</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,</span> <span class="p">:]</span>
    <span class="n">overlay</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">overlay</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">overlay</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220807486-75c16ef7-ece3-41b5-9c2a-f8a4c60e2706.png" width="600" /></p><h4 id="positive-sample-만들기">Positive sample 만들기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">overlay_pos1</span> <span class="o">=</span> <span class="n">label_on_mnist</span><span class="p">(</span><span class="n">sample1_array</span><span class="p">,</span> <span class="n">label1</span><span class="p">)</span>
<span class="n">overlay_pos2</span> <span class="o">=</span> <span class="n">label_on_mnist</span><span class="p">(</span><span class="n">sample2_array</span><span class="p">,</span> <span class="n">label2</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220806997-706465ff-ba3d-4c2e-8960-0052922621c0.png" width="400" /></p><h4 id="negative-sample-만들기">Negative sample 만들기</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">overlay_neg1</span> <span class="o">=</span> <span class="n">label_on_mnist</span><span class="p">(</span><span class="n">sample1_array</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="n">num</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="k">if</span> <span class="n">num</span> <span class="o">!=</span> <span class="n">label1</span><span class="p">]))</span>
<span class="n">overlay_neg2</span> <span class="o">=</span> <span class="n">label_on_mnist</span><span class="p">(</span><span class="n">sample2_array</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="n">num</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="k">if</span> <span class="n">num</span> <span class="o">!=</span> <span class="n">label2</span><span class="p">]))</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220808333-bb077ab7-9eae-41db-b552-c17731750bfe.png" width="400" /></p><hr /><h1 id="ff를-사용하여-perception-모델링하기">FF를 사용하여 perception 모델링하기</h1><p>지금까지 제시된 방법은 모두 <u>단일 layer</u>를 각각 <u>greedy algorithm</u>을 기반으로 따로 학습하는 과정이었다. 즉, 이후의 레이어가 학습하는 것이 기존의 <u>backpropagation과는 다르게</u> 이전 레이어의 학습에 아무런 영향을 미치지 않고 <u>독립적으로 학습된다</u>는 뜻이다. <br /> 이는 backpropagation에 대해 가지는 <u>FF 알고리즘의 명백한 한계점</u>이다. 따라서 computation에서 학습되는 과정보다는 인간이 실제로 시각 정보를 받아들이는 과정을 생각해보았다. 예를 들어, 단순히 이미지를 학습에 사용하고 끝나는 것이 아니라, 정적 이미지를 좀 지루한 비디오로 간주하고, 기존의 신경망 구조를 <u>다층 recurrent neural network</u>로 보자는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220810512-6c43d83a-b28e-4751-a096-559b28204b4d.png" width="700" /></p><p>그런데 여기서 의문점이 생긴다. 앞서 설명했던 FF는 forward pass를 진행하는 과정에서 positive와 negative data를 각각 processing하는 과정을 거쳤었는데, 결국 단일 이미지를 연속된 input으로 간주하게 되면 <u>negative sample은 학습에 활용할 수 없는 구조</u>가 되는 것이 아닌가라는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220811042-0f013fa1-2e35-458a-b80c-2433209320bf.png" width="700" /></p><p>저자는 다음과 같이 밝힌다. FF가 forwarding을 진행하는 것은 기존에 제시했던 positive data와 negative data에 대해 모두 해당되는 것은 맞지만, recurrent 구조에서는 <u>각 layer의 output이 activity에 관여하는 과정</u>만 달라지는 것이다.</p><p>예컨데 $t$ 시점에서의 이전 레이어의 output과 다음 레이어의 output(인접한 레이어를 의미)가 $t+1$ 시점에서의 <u>기준 레이어의 activity vector</u>를 결정하게 된다. 보다 디테일하게 구현한 형태는 다음과 같다.</p><p>저자는 MNIST 이미지를 여러 time frame으로 늘려 사용하였다. 가장 <u>bottom layer</u>는 위의 figure에서 볼 수 있듯이 MNIST 이미지를 의미하고 가장 <u>top layer</u>는 각 이미지의 one-hot encoding label을 의미한다. 각 hidden layer는 $2000$개의 뉴런을 가지고 총 $2 \sim 3$개의 층을 가진다고 한다. 위의 그림을 기준으로 보면 input/output layer를 포함해서 총 $4$개의 layer가 있다고 생각하면 될 것 같다.</p><p>이 논문이 preliminary로 삼은 recurrent multilayer learning 논문은 ‘How to represent part-whole hierarchies in a neural network’이다(<a href="https://arxiv.org/pdf/2102.12627.pdf">참고 링크</a>). 각 시점에서 짝수 번째의 layer activity는 홀수 번째의 layer activity를 normalize한 결과(앞서 봤었던 <u>layer normalization</u>)를 기준으로 업데이트가 되며, 반대로 홀수 번째의 layer activity는 짝수 번째의 layer activity를 normalize한 결과를 기준으로 업데이트가 된다.</p><p>그러나 이러한 방식의 alternating(번갈아 가며 학습하는) 구조는 biphasic oscillation(서로 다른 상(phase)이 공존하는 형태를 의미하는 말인 것 같은데, <u>학습 과정의 불안정성</u>이라고 보면 될 것 같다)를 방지하기 위함이었지만, 이러한 학습법이 굳이 <u>불필요하다는 것</u>이 밝혀졌다. 약간의 정규화 장치를 포함한 synchronize(짝수/홀수 layer를 구분하지 않고 <u>한번에 layer를 학습</u>하는 것)이 효과적이었으며, 실제 실험에서는 모든 층의 layer를 동시에 학습하였고 앞서 말했던 biphasic oscilation을 방지하기 위해 new pre-normalized state와 previous pre-normalized state를 <u>$7 : 3$로 weight하는 방식</u>을 대신 사용하였다.</p><p>앞서 본 구조에 따라 MNIST가 학습되는 과정은 이와 같다. 먼저 각 이미지는 hidden layer를 단일 bottom-up pass를 통해 초기화하게되고,</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220823188-bd5efd49-3dfe-4ef8-adc5-93e5961f0e4d.png" width="600" /></p><p>그 후에 $8$번의 synchronous iteration(damping 적용)이 <u>단일 이미지에 대한 학습</u>을 진행하게 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220824075-9118741a-72d0-4454-9a18-400776871989.png" width="600" /></p><p>Test data에 대해서 performance를 측정할 때에는 $10$개의 <u>각 라벨에 따라</u> goodness 측정을 하게 되고, $3$에서 $5$ iteration 동안 <u>평균 goodness가 가장 높은 label</u>을 기준으로 예측을 진행한다. 그 결과 $1.31\%$의 test error를 보였다고 한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/220824953-e237739d-d727-48b1-955d-7c6e6089680e.png" width="600" /></p><p>Negative data는 single forward pass를 통해 모든 class에 대한 probability를 구한 뒤 각 probability에 따라 incorrect class를 고르는 식으로 결정하였으며, 이러한 방식이 <u>학습 과정에서 효과적</u>이라고 하였다. 아무래도 probability에 따라서 incorrect class를 정하게 될수록 <u>hard negative sample이 생성될 가능성이 높기 때문</u>이지 않을까 생각해본다.</p><hr /><h1 id="predictions-from-spatial-context-as-a-teacher">Predictions from spatial context as a teacher</h1><p>Recurrent network 구조를 사용하게 되면 objective를 학습하는 과정은 위쪽 그리고 아래쪽 layer의 input과 <u>높은 agreement</u>를 가지게 된다(인접한 레이어끼리 의견 공유가 활발하다고 생각하면 된다). 즉 positive data에 대해서는 위쪽과 아래쪽 레이어와의 agreement가 크게, 반대로 negative data에 대해서는 작게끔 학습되는 구조가 되는데, 이는 <u>spatially local connectivity</u>를 가지는 네트워크 구조에서는 <u>좋은 property로 사용</u>될 수 있다.</p><p>위쪽 레이어로부터 내려오는 input은(top-down) prediction의 결과에 가까운 레이어로부터 오기 때문에 이미지를 기준으로 보다 넓은 영역에 의해 결정된 representation일 것이고, 이는 아래쪽 레이어로부터 올라오는 input(bottom-up)으로 하여금 어떠한 output을 만들어내야 하는지에 대한 내용이 된다. 즉 시간이 지남에 따라 <u>recurrent하게 forward forward algorithm을 적용하는 것</u>은 local representation을 학습하는 <u>bottom layers</u>가 prediction에 대한 contextual information을 가지고 있는 <u>top layers</u>로부터 영향을 받을 수 있게 되는 것이다. 반복된 input의 노출이 lower layer의 causality에 대한 문제를 줄여주고, layer 간 <u>의존성이 없던 forward 알고리즘</u>에게는 일종의 문맥상의 힌트를 줄 수 있게 된다는 것이다.</p><p>Positive data의 경우에는 <u>bottom-up information</u>(image에 대한 representation)이 prediction(label) information을 보다 <u>잘 반영하게끔</u> activity를 학습하고, 반대로 negative data의 경우에는 <u>bottom-up information</u>이 prediction(wrong label) information을 <u>cut-off(ignore)</u>하도록 activity를 학습하게 된다.</p><hr /><h1 id="cifar-10-dataset을-통한-실험">CIFAR-10 dataset을 통한 실험</h1><p>MNIST dataset의 경우 modality 특성상 단순한 형태를 가지다보니 forward forward algorithm으로도 충분히 성능이 보장되는 것을 확인할 수 있다. 하지만 CIFAR-10 dataset은 <u>이와는 다르게</u> $50,000$ 장의 $32 \times 32$ 크기의 RGB 이미지를 modality로 가지고, black border를 가지는 MNIST와는 다르게 background나 object의 형태가 보다 다양하기 때문에 <u>한정된 training data로</u> 모델링하기 힘들다는 특징이 있다. 그렇기 때문에 단순히 $3\sim 4$개의 hidden layer를 가지는 MLP 구조를 backpropagation 알고리즘으로 학습하게 되면 overfitting이 발생하는 등 학습이 제대로 진행되지 않는 것을 볼 수 있으며, <u>대부분의 결과</u>는 <u>convolutional</u> neural network 구조에 대한 내용이다.</p><p>앞서 설명했던 convolution 구조에서 얼핏 확인했겠지만, FF는 <u>weight sharing이 불가능</u>한 네트워크이다. 기존 CNN이라면 단일 convolutional layer에서의 parameter는 feature map의 모든 receptive에 동일하게 적용되겠지만, FF 알고리즘에서는 불가능하다. 그렇기 때문에 지나치게 weight 개수를 제한하지 않는 선에서 최대한 <u>적당한 갯수의 hidden unit를 적용한 backpropagation baseline과 비교</u>를 하였다.</p><p>네트워크는 총 $2 \sim 3$개의 hidden layer 수를 가지는데, 각 layer는 $3072$의 ReLU 노드로 구성된다. 각 hidden layer에서 output으로 나오는 <u>tensor의 차원을 모두 곱한 값</u>을 곧 노드 갯수라고 생각하면 된다($32 \times 32 \times 3 = 3072$). 연산이 되는 hidden unit은 $11 \times 11$의 receptive field를 가지기 때문에 각 point에서의 input 크기는 $11 \times 11 \times 3 = 363$이다. FF 알고리즘의 경우 sequential input에 대해 단일 레이어를 기준으로 top-down activity와 bottom-up activity에 대해 receptive를 가지는데, 가장 말단의 hidden layer를 제외하고는 모두 $11 \times 11$의 크기를, 가장 말단의 hidden layer의 경우 top-down activity에 대해서는 $10$의 input을 가지게 된다. 그림으로 간단하게 표현하면 다음과 같다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/221398579-a7a1b8db-3f6d-4a69-89c1-d2823e08e9ea.png" width="800" /></p><p>그림의 구조를 보면 마치 CNN(Convolutional Neural Network) 처럼 표현되어있지만, 사실은 <u>fully-connected되지 않은</u>(한정된 receptive field 크기를 가지는) <u>MLP 구조</u>로 볼 수 있다. 그리고 input/output resolution을 유지할 때 padding을 사용했던 CNN 방식과는 다르게, edge 부분의 hidden unit에 대해서는 receptive field를 <u>input에 맞게 truncate</u>해서 사용하게 된다. <br /> 학습 과정에서 CIFAR dataset 특성 상 overfitting 문제가 생기는 것을 방지하기 위해서 정규화 방법으로는 <u>weight decay</u>를 사용했다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/221399147-72e210a6-165d-42a2-a991-d375ee2976e5.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/221399178-0c5a71ff-e5af-448a-993c-8d285b9b9f7d.png" width="400" /></p><p>같은 학습 구조에 대해 <u>BP와 FF algorithm</u>의 각 objective에 대해 <u>training/test error</u>를 비교하였다. 실험하는 과정이 앞서 설명한 image를 boring video로 간주하는 작업이기 때문에 각 이미지를 $10$ iteration 동안 네트워크에 통과시키며 activity를 연산하고, 이 중에서 goodness 기반 error가 가장 적은 $4 \sim 6$의 <u>energy를 축적</u>하여(summation or mean으로 간주하면 될 듯) 하는 방법을 사용하거나, 단순히 <u>single-pass softmax</u>를 보는 방법 두 경우에 대해 모두 실험하였다.</p><p>결과를 보면 hidden layer의 개수에 따라 성능 차이가 거의 없다고 볼 수 없고, FF 알고리즘이 BP 알고리즘에 test error 기준으론 크게 뒤쳐지지는 않지만 확실히 기존의 BP가 FF에 비해서는 <u>training error가 피팅되는 속도</u>가 더 빠른 것을 볼 수 있다.</p><hr /><h1 id="sequence-learning-with-string-dataset">Sequence learning with string dataset</h1><p>현재 아카이브에 올라온 FF algorithm paper는 <u>수정본</u>이고, 이전에 올라온 <u>draft 버전</u>(<a href="https://www.cs.toronto.edu/~hinton/FFA13.pdf">참고 링크</a>)에 수록된 간단한 실험이 있다. 해당 내용에 대해 간단하게 요약하자면, 앞서 실험한 FF로는 연속된 이미지와 같이 video를 생성하기에는 아직 performance가 부족하지만, <u>문장을 완성하는</u> 형태의 <u>discretized sampling</u>은 상대적으로 구현하기 간단한 편에 속한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/221448892-056b30f8-1fcf-4495-a378-6e32e371dfa2.png" width="400" /></p><p>예를 들어 위의 그림에서 보는 것과 같이 공백을 포함한 $10$개의 character를 기준으로 <u>다음 character를 예측</u>하는 task를 생각해보자. 구현하기 가장 간단한 방법으로는 앞선 $10$개의 string character를 토대로 higher-order feature를 생성하는 앞단의 hidden layer와, 이 activity를 사용하여 <u>다음 character를 예측</u>하는 softmax(알파벳, 공백 혹은 마침표 등을 classification)를 고려할 수 있다. 실제 실험에서는 이솝 우화의 fable에서 각각 $100$개의 character로 구성된 $248$개의 string을 사용했으며, 공백이나 콤바, 세미콜론을 포함한 총 $30$개의 <u>정해진 문자 이외에는 모두 제거</u>하였다고 한다. 네트워크는 모든 hidden layer가 $2000$개의 ReLU 노드를 가지는 구조다. Positive sample은 기존 string에서의 $10$개의 character를 그대로 가져온 경우가 되고(The wolf r), negative sample은 마지막 character를 이전의 $10$개의 prediction 중 하나의 character로 대체하는 것으로 구성하였다(The wolf h). <u>Sequence to sequence</u> 구조에서 주로 활용하는 <u>teacher forcing</u>이 되는 것이다(ground truth를 학습에 사용하여 수렴 속도를 높이는 방법). 다른 방법으로는 negative data를 아예 predictive model의 output으로만 구성하는 방법이 있는데, 네트워크가 가장 앞단의 $10$개의 character를 ‘기억한다’는 가정을 하는 것이다. 사실 이 부분이 기존 논문에서 가장 이해가 안되는 부분이였는데, 앞서 마지막 character만 바꾸어 teacher forcing을 하는 방식과는 다르게 직접 <u>predictive model의 generation</u>을 데이터로 쓰게 되면, sleep phase에서 <u>offline으로 negative data를 만들어낼 수 있다</u>는 것이다. 이 부분에 대한 내용은 <u>수정본에서 다시 설명</u>되어 있어서 확인해보았다. 나름대로 이해한 사실을 바탕으로 정리하면 다음과 같다.</p><hr /><h1 id="sleep-phase-and-awake-phase">Sleep phase and Awake phase</h1><p>Awake phase는 positive data로 학습되는 과정을 의미하고, Sleep phase는 negative data로 학습되는 과정을 의미한다. Draft 논문에서는 <u>실제 실험을 토대로 아래와 같은 결과</u>를 얻을 수 있었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/221470160-05e65e73-1b99-435a-bc2e-5dc181ad6fe6.png" width="400" /></p><p>파란색은 hidden representation 학습이 실제 estimation에 미치는 영향을 보기 위해 <u>랜덤하게 초기화한 hidden weight</u>를 freeze한 채로 linear classifier를 학습하게 된다. 검은색 선과 붉은색 선이 사실상 가장 중요한 부분인데, synchronous phase(검은색)에서는 실제 data에 해당되는 positive sample 그리고 학습 과정에서 network가 generation하는 sequence를 negative sample로 사용하여 positive gradient와 negative gradient를 함께 학습하는 것이다. 이와는 다르게 seperated phase(붉은색)에서는 처음에는 positive sample만 사용하여 네트워크를 학습시키고, 그 뒤에 학습된 네트워크가 만들어내는 negative sample로 네트워크를 학습하는 것이다. 학습 과정이 분리되었지만 실제 성능 상으로는 synchronize하는 것과 거의 유사하였고 저자는 여기서 <u>하나의 해석을 추가한 것</u>이다.</p><p>성능이 크게 악화되지 않다면, 굳이 online(데이터셋을 사용하여 학습하는 과정) 상에서 negative sample을 사용할 필요 없이 offline(데이터셋 없이 네트워크를 통해 만들어낸 임의의 sequence)를 이후에 학습하면 된다. 사람으로 치면 awake(깨어있는) 상태에서 감각과 interaction하는 현실을 경험하고, sleep(자고있는) 상태에서 <u>학습된 경험에 대한 disequillibrium을 조정</u>하는 과정으로 볼 수 있다. 그렇기 때문에 저자가 언급하는 <u>sleep phase</u>란 실제 데이터셋 없이 <u>negative sample에 의한</u> 최적화 과정이고, 반대로 <u>awake phase</u>란 실제 데이터셋으로 <u>positive sample에 의한</u> 최적화를 진행하는 과정이 된다. 좀 재밌었던 파트는 draft에서의 결과 그래프가 어째선지 reproduce가 안되었는데(그래서 수정본에는 결과 그래프가 없다), 프로그램상 버그인 것 같다고 하는 저자의 푸념섞인 글이었다. <u>학습이 안정적이지 못하다는 점</u>이 reproduction에 실패한 이유 같기도 한데, phase를 번갈아가며 학습하는 과정은 learning rate가 아주 작으며 momentum이 극도로 큰 경우에만 수렴이 된다고 한다. 저자는 이런 시행착오를 겪는 과정 자체가 FF 알고리즘이 biological하다는 증거로 간주하고, 실제로 <u>사람이 수면 부족을 겪는 것</u>처럼 positive phase만 학습시키다 보면 <u>성능이 떨어지는 상황</u>이 발생한다고 한다.</p><hr /><h1 id="마무리하며">마무리하며</h1><p>사실 논문이 여기서 끝나는 것은 아니고 FF 알고리즘의 학습법이나 컨셉에 대해서 조금 더 서술해주는 부분이 실험이나 구현 뒤쪽에 나와있다. Contrastive learning에 대한 내용 그리고 레이어 정규화 작업, 하드웨어와의 연관성 그리고 해당 연구를 기반으로 또다른 학습 가능성에 대해 future work를 제시한다. 이후에 다른 포스트에서 따로 다룰 예정이다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/CoOp"><p id="prev_title"> ❮❮ Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/ffalgorithm2"><p id="next_title"> ❯❯ 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (2)</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/79881119?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
