<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | Low shot learning에 대하여</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | Low shot learning에 대하여"><meta name="og:description" content="Few shot, Zero shot learning"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/235353456-8461d395-cb9d-4471-8208-d72754519f4c.jpg"><meta name="og:image:alt" content="Welcome to my blog | Low shot learning에 대하여"><meta name="og:url" content="http://localhost:4000/blog/lowshot"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | Low shot learning에 대하여"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | Low shot learning에 대하여"><meta name="twitter:description" content="Few shot, Zero shot learning"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/235353456-8461d395-cb9d-4471-8208-d72754519f4c.jpg"> <!-- Search Engine --><meta name="description" content="Few shot, Zero shot learning"><meta name="image" content="https://user-images.githubusercontent.com/79881119/235353456-8461d395-cb9d-4471-8208-d72754519f4c.jpg"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | Low shot learning에 대하여"><meta name="author" content="JY"/><meta itemprop="description" content="Few shot, Zero shot learning"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/235353456-8461d395-cb9d-4471-8208-d72754519f4c.jpg"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a> <!--<hr class="navbar-divider"> <a class="navbar-item"> Report an issue </a> --></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { const gitcus_theme = localStorage.theme === 'dark' ? 'dark' : 'light'; function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: gitcus_theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; changeGiscusTheme(); /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/lowshot" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">Low shot learning에 대하여</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/235353456-8461d395-cb9d-4471-8208-d72754519f4c.jpg" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Few shot, Zero shot learning</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>December 20, 2022</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Meta learning</span> <span class="tag is-link">Low shot</span> <span class="tag is-link">Few shot</span> <span class="tag is-link">Zero shot</span> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 14 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><p>일반적으로 네트워크를 학습시킬 때, 대량의 데이터를 통해 최적의 parameter를 찾는 과정을 생각하게 된다. 그러나 만약 inference에 사용될 데이터셋에 대한 학습 데이터가 없거나 부족하다면, 네트워크는 <u>적은 데이터로도</u> 충분히 최적화될 수 있어야한다. 여기서 출발한 개념이 바로 ‘Low-shot learning’이며, 여기서의 ‘shot’은 <u>네트워크에게 제공되는 학습 데이터셋을 의미</u>한다.</p><h1 id="few-shot-learning">Few shot learning</h1><p>현재의 딥러닝 방법들은 few examples(적은 학습 데이터셋)을 기반으로 일반화가 불가능하다. 대부분 다량의 parameter를 업데이트하는 방법으로 충분히 <u>많은 training sample을 통한 정규화</u>에 초점이 맞춰져있으며, 특히 overfitting을 방지하기 위해 다양한 data augmentation과 같은 정규화 방식이 제안되고 있다. 그렇기 때문에 데이터가 없는 상황에서 특정 task에 대해 적용 가능한 최적화된 네트워크를 만드는 것은 어쩌면 불가능할 수 있다. 일반적인 few-shot learning(FSL)에는 다음과 같은 예시가 있다.</p><ul><li>Character generation : 캐릭터를 생성하는 작업의 경우, 해당 캐릭터에 대한 example이 많이 존재해야하지만, 저작권 문제나 이런 저런 이슈들로 인해 충분한 샘플을 확보하지 못할 수 있다.</li><li>Advance Robotics</li><li>Training sample을 얻기 힘든 task : drug discovery, FSL translation, cold-start item recommendation</li></ul><p>Machine learning에서는 computer program은 특정 <u>task</u> $T$로부터 나오는 $E$라는 <u>experience</u>를 학습하고, 학습 결과로 나오는 <u>performance measure</u> $P$에 대한 성능 향상을 이루는 것이 주 목적이다. 예를 들어 <strong>Image classification</strong>이라는 <u>task</u>가 있다면, 각 클래스 별로 존재하는 <strong>대용량의 labeled image</strong>(클래스 별로 구분된 이미지)가 곧 computer program이 경험할 수 있는 <u>experience</u> $E$가 되고, 네트워크로 하여금 예측된 각 이미지의 class에 대한 정확도(accuracy)가 측정 메트릭, <u>performance</u>가 된다. Few-shot learning에서는 바로 여기서 말하는 experience $E$가 현저히 부족한 상황에서의 문제를 이야기하며, 이를 딥러닝에서 사용하는 용어로 표현하자면 task $T$에 대한 supervision이 limited되었다고 할 수 있다. <br /> FSL 방법은 주로 사용할 수 있는 <u>supervision dataset</u> $E$를 활용함과 동시에, 이미 가지고 있는 <u>prior knowledge</u>와 함께 결합하여 learning이 feasible하도록 유도하는 것이다. 예를 들어 character generation이라면 supervision은 <u>각 캐릭터에 대해 존재하는 적은 샘플들</u>을 의미하고, 같이 활용될 수 있는 prior knowledge로는 캐릭터를 생성함에 있어서 <u>각 부분이나 관계에 대한 생성법</u>이 될 수 있다. 또다른 예시로 drug toxicity discovery에 대해서는 <u>새로운 분자 구조</u>가 주어지는 환경에서, 이미 알고있는 <u>유사한 형태의 분자 구조</u>를 prior knowledge로 생각해볼 수 있다. 마지막으로 image classification의 경우에는 <u>각 클래스별 라벨링된 데이터셋이 부족한 환경</u>에서, <u>다른 classification task에 대해서 학습된 네트워크</u>가 prior knowledge로 사용될 수 있다. <br /> 또한 이러한 few-shot learning에서의 특별한 케이스로, 학습 가능한 샘플의 수가 하나만 있는 one-shot learning, 그리고 task $T$에 대해서 참고할 만한 example이 아예 없는 zero-shot learning으로 구분될 수 있다. Zero-shot learning에서는 environment $E$가 다른 modality(attribute 혹은 word embedding 등)를 가지고 있어야 하고, 이를 통해 몇몇 supervised information을 transfer하여 inference가 가능하게끔 해야한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151534-534fbd4d-e614-4e2d-b9bc-e4219d464120.png" width="600" /></p><p>일반적으로 사람은 자신이 알고 있던 <u>배경 지식을 토대로 추정</u>하고(고양이를 살면서 한 번도 본적이 없는 사람이 고양이를 보고 강아지라고 한다), 만약 이렇게 추정된 내용이 잘못되었다고 하면(친구가 그건 강아지가 아니라 고양이라고 알려줌) <u>즉각적으로</u> 해당 object에 대한 지식을 얻게 되고(사실 강아지가 아니라 고양이었다는 사실을 알게됨), 다음 번에 다시 해당 object를 보게 되면(길고양이를 다시 마주함) 그때는 <u>잘못된 추론이 아닌 제대로 된 정답을 낼 수 있다</u>. 이를 딥러닝의 일련의 과정으로 나타내면 error를 통한 loss 발생이 한번에 해당 task에 대한 optimization으로 이어져서 한 번의 경험(데이터셋)으로도 일반화가 가능하다. 물론 이는 사람의 경우이고, 딥러닝에서는 단순히 이미지 하나에 대해서만 최적화를 하는 것은 optimal solution이 될 수 없다. 결국 이전에 우리가 진행했던 supervised learning에서의 학습법인 ‘learn from scratch’ 방식을 사용할 수 없다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151538-3c1ee16f-8ef6-4f65-a6ab-68cf0dc8a732.png" width="600" /></p><p>따라서 위와 같이 ‘support set’이라는 학습 가능한 하나의 에피소드를 구성하게 된다. 여기서 $N$-classes $K$-shots라 표현된 부분은, 학습에 사용될 support set의 클래스 개수가 $N$이고 각 클래스 별로 존재하는 샘플의 수가 $K$라는 것이다. 그리고 Query set은 이렇게 최적화된 네트워크를 통해 실제로 추론을 진행할 샘플이라고 보면 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151541-e5100481-f82d-4bc8-92c5-416fb4211a98.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/209151549-6aa5e461-d58a-4008-80fb-15c8f21a7811.png" width="400" /></p><p>일반적인 supervised learning은 image classification에서 첫번째 이미지와 같이, <u>모든 클래스에 대해</u> $N$개의 샘플이 있다면 그 중에서 $K$개를 training, 나머지 $N-K$개를 testing에 사용한다. 그러나 few-shot learning에서는 이와는 약간 다르게 <u>class중의 일부</u>(위의 예시에서는 5개)를 pre-training 및 training dataset에 사용하고, 나머지 class 5개에 대해서 <u>support set</u>(사전 학습된 데이터셋과 겹치지 않는 class)를 구성하게 되고, task에 맞게 적은 샘플들로만(ex. $3$ samples) 구성한다. 그리고 학습될 때 사용되지 않는 데이터(ex. $N-3$ samples)는 <u>query set</u>으로 이후 few-shot training의 성능 평가에 사용된다. Few-shot learning의 접근법에는 다음과 같은 방식들이 있다.</p><ul><li>Transfer learning</li><li>Data augmentation/Transformation/Synthesis</li><li>Meta Learning(learning to learn)</li><li>Metric based approach(Embedding Learning)</li><li>Multi-task Learning</li><li>Generative learning</li></ul><p>여기서 transfer learning, multi-task learning과 관련된 부분은 앞서 게시글 중 <a href="https://6unoyunr.github.io/blog/transfer">다양한 딥러닝 학습법</a>에서 다룬 내용과 같다. 이 중에서 이번 글에서는 ‘<u>학습 전략을 위한 학습</u>‘인 meta learning 그리고 ‘<u>metric 기반 추론법</u>‘인 embedding learning에 대해 살펴볼 것이다. 그리고 여력이 된다면 마지막 부분인 생성 모델을 활용한 방법에 대해서도 간략하게 정리를 해보도록 하겠다.</p><hr /><h1 id="utilizer-prior-knowledge-for-fslfew-shot-learning">Utilizer prior knowledge for FSL(Few-Shot Learning)</h1><p>다른 게시글에서도 언급했던 내용인데, transfer learning은 각 클래스 별로 적은 샘플만 가지고는 잘 작동하지 않는다. 그리고 fine tuning을 마지막 레이어에 대해서만 진행한다고 해도 학습에 사용되는 sample 개수가 너무 부족하기 때문에 <u>overfitting</u>의 위험성이 높다. 따라서 다른 유사한 problem들로부터 experience를 획득하고, 이렇게 얻은 prior knowledge를 활용하는 방식을 채택하게 된다. 이러한 prior-knowledge를 각 domain에서 얻는 방법으로는,</p><ol><li>Dataset으로부터 얻는 방법</li><li>Similarity로부터 얻는 방법</li><li>Learning으로부터 얻는 방법</li></ol><p>크게 세 방법으로 구분될 수 있다.</p><h2 id="prior-knowledge-about-data">Prior knowledge about data</h2><p>Dataset으로부터 prior knowledge를 얻는 방법은 비교적 심플하며, 정말 말 그대로 support set의 augmented dataset을 통해 진행된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151550-f1a2b664-68b4-4bb0-9712-c9060c9c710c.png" width="600" /></p><p>예를 들어 학습에 사용될 support set인 $D_{train}$이 있다고 해보자. 여기에서 추출할 수 있는 sample과 label pair에 해당되는 $(x_i,~y_i)$에 대해, 학습된 transformation function $t$를 적용해서 augmented support set $(t(x_i),~y_i)$를 ouput으로 얻을 수 있다. 이에 추가적으로 labeling이 부적합하거나 완료되지 못한 샘플 $(\bar{x},~-)$에 대해 $D_{train}$을 기반으로 라벨링을 진행한다. 즉, 예측값을 토대로 $(\bar{x},~t(\bar{x}))$와 같이 labeling을 하는 것이다. 또한 similar data sets로부터의 샘플 $(\hat{x}_j, \hat{y}_j)$를 aggregator $t$를 통해 조합한 output $(t( \hat{x}_j ),~t( \hat{y}_j))$를 도출하게 된다. 여기서는 mixup을 사용한다. 이처럼 transformer $t$가 prior knowledge로 사용되어 각 dataset category에 따른 output을 만들어낸다.</p><h2 id="prior-knowledge-about-learning">Prior knowledge about learning</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151554-3947069e-53d8-4172-9200-7ab0deff1d0b.png" width="600" /></p><p>Transfer learning은 사실 이 카테고리에 속하는 내용이다. 그러나 보다 좋은 방법이 바로 <u>Meta learning</u>이라는 방법이고, 학습 전략을 위한 학습법이다. 만약 task $T$를 해결하고 싶다면, meta-learning 알고리즘은 training task $T_i$의 여러 배치에 대해서 학습이 되며, 이때 중요한 것은 모든 $T_i$에 대해 [ T_i \cap T = \emptyset ] 임이 보장되어야 한다. 결국 이러한 다양한 task인 $T_i$에 대해서 학습되면서 정말 풀고자 하는 task $T$를 잘 학습하고자 하는 것이다. 쉽게 비유하자면 수능을 잘 풀기 위해서 모의고사를 많이 푸는 것이다. Training data에 대한 multiple task가 학습이 되고, 이러한 multiple task의 결과로 learning algorithm을 meta-learning한다. 이렇게 최적화된 learning algorithm을 사용해서 network를 학습시키고, 이렇게 <u>최적의 방법으로 최적화된</u> model을 사용하여 실제 task에 대한 성능을 확인한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151560-490a34d1-b32b-426f-a561-d5286a28f7ea.png" width="600" /></p><p>각 base set은 여러가지의 episode로 구성되고, 각각의 episode는 (support set, query set)으로 이루어져있다. 앞서 언급한 learning algorithm을 meta-learning하는 부분이 바로 이 base set을 사용하는 과정이다. 학습법에 대한 최적화가 끝나게 되면, 해당 방법을 통해 support set으로 네트워크를 학습한다. 이때는 실제 task에 대한 $N$-ways $K$-shots support set을 학습에 사용한 뒤 실제 task의 query set에 대한 예측 성능을 확인하게 된다. 모델은 여러 episode가 포함된 batch를 해결하면서 파라미터를 업데이트하고, 이러한 방법을 통해 새롭게 unseen few-shot classification task가 나왔을 때도 안정적이고 효과적으로 학습할 수 있는 학습법을 찾게 된다.</p><p><a href="https://arxiv.org/pdf/1703.03400.pdf">MAML(Model-Agnostic Meta-Learning)</a>이 이러한 learning 알고리즘 중 하나로 소개되는데, meta learning을 활용하면서도 동시에 neural network를 <u>두 가지의 backpropagation을 진행</u>하면서 최적화한다. 가장 메인이 되는 컨셉은, neural network를 학습하는 과정에서 빠르고 적은 샘플로도 새로운 classification task에 적응 가능한 parameter를 가지게끔 학습하는 구조를 잡는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151563-ca25dde5-5a2a-485f-9cd9-be11937bd357.png" width="400" /></p><p>따라서 MAML은 총 두 개의 neural network 구조를 가지고, 두 네트워크는 서로 같은 구조를 공유한다. 즉 하나의 네트워크를 통해 두가지의 역할을 수행하는데, 그 중 하나는 learner(학습자)로서 앞서 언급했던 것과 같이 새로운 task에 대해 잘 학습할 수 있는 학습법(meta-learning)을 학습하고, 나머지는 adapter(적응자)로서 각 task에 빠르게 적응할 수 있는 알고리즘을 학습한다. 서로 다른 task를 해결하는 과정이기 때문에, 각자의 task에 맞는 learning rate이 사용된다. 구체적인 학습 알고리즘을 각 step 별로 나타내면 아래와 같다.</p><ul><li>Learner(학습자)를 랜덤하게 초기화한다.</li><li><p>meta-training에 사용되는 모든 episode에 대해 아래와 같은 과정을 반복한다. 반복은 특정 epoch 수만큼 진행되거나, meta-parameter가 안정화에 접어들 때까지 진행한다.</p><ul><li>Meta-training에 사용될 episode의 batch를 샘플링한다.</li><li>Adapter의 parameter를 learner의 parameter로 초기화한다.</li><li>Inner training step이 정해진 횟수만큼 진행될 때까지, adapter를 batch의 support set에 대해서 학습시키고 loss 및 gradient based optimization을 진행한다.</li><li>최적화된 adapter의 parameter를 사용하여 batch의 query set에 기반한 meta-loss를 계산한다.</li></ul></li><li>meta-gradient를 계산한다. 위에서 batch의 query set에 기반한 meta-loss를 계산하였는데, 이를 토대로 meta-parameter를 최적화하고, learner의 parameter를 update한다.</li></ul><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151566-dea16fbd-3baa-4beb-8154-83aea72e01fe.png" width="600" /></p><p>각 task에 대해서 구체적인 과정을 살펴보면 다음과 같다. 먼저 meta-learning에 있어 <u>학습된 learner</u> $M$을 <u>복사한 adapter</u> $f$를 만들고, $f$의 parameter를 learner의 parameter인 $\Theta$로 초기화한다. 그런 뒤 meta-training support set으로 adapter $f$를 빠르게 fine-tuning한다. 이렇게 특정 task의 support set에 대해 최적화가 끝난 adapter $f$에 query set을 적용하고, 이 과정에서 추출된 meta-learning loss를 사용하여 $\Theta$와 함께 meta-learning parameter를 최적화한다. <u>Meta-training</u> 과정에서는 MAML은 <u>initialization parameter</u>를 학습하게 되며, 각 network(adapter)가 새로운 few-shot task에 대해 빠르게 학습 및 최적화될 수 있도록 학습된다. 그리고 <u>Inference</u>에는, meta-trained model을 사용하여 meta-test set를 예측하고, 여기서 중요한 점은 query set에 대한 <strong>추가적인 gradient는 있지만</strong> 직접적으로 learner의 parameter는 이를 통해 <strong>바뀌지 않는다</strong>. 즉 학습된 초기화 지점은 고정으로, adapter 및 meta learning 과정만 학습하는 것이다. MAML 방식은 어떠한 neural network에서도 활용될 수 있기 때문에 model agnostic하다고 할 수 있다.</p><h2 id="prior-knowledge-of-similarity">Prior knowledge of similarity</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151570-ff472ca7-4067-4427-9f6e-752e1864e665.png" width="600" /></p><p>해당 내용에 대한 알고리즘은 compact representation(일종의 embedding으로의 mapping)을 학습하고자 하는 것이고, 여기서 data vector는 intra-class variation에 대해 크게 영향을 받지 않으며 각 class 간의 관계를 잘 유지할 수 있는 space를 구성하게끔 만드는 것이 주된 목적이다. 이를 다른 말로 <u>metric(embedding) learning approach</u>라 부른다. 모델링에 있어, similarity function 혹은 metric을 각 샘플에 대해 적용하여 similar sample의 경우는 closer, 상이한 sample은 take apart하는 형태로 학습한다. 이전에 다뤘던 contrastive 개념과 유사하다. 대표적인 방법으로는 <a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">Siamese</a> Network가 있다. 이외에도 Matching network, Prototype network 그리고 Relation network 등등이 있다. 간단하게 위의 그림을 참고해서 <u>support set</u>에 ‘3-way and 3-shots’ sample이 있다고 가정해보자. 총 9개의 샘플에 대해 encoder $E$를 통과시킨 결과 embedding($g$)를 획득할 수 있다. 이렇게 획득한 총 9개의 임베딩을 실제 mappind된 embedding space에서 보게 되면, 서로 같은 class에 속하는 샘플 3개씩은 모여있어야 하고, 그와 동시에 서로 다른 class는 일정 거리만큼 떨어져 있는 것이 이상적이다. 이런 식으로 학습된 네트워크에 query set image를 encoder $E$에 통과시킨 결과 embedding($f$)를 얻을 수 있고, 이렇게 매핑된 query embedding이 가장 가까운 거리(nearest neighbour)를 보이는 class가 바로 이 query image가 예측되는 class가 되는 방식이다. <br /> Siamese network는 두 개의 쌍둥이 형태의 sister network를 구성하고, 두 네트워크는 같은 weight를 공유한다. 해당 네트워크를 최적화하는 과정에서 다음과 같은 contrastive loss function을 계산한다.</p><p>[ (1-Y)\frac{1}{2}(D_W)^2 + (Y)\frac{1}{2} (\max (0, m-D_W))^2 <br /> ] 여기서 $Y = (0, 1)$은 서로 다른 클래스인지 여부에 대한 <strong>T/F value</strong>가 된다. 만약 두 개의 input이 같은 class라면 $0$을, 다른 class라면 $1$이 된다. 일종의 <u>클래스 종류에 대한 exclusive 'OR'</u>이라고 생각하면 된다. Margin m은 0보다 큰 값으로, Euclidean distance metric $D_W$가 특정 값 이상인 두 샘플에 대해서는 고려하지 않는다. 학습은 similar pair $((x_i,~x_j), 0)$와 dissimilar pair ((x_i,~x_k), 1)을 네트워크에 통과시킨다. Similar pair와 Disimilar pair가 서로 같은 비율일 때가 학습이 가장 안정적이다. 이렇게 획득된 pair에 대해서 loss를 계산하고, 이에 weight를 최적화하는 과정을 거친다. 클래스별 Shot 개수 $E$, class의 개수 $C$에 대해 similar sample과 different sample의 개수를 수식으로 표현하면 다음과 같다.</p><p>[ N_{same} = \left( \begin{matrix} E \newline 2 \end{matrix} \right)C ]</p><p>[ N_{diff} = \left( \begin{matrix} EC \newline 2 \end{matrix} \right)- \left( \begin{matrix} E \newline 2 \end{matrix} \right)C ]</p><p>다음은 matching network이다. 앞서 언급했던 내용과 유사하게 support set은 $(x_i,~y_i)$가 있고, 각각의 $y$는 class별로 표현된 one-hot label vector가 된다. 그리고 $f$, $g$는 embedding function이라고 하자. Classifier는 weighting factor $a$(softmax)와 query sample $\hat{x}$에 대해서,</p><p>[ \hat{y} = \sum_{i=1}^k a(\hat{x}, x_i)y_i = P(\hat{y} \vert \hat{x},~S) <br /> ]</p><p>이처럼 나타내어진다. 여기서 weighting factor를 적용하여 support set에 대해서 query sample의 attention을 계산하는 것이다. Attention kernel은,</p><p>[ a(\hat{x},~x_i) = e^{c(f(\hat{x},~g(x_i)))} / \sum_{j=1}^k e^{c(f(\hat{x},~g(x_j)))} <br /> ] 이처럼 표현되며 이는 contrastive loss에서 표현되는 similarity term과 동일하다. 위의 수식에서 $c$는 cosine similarity로, 두 샘플이 유사한 확률로 간주할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151572-c066ff1a-9b6a-4390-8993-8053fa56669d.png" width="600" /></p><p>이러한 방식을 통해 계산한 term이 곧 similarity에 기반한 nearest neighbor가 된다. 앞서 언급한 Siamese network에서의 $D_W$는 Euclidean distance였지만 matching network에서는 cosine distance(similarity)라는 점이 차이가 될 수 있겠다. Matching network는 결국 support set $S$에 기반한 query $x$의 output class $y$를 예측하는 classifier network $P$의 log likelihood를 최대화하는 방향이 된다.</p><p>[ \theta = \arg \max_\theta \mathbb{E}_{L \sim T}\left(\mathbb{E}_{S \sim L,~B \sim L} \left(\sum_{(x,~y) \in B} \log P_\theta (y \vert x,~S) \right) \right) <br /> ]</p><hr /><h1 id="zero-shot-learning">Zero-shot learning</h1><p>앞서 소개했던 방법들은 샘플의 수가 <u>최소한 1개 이상일 경우</u> 사용할 수 있는 방법이고, 다음에 볼 내용은 zero-shot learning에 대한 부분이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151573-accfd4ee-7dcc-469c-9620-6482c65e020e.png" width="600" /></p><p>사람의 경우, 살면서 얼룩말은 한번도 본적이 없다고 하더라도 말에 대한 지식을 알고 있으며, 얼룩말은 단순히 말과 비슷하게 생겼으나 무늬가 stripe로 구성된 형태라는 말만 듣고서도 충분히 얼룩말을 구분해낼 수 있다. ZSL(Zero-shot learning)은 이렇게 이미 알고 있는(seen) class에 대한 존재를 기반으로 이에 의존하는 inference를 제시하며, 이렇게 사전 학습된 knowledge 혹은 attribute를 통해 어떤 식으로 unseen class에 대응할 수 있는지에 대한 task가 된다. <u>Attribute</u>는 일종의 <strong>category vector</strong>로 표현된다. <br /> ZSL에서 데이터셋은 다음과 같은 구성을 가진다.</p><ul><li>Seen classes : 학습 과정에서 labeling된 이미지</li><li>Unseen classes : 학습 과정에서 볼 수 없었던 class를 가진 이미지</li><li>Auxiliary information : 각 클래스에 대한 묘사/semantic attributes 혹은 word embedding을 의미. Seen class와 Unseen class 사이에 유의미한 관계를 이어줄 다리 역할을 하게 된다.</li></ul><h2 id="semantic-vectors">Semantic vectors</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151577-537bd738-3dc3-4c9c-ac03-58b43ca0016c.png" width="200" /></p><p>그렇다면 Semantic vector는 어떠한 방식으로 구할까? 이를 테면 위의 그림은 Attribute vector의 한 예시이다. 말 그대로 visual appearance를 각 property에 대해 매핑하는 과정이다. 이렇게 되면 ‘고양이’라는 class를 한번도 본 적이 없더라도 ‘꼬리가 있고, 털이 있다’ 등의 attribute로 예측이 가능하다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151580-798e115f-6d65-44b5-b4a9-b24e7dc9bb2c.png" width="500" /></p><p>또한 다른 방법으로는 word vector가 있다. NLP 기술을 활용해서 attribute에 대한 정보를 자동적으로 학습하는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151584-c4f48fe5-2164-4fdf-a3be-3bd5649d00f3.png" width="500" /></p><p>이렇듯 image feature를 실제로 semantic feature에 대입하기 위해서는 deep learning network를 활용해야한다. 이렇게 image feature와 semantic attribute가 같이 공존하는 embedding 공간을 <u>common embedding space</u>라 부르게 된다. 일종의 만남의 광장같은 느낌이다. Common embedding space는 high level vision의 visual space가 될 수도 있고, low level vision의 semantic space가 될 수도 있으며, 혹은 새롭게 학습된 중간 space가 될 수도 있다. 학습은 간단하게도 semantic vectors $v$를 seen data $x$에 대한 supervision으로 삼아서 projection function $f$를 $v = f(x)$와 같이 학습하게 된다. Inference를 진행할 때는 새로운 class dataset $(x^*,~y^*)$에 대해 class specified semantic vector $v^*$를 정의하고, semantic vector space에 mapping된 $f(x^*)$를 기준으로 가까운 neighborhood로 예측을 하게 된다. Attribute에 대해서 제대로 학습이 된 embedding space라면 새로운 class에 대한 attribute도 합리적으로 mapping할 수 있기 때문에, 해당 space에서 구분하면 학습에 사용되지 않았던 class에 대해서도 discrimination이 가능하다. <br /> Task의 종류에 따라 zero-shot learning 그리고 generalized zero-shot learning으로 구분할 수 있는데, 학습에서 사용된 class에 대해서도 함께 구별하는 task가 generalized zero-shot learning이며 그와는 다르게 test에서만 사용되는 new class에 대해서 discrimination하는 zero-shot learning이 있다.</p><hr /><h1 id="generative-model-based-methods">Generative model based methods</h1><p>그러나 embedding method에 대해서 발생하는 단점이 있는데, 바로 <u>Bias</u>랑 <u>Domain shift</u>이다. Mapping 함수 $f(\cdot)$는 seen class에 대해서만 학습을 하기 때문에 output을 추출하는데 있어 기존에 학습될 때 본 sample들에 대해서 <u>biasing</u>되는 문제가 생긴다. 그리고 test modality에 대한 domain shift에 의해, $f(\cdot)$가 unseen class image feature를 semantic vector로 제대로 mapping한다는 보장이 없기 때문에 단점이 존재한다. 풀어서 설명했지만 간단하게 말하자면 test sample에 대해서도 <strong>semantic vector를 잘 뽑아내어야</strong> 위의 방법이 의미가 있는데, 결국 zero-shot이라서 unseen class에 대한 보장이 안되기 때문에 곤란하다는 뜻. 따라서 이런 단점을 보완하고자, zero-shot classifier가 <u>seen/unseen class 모두에 대해서</u> 학습 과정에서 guidance를 받을 수 있는 방법이 필요하다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151588-e31ecf78-f3a0-4d16-923a-aeac02e3fc01.png" width="500" /></p><p>그래서 고려를 한 방법이 generative network에 고양이 이미지에 대한 ‘attribute vector’를 conditional GAN 형태로 넣어주며, fake image가 이러한 out-of-distribution sample의 역할을 대신할 수 있게끔 하는 것이다. 이를 통해 GAN이 unseen class에 대한 semantic feature를 보완해줄 수 있는 효과를 기대하는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209151590-e5754607-9dbe-4676-8fd7-2d72b0bb6c30.png" width="600" /></p><p>Conditional GAN이 학습되고 나서는 generator의 weight를 고정한 채 unseen class의 attribute를 input으로 전달하여 image feature를 뽑게 한다(이미지 자체를 넣는게 아니라 cGAN이 생성한 feature를 예측에 사용하는 것). 그렇게 되면 GAN이 생성하는 이미지의 퀄리티는 어떨진 모르겠지만, image를 직접적으로 사용하는 방법과 다르게 class agnostic한 학습에 사용될 수 있다는 점이 concept으로 사용되었다고 볼 수 있다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/transfer"><p id="prev_title"> ❮❮ 다양한 Deep learning 학습법</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/trnmultimodal"><p id="next_title"> ❯❯ Transformer와 Multimodal에 대하여</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="gitcus" id="gitcus_container"> <script src="https://giscus.app/client.js" data-repo="6unoyunr/comments" data-repo-id="R_kgDOODmwzQ" data-category="General" data-category-id="DIC_kwDOODmwzc4Cn5wD" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="ko" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
