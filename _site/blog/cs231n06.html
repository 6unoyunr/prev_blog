<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"><meta name="og:description" content="Lecture summary"><meta name="og:image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"><meta name="og:image:alt" content="Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"><meta name="og:url" content="http://localhost:4000/blog/cs231n06"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"><meta name="twitter:description" content="Lecture summary"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Search Engine --><meta name="description" content="Lecture summary"><meta name="image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"><meta name="author" content="JY"/><meta itemprop="description" content="Lecture summary"><meta itemprop="image" content="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/cs231n06" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://github.com/user-attachments/assets/cdb8f444-3819-448c-868e-03e4fabb4dcc" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Lecture summary</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>November 07, 2022</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span> <span class="tag is-link">cs231n</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 26 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="들어가며">들어가며…</h1><p>이전까지 했던 내용에 대한 <strong>전반적인 요약</strong>은 다음과 같다. Linear classification을 예시로 들면서 softmax, SVM(support vector machine)의 classifier를 소개했었고, 이러한 classifier가 학습되는 과정에서 사용되는 score function의 한 형태인 <u>neural network</u>를 언급했었다. 생체 뉴런을 유사한 형태로 표현한 <u>perceptron</u>의 구조와 각 연산이 가지는 의미에 대해서 살펴봤었고, 단순한 논리 구조를 벗어나 <u>non-linearity</u>가 적용된 여러 층의 레이어를 가지는 <u>deep neural network</u>를 chain-rule에 기반하여 최적화하는 과정을 통해 보다 복잡한 형태의 함수를 가지는 real-world task들에 대한 universal approximator로 사용될 수 있는 것을 확인하였다. 다만 레이어가 깊어지면 깊어질수록 <u>representation power</u>(복잡한 함수를 표현할 수 있는 정도)는 증가하지만 그에 따른 부작용으로 overfitting(weight값의 표준편차가 커지는 것, training dataset에 네트워크가 과적합되는 것)이 생겼고, 이를 해결하기 위한 수단으로 여러 regularization 방법들(dropout, $L_2$ regularization 등등)을 소개하였다. <br /> 이번에는 지금까지 살펴본 딥러닝 네트워크의 구조나 의의가 아닌, 실질적으로 학습 과정에서 필요한 데이터 전처리, Weight의 초기화에 대해 알아보고 학습 시 batch normalization과 같은 regularization이 가지는 장점에 대해 알아보는 글이 될 것이다.</p><hr /><h1 id="data-preprocessing전처리">Data preprocessing(전처리)</h1><p>사용할 데이터 $X$에 여러 조작을 가해보도록 하자. 여기서 사용되는 data $X$는 $X \in \mathbb{R}^{N \times D}$의 batch 단위의 matrix를 가정하도록 하자. $N$은 data sample의 갯수를 의미하고, $D$는 각 샘플의 dimensionality(차원)을 의미한다. 예를 들어 만약 $3 \times 32 \times 32$의 RGB 채널을 가지는 이미지가 총 10장이 있다면 $N = 10,~D = 3072$가 될 것이다.</p><h2 id="original-data">Original data</h2><p>가장 먼저 사용할 데이터 자체를 의미하는 original data에 대해서 살펴보자. 원본은 아무런 preprocessing이 되지 않은 그대로를 의미하고, 다음과 같이 데이터가 분포한다고 가정해보자.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213594618-5db41741-b847-4fae-a6e3-d7dbce8e8f31.png" width="300" /></p><p>간단하게 $2$차원의 데이터를 가정했으며, 좌표평면의 각 점은 sample을 의미한다.</p><h2 id="mean-substraction">Mean substraction</h2><p>데이터 전처리에서 사용되는 방법 중 하나는 data 분포에 존재하는 bias를 없애주기 위한 ‘<u>mean substraction</u>’ 작업이다. Mean substraction은 각각의 dimension에 대한 평균값을 빼주게 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original_data</span> <span class="o">=</span> <span class="n">X</span> <span class="c1">#(assume that X is numpy array with shape N*D)
</span><span class="n">zero_centered_data</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p><code class="language-plaintext highlighter-rouge">numpy</code> 모듈을 잘 모르는 사람이 있을 수도 있기 때문에 언급하자면 <code class="language-plaintext highlighter-rouge">np.mean(_, axis=0)</code> 메소드의 경우에는 인자로 들어가게 되는 array의 <u>$0$번째 축</u>에 대한 평균을 구하고자 하는 것이다. $0$번째 축을 따라서 평균을 내는 것은 다시 말하자면 $2$차원의 데이터에 대해서 첫번째 column vector와 두번째 column vector($\mathbb{R}^{N \times 1}$)의 평균을 빼주는 과정이다. $2$차원 데이터에 대해서 각 column vecor의 평균으로 bias를 없애게 되면, original data가 가지던 분포의 중심점이 원점인 $(0, 0)$로 이동하게 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213595506-e8b3356f-67c8-4742-943e-7d1dac31aef1.png" width="300" /></p><h2 id="normalization">Normalization</h2><p>각 차원 축은 하나의 feature로 이해할 수 있고, 만약 위의 그림과 같이 <u>각 차원 축에 대해 분산값이 상이할 경우</u> 학습 과정에서 최적화 시 가지는 중요도나 learning rate 비율이 달라질 수 있기 때문에 이를 어느 정도 유사하게 맞춰주는 작업이 필요하다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original_data</span> <span class="o">=</span> <span class="n">X</span> <span class="c1">#(assume that X is numpy array with shape N*D)
</span><span class="n">zero_centered_data</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">normalized_data</span> <span class="o">=</span> <span class="n">zero_centered_data</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213595891-6d7ac658-9be3-48d8-8193-78870d3a2492.png" width="300" /></p><hr /><h1 id="pca-and-whitening">PCA and Whitening</h1><p>위에서 본 내용은 dataset sample을 정규화하는 preprocessing이었다. 일반적으로 feature vector는 위에서 보는 바와 같이 각 column마다 correlation이 어느 정도 존재하는 형태가 된다($y = ax$). 이러한 correlation을 풀어주는 작업이 <u>PCA and Whitening</u>이라고 생각하면 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original_data</span> <span class="o">=</span> <span class="n">X</span> <span class="c1">#(assume that X is numpy array with shape N*D)
</span><span class="n">zero_centered_data</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">zero_centered_data</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">zero_centered_data</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div><p>연산 과정은 다음과 같다. 가지고 있는 data $X \in \mathbb{R}^{N \times D}$에 대해 zero-centered matrix를 구한 뒤, 이렇게 구해진 zero-centered matrix를 서로 inner product한 뒤 샘플의 개수 $N$으로 나눠주면 covariance matrix를 구할 수 있게 된다.</p><p>[ Cov(X) = \frac{\left(X-\mu(X) \right)^\top \left(X-\mu(X) \right)}{N} <br /> ]</p><p>구한 covariance matrix의 각 요소가 의미하는 것은 $i$번째 feature와 $j$번째 feature의 관련성을 의미한다. 위의 예시에서는 $2 \times 2$ matrix가 나오게 된다. 따라서 covariance matrix는 자동으로 symmetric matrix가 되는데, 이 행렬의 diagonal component는 feature 각각에 대한 autocorrelation이고, autocorrelation은 수학적으로 보면 variance가 된다. Covariance matrix에 singular value decomposition(SVD)를 수행하게 되면 covariance matrix로부터 eigenvector $U$, eigenvalues $V$, singular values $S$를 추출할 수 있게 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">covariance_matrix</span><span class="p">)</span>
</code></pre></div></div><p>SVD를 수행하는 이유는 covariance matrix의 요소가 각 feature에 대한 correlation을 표현한다고 하였는데, 이 식에서 orthonormal matrix $U$를 projection에 대한 basis로 사용하여 원본 데이터(평균에 대한 bias가 제거된)의 correlation을 없애기 위한 용도로 작용한다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_dr</span> <span class="o">=</span> <span class="n">zero_centered_data</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
</code></pre></div></div><p>Dot production이 의미하는 것은 orthonormal basis $U$에 따른 축 rotation을 의미한다. 앞서 살펴봤던 바와 같이 기존 데이터가 각 feature vector에 대한 correlation을 가지고 있었는데, 축을 회전함으로써 이를 제거해줄 수 있다(decorrelation 과정).</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213595506-e8b3356f-67c8-4742-943e-7d1dac31aef1.png" width="300" /> <img src="https://user-images.githubusercontent.com/79881119/213597407-40ac00c8-6802-4342-b096-054e8b35bd40.png" width="290" /></p><p>여기서 PCA는 <u>Principal Component Analysis</u>인데, 이는 SVD로 하여금 추출된 feature basis를 사용하는 것이 아니라 ‘중요한’ feature만 사용하겠다는 의미가 된다. 만약 eigenbasis에서 eigenvalue가 큰 값을 기준으로 정렬했다고 생각한다면 $D$개의 feature 중에서 정말 중요한 $100$개를 사용한다고 생각해볼 수 있다. 물론 위의 경우와 같이 $2$차원 데이터일 경우엔 굳이 PCA를 적용할 필요가 없지만, 만약 feature의 차원 수가 늘어나게 되면 유의미한 feature만 사용하는 것이 중요하다. <u>Curse of dimension</u>이라 표현되는 해당 문제는 data가 구성하는 manifold는 실제로 데이터가 놓인 공간이 아닌, 공간의 일부를 이루는 hyperplane 등등 특정 표면을 구성한다는 점에서 등장한다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xrot_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">U</span><span class="p">[:,:</span><span class="mi">100</span><span class="p">])</span> <span class="c1"># Xrot_reduced becomes [N x 100]
</span></code></pre></div></div><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213598817-8a7b082a-abcf-44e4-9ca7-ae6c7e3c947e.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/213598863-fcf469d6-24f5-4614-8b83-d21acd81eaa7.png" width="400" /></p><p>대표적인 데이터셋 중 하나인 Swiss roll의 경우 3차원으로 구성되어있지만 데이터셋 분포는 말려있는 하나의 plane(평면)을 구성하며, 이를 그대로 3차원의 공간에 대해서 활용하는 것보다 <u>데이터 분포를 가장 잘 나타낼 수 있는 평면</u>을 기준으로 projection해서 사용하는 것이 PCA의 한 예시가 된다. <br /> 앞서 살펴본 예시에서 차원 수를 줄여서 사용한다면, 우리가 사용할 데이터셋은 $N \times D$에서 $N \times 100$으로 적은 feature를 가지게 되면서 그와 동시에 decorrelation이 진행된 데이터셋이 될 것이다. 여기에 추가로 data를 eigenbasis에 대해 projection 시킨 다음에 각각의 dimension을 eigenvalue로 나눠주게 되면, 각 basis에 data sample들이 가지는 variance로 normalization이 가능하다. 이를 whitening이라고 한다. $X$는 애초에 basis에 대해서 회전된 상태(projection)이고, 우리가 가지는 각 feature에 대한 분산 정보는 eigenvalue가 가지고 있다. 왜냐하면 해당 행렬의 diagonal에는 각 basis에 대한 $\sigma$가 있고, 해당 value가 covariance matrix에서 diagonal element를 결정하기 때문이다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
</span><span class="n">Xwhite</span> <span class="o">=</span> <span class="n">Xrot</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div></div><p>Whitening이라고 불리는 작업은 평균이 $0$이고 identity covariance matrix를 가지는 white noise 형태의 분포를 만든다는 관점에서 나온 이름이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213600588-8f6cfa86-243f-4354-882c-40df505ec05a.png" width="800" /></p><hr /><h1 id="weight-initialization">Weight initialization</h1><p>KNN(K-neareat neighbors)과 같은 <u>instance-based learning</u>(training dataset이 instance로 사용되어 test data의 예측에 관여하는 것)이 아닌 <u>model-based learning</u>은 모델 구조에 따라 weight parameter가 존재하고, 이를 update하는 과정으로 학습을 진행한다. 그렇기 때문에 학습을 시작하게 될 <u>weight의 초기화</u>에서도 성능의 차이가 발생하는데, 어떤 식으로 하는 것이 가장 효율적일까?</p><h2 id="zero-initialization">Zero initialization</h2><p>가장 먼저 생각해볼 수 있는 것은 모든 weight를 $0$으로 초기화하는 것이다. Weight의 초기화 방법은 배제하고 최적화된 네트워크의 parameter가 어떤 값을 가지는지 모두 예측할 수 없지만, 수없이 많은 parameter를 가지는 deep neural network 구조에서 weight의 절반은 <u>positive</u>로, 나머지는 <u>negative</u>로 학습이 될 것으로 예상할 수 있다(큰 수의 법칙). 그렇기에 처음부터 평균이 $0$인 상태로 시작하면 어떤 weight는 positive로, 어떤 weight는 negative로 가면서 자연스럽게 우리가 생각했던 이상적인 weight parameter의 구조를 가질 수 있지 않을까 싶지만 이는 잘못된 관점이다. 만약 network의 모든 뉴런이 같은 output을 내보내면, backpropagation 과정에서도 같은 값으로 update가 될 것이다. 즉 모든 weight가 같은 값으로 초기화되면 weight matrix가 assymetric하지 않게 된다. 간단한 예시를 위해 다음과 같은 perceptron code를 짜보았다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># sigmoid as an activation function
</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># input : 2 * 3 [N * D]
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># target : 2 * 2 [N * out]
</span><span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Weight 1 [3 * 4]
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Weight 2 [4 * 2]
</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># feed forward
</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

<span class="c1"># calculate difference
</span><span class="n">diff</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">out</span>

<span class="c1"># backpropagation
</span><span class="n">W2</span> <span class="o">-=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">-=</span> <span class="nb">input</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">hidden</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">hidden</span><span class="p">)))</span>

<span class="c1"># print results
</span><span class="k">print</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
</code></pre></div></div><p>코드는 가독성이 떨어지기 때문에 수식으로 표현하면 다음과 같다.</p><p>[ \begin{aligned} \text{input} =&amp; \begin{bmatrix} -1 &amp; 0 &amp; 1 \newline -1 &amp; 1 &amp; 0 \end{bmatrix},~\text{target} = \begin{bmatrix} 0 &amp; 1 \newline 1 &amp; 0 \end{bmatrix} \newline \text{output} =&amp; \sigma \left( input \odot W1 \right) \odot W2 \newline \text{diff} =&amp; \text{target} - \text{output} \end{aligned} <br /> ]</p><p>위의 코드를 돌렸을 때의 weight parameter를 직접 확인해보면,</p><p>[ W_1 = \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \newline -0.25 &amp; -0.25 &amp; -0.25 &amp; -0.25 \newline -0.25 &amp; -0.25 &amp; -0.25 &amp; -0.25 \end{bmatrix},~W_2 = \begin{bmatrix} -0.5 &amp; -0.5 \newline -0.5 &amp; -0.5 \newline -0.5 &amp; -0.5 \newline -0.5 &amp; -0.5 \end{bmatrix} ]</p><p>위와 같이 각 row의 모든 값이 동일하게 update되는 것을 알 수 있다. 사실상 weight parameter의 개수가 의미하는 것이 neural network에서 가용할 수 있는 node의 갯수가 되는데, 이렇게 update가 되면 굳이 큰 parameter 개수를 가지는 weight를 사용할 필요성이 없어지는 것이다. 다르게 말하면, parameter의 수가 많아도 <u>representation power가 떨어지게</u> 된다. 사실 이러한 문제는 모든 parameter를 $0$으로 초기화하는 상황 뿐만 아니라 같은 값으로 초기화할 때 발생하는 문제다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># sigmoid as an activation function
</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">value1</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">value2</span> <span class="o">=</span> <span class="mf">0.09</span>

<span class="n">W1</span> <span class="o">=</span> <span class="n">value1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">value2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># feed forward
</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

<span class="c1"># calculate difference
</span><span class="n">diff</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">out</span>

<span class="c1"># backpropagation
</span><span class="n">W2</span> <span class="o">-=</span> <span class="n">hidden</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">-=</span> <span class="nb">input</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">hidden</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">hidden</span><span class="p">)))</span>

<span class="c1"># print results
</span><span class="k">print</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
</code></pre></div></div><p>W1, W2 각각을 동일한 값으로 초기화한 후 학습하였다. 결과는 다음과 같다.</p><p>[ W_1 = \begin{bmatrix} 0.51 &amp; 0.51 &amp; 0.51 &amp; 0.51 \newline -0.24 &amp; -0.24 &amp; -0.24 &amp; -0.24 \newline -0.24 &amp; -0.24 &amp; -0.24 &amp; -0.24 \end{bmatrix},~W_2 = \begin{bmatrix} -0.23 &amp; -0.23 \newline -0.23 &amp; -0.23 \newline -0.23 &amp; -0.23 \newline -0.23 &amp; -0.23 \end{bmatrix} ]</p><h2 id="small-random-numbers">Small random numbers</h2><p>Overfitting 및 위에서 언급한 문제를 해결하기 위해서는 weight를 <u>같은 값으로 초기화</u>하는 방법을 사용할 수 없다. 다음으로 생각해볼 수 있는 상황은 모두 random하게 생성하는 것이다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="mf">0.01</span><span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
</code></pre></div></div><p>여기서 $H$는 hidden layer의 output dimension을 의미한다. <code class="language-plaintext highlighter-rouge">randn</code>은 $D \times H$의 정규 분포 랜덤 난수를 생성하므로, 코드에 따라 $0.01$의 표준편차를 가지는 가우시안 분포를 따르는 weight matrix $W \in \mathbb{R}^{D \times H}$가 생성된다.</p><h2 id="calibrating-the-variances-with-frac1sqrtn">Calibrating the variances with $\frac{1}{\sqrt{N}}$</h2><p>위의 제시된 방법은 output의 분포가 가지는 variance가 input 개수에 따라 증가할 수 있다는 것이다. 위에서와 같이 임의의 상수 <code class="language-plaintext highlighter-rouge">0.01</code>의 표준편차를 가지는 가우시안 분포를 생성하는 것이 아닌, 각 neuron의 output 분포를 $1$의 variance(혹은 표준편차)를 갖게끔 해준다. 특정 layer의 input 개수가 $n$이라면 해당 layer의 연산을 담당하는 neuron의 weight을</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</code></pre></div></div><p>로 초기화하게 되면, 네트워크의 모든 neuron들이 같은 output distribution을 갖게되고, 이러한 방법이 convergence 속도를 증가시킬 수 있다.</p><p>[ \begin{aligned} \text{Var}(s) =&amp; \text{Var} \left( \sum_i^n w_i x_i \right) \newline =&amp; \sum_i^n \text{Var} (w_i x_i) \newline =&amp; \sum_i^n \left( E(w_i) \right)^2 \text{Var} (x_i) + \left( E(x_i) \right)^2 \text{Var}(w_i) + \text{Var} (x_i) \text{Var}(w_i) \newline =&amp; \sum_i^n \text{Var} (x_i) \text{Var}(w_i) \newline =&amp; (n \text{Var}(w)) \text{Var} (x) \end{aligned} <br /> ] 해당 concept에 대한 수식 증명은 위와 같다.</p><h2 id="sparse-initialization">Sparse initialization</h2><p>위에서 언급했던 calibration 관련 문제(neuron의 output마다 분포가 달라지는 현상)를 해결하기 위해서는 모든 weight matrix를 0으로 초기화를 해야하지만, 앞서 말했던 것처럼 이대로 학습을 진행하면 모든 weight가 동일하게 학습되는 문제가 발생하기 때문에 고정된 갯수의 neuron을 정해두고 랜덤한 gaussian noise(이 때는 calibration이 진행되지 않은, 앞서 봤던 small random number에 해당된다)에서 샘플링한 weight만 연산하게 된다. 즉, 연산량을 고정시켜서 output distribution이 항상 동일하게 유지되게 하는 것이다.</p><h2 id="initializing-bias">Initializing bias</h2><p>Bias는 모든 값을 $0$으로 초기화해도 괜찮다. 이는 만약 weight의 assymetry가 보장되면 bias가 동일한 값을 가지더라도 node에 따른 assymetry가 유지되기 때문이다. ReLU 특성상 $0$보다 작은 값에 대해서 gradient를 주지 않기 때문에 해당 non-linearity를 가진 neuron에 대해서는 $0.01$과 같이 작은 양의 값으로 초기화하는 걸 선호하는 case도 있지만, 이러한 과정이 실질적으로 performance에 긍정적인 영향을 준다는 근거는 없으며 더 안좋은 결과를 보여주기도 한다. 따라서 bias는 $0$으로 초기화하는 것이 가장 일반적이다.</p><h2 id="he-initialization">He initialization</h2><p>He et al. 저자들이 밝힌 바로는(<a href="https://arxiv.org/abs/1502.01852">참고링크</a>) ReLU를 사용하는 network에 기반하여 다음과 같은 초기화 과정이 더 낫다고 하였다. 논문에 weight initialization 말고도 실험한 부분들이 유의미한 내용을 담고 있기 때문에 한번쯤 보는 것을 추천한다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
</code></pre></div></div><hr /><h1 id="batch-normalization">Batch normalization</h1><p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>(이른바 ResNet)를 시작으로 convolutional neural network 구조에 <u>batch normalization</u>을 추가하는 것이 trend가 되었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213616285-70c21d20-a4e9-4cc9-a291-87c8b041e133.png" width="800" /></p><p><a href="https://arxiv.org/abs/1502.03167">Batch normalization</a>은 말 그대로 <u>'batch' 단위로 정규화를 진행</u>하는 작업이며, batch normalization을 포함하여 layer normalization, instance normalization 그리고 group normalization이 있다. 각각 그림을 보게 되면 정규화를 진행하는 단위가 다른 것을 알 수 있는데, 예를 들어 layer normalization의 경우에는 단일 batch 내에서 채널 전체의 평균 및 표준편차를 통해 정규화를 진행하게 되고, Instance normalization은 단일 batch, 그리고 channel에 대해서 normalization을 진행한다. 마지막으로 group normalization은 단일 배치 내에서 채널을 묶어서 하나의 그룹을 생성하고, 이 그룹 내에서 normalization을 진행한다. 이 중에서 지금 소개할 것은 가장 흔하게 사용되는 batch normalization이다. <br /> 보통 gradient descent 알고리즘에서 단일 샘플을 사용하지 않고 mini-batch(혹은 batch라고도 부른다)를 사용하여 parameter update를 진행한다. 이유는 앞서 작성한 글에서도 확인할 수 있지만 다시 한번 언급하자면 단일 샘플로 업데이트하게 될 경우 noisy한 학습이 진행되고, batch 단위로 진행할 때와는 다르게 병렬 연산이 불가능하기 때문에 비효율적이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/211718291-66494f31-f22a-4221-aaf8-b751c03ff011.png" width="700" /></p><p>만약 여러 레이어를 가진 neural network에 batch 단위로 샘플을 통과시키게 되는 상황을 생각해보자. Batch는 랜덤하게 dataset으로부터 추출하기 때문에 각 batch 마다의 데이터 분포는 차이가 있으며, 마찬가지로 layer를 통과하면서 생기는 feature map의 분포 또한 균등하지 않다는 문제가 발생한다. 이를 ‘<u>internal covariance shift</u>‘라고 부른다. 사실 이 부분이 중요한 이유는 앞서 설명해왔던 weight initialization 과정과도 직결되기 때문인데, 만약 학습되는 batch 단위와 중간 layer에서의 feature map 분포 및 형태가 상이하다면 최적화 과정에서 <u>initialization metric</u>을 확정적으로 사용할 수 없게 된다. 따라서 이러한 batch에 따른 분포 차이를 줄여주기 위해, batch 단위로 정규화를 진행하여 <u>변동성을 줄이고 학습을 안정화</u>시키고자 한 것이 batch normalization의 concept이다. <br /> 각 batch에 대한 mean(평균) 및 variance(분산)은 정의에 따라 다음과 같이 구할 수 있다.</p><p>[ \begin{aligned} \mu_\text{batch} =&amp; \frac{1}{m} \sum_{i=1}^m x_i \newline \sigma^2_\text{batch} =&amp; \frac{1}{m} \left(x_i - \mu_{\text{batch}} \right)^2 \end{aligned} ]</p><p>여기서 $x_i$가 의미하는 바는 $m$ 만큼의 batch가 하나의 데이터 묶음을 차지할 때, batch 내에서의 $i$번째 샘플이다. 이렇게 구한 각 batch mean, variance를 활용하여 batch dataset을 normalize 및 scale and shift를 진행한다.</p><p>[ \hat{x_i} = \frac{x_i - \mu_\text{batch}}{\sqrt{\sigma_\text{batch}^2 + \epsilon}} <br /> ]</p><p>하지만 단순히 위와 같은 방법으로 정규화를 진행한다면, non-linearity를 가지는 neural network로 하여금 linear regime에 머무르게끔 강제하는 것이 될 수 있고(representation power가 줄어드는 것을 의미), 각 레이어에서 activation function 이전의 layer의 output에 대해 적용되는 batch normalization layer이 모두 다른 형태의 representation에 대해 적용될 수 있게끔 learnable parameter인 $\gamma$, $\beta$를 두어 scale and shift를 진행한다.</p><p>[ y_i = \gamma \hat{x_i} + \beta ]</p><hr /><h1 id="appendix-for-batch-normalization">Appendix for batch normalization</h1><p>Batch normalization은 사실상 딥러닝 네트워크를 건드려본 사람이라면 필수적으로 알아야 하는 개념이지만, 말만 들어보고 적용만 했을 뿐 생각보다 <u>실제로 구현하는 방식</u>이나 <u>동작하는 원리</u>를 잘 모르고 쓰는 일이 많다. 그렇기 때문에 직접 공식을 기반으로 forward, backward가 어떻게 코드로 동작하는지 구현해보도록 하겠다. 사실 코드랑 내용 전부 다른 페이지 내용을 기반으로 작성하는 것이라 굳이 이 글을 보기 귀찮다면 참고 링크를 통해 직접 내용을 확인해봐도 좋을 것 같다.(<a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">참고링크1</a>, <a href="http://proceedings.mlr.press/v37/ioffe15.html">참고링크2</a>)</p><h2 id="batch-normalization를-사용하는-이유와-그-과정">Batch normalization를 사용하는 이유와 그 과정</h2><p>배치 단위로 정규화를 진행하는 말 그대로 ‘Batch normalization’은 input $$x가 있으면 배치 단위로 정규화를 진행하는 과정이다. 이는 흔히 activation function이 뒤따르는 convolution layer 사이사이 들어가게 되며, covariance shift(feature map 및 batch 단위로 분포가 왔다 갔다 하는 현상)을 방지하여, 보다 큰 learning rate에 대해서도 안정적인 학습을 보장한다던지, 빠른 optimization을 가능하게 하는 등 다양한 장점이 있다. 하지만 Batch normalization은 무분별하게 사용되면 안되는데, 이를테면 GAN based 구조를 가진 모델에서 batch normalization이 encoder나 decoder 말단에 들어가서 <u>학습을 방해하는 경우</u>도 있기 때문이다. 사실 간단하게 설명하자면 batch normalization이 등장한 것은 안정적인 학습을 위함이었지만, 그렇다고 해서 모든 task에 만병통치약과 같은 존재가 될 순 없다는 것이다. 논문에 제시된 batch normalization 과정은 위에서 소개했던 수식 전개 과정과 동일하다.</p><p>[ \begin{aligned} \mu_\text{batch} =&amp; \frac{1}{m} \sum_{i=1}^m x_i \newline \sigma^2_\text{batch} =&amp; \frac{1}{m} \left(x_i - \mu_{\text{batch}} \right)^2 \newline \hat{x_i} =&amp; \frac{x_i - \mu_\text{batch}}{\sqrt{\sigma_\text{batch}^2 + \epsilon}} \newline y_i =&amp; \gamma \hat{x_i} + \beta \end{aligned} ]</p><p>가장 먼저, batch 단위로 평균($\mu$)과 분산($\sigma^2$)을 구해주고, 구한 평균과 분산을 기반으로 샘플을 정규화한다. 그런 뒤 학습 가능한 parameter인 $\gamma$와 $\beta$를 통해 scaling and shifting이 진행된다. 학습 가능한 parameter인 $\gamma$와 $\beta$는 다음과 같이 업데이트된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213619378-f30a8913-43fd-4c29-9278-d4fa04b2f273.png" width="1000" /></p><p>흔히 backpropagation을 계산할 때 위와 같이 diagram을 그린 뒤, chain rule에 맞춰서 local gradient를 구하고 이 값에 backward된 gradient value를 곱하게 되면, output부터 차례대로 input까지 거쳐온 <u>모든 parameter에 대한 gradient</u>를 구할 수 있게 된다.</p><h2 id="forward-propagation">Forward propagation</h2><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

<span class="c1">#step1: calculate mean
</span><span class="n">mu</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#step2: subtract mean vector of every trainings example
</span><span class="n">xmu</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mu</span>

<span class="c1">#step3: following the lower branch - calculation denominator
</span><span class="n">sq</span> <span class="o">=</span> <span class="n">xmu</span> <span class="o">**</span> <span class="mi">2</span>
</code></pre></div></div><p>천천히 3단계 정도씩 끊어서 볼 예정이다. 가장 먼저 샘플에 대한 평균을 구한다. sample <code class="language-plaintext highlighter-rouge">x</code>의 모양은 <code class="language-plaintext highlighter-rouge">batch * dimension</code>이므로 <code class="language-plaintext highlighter-rouge">axis=0</code>을 기준으로 평균을 구해야 batch에 대한 평균을 구할 수 있다. 참고로 <code class="language-plaintext highlighter-rouge">np.sum()</code> 메소드가 아닌 <code class="language-plaintext highlighter-rouge">np.mean()</code> 메소드를 통해 평균값을 바로 구할 수 있다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p>평균을 구했으니 이를 sample에서 빼준다. 여기서 중요한 점은 우리가 <code class="language-plaintext highlighter-rouge">x - mu</code>를 두 가지 계산에 동시에 활용할 것인데, 바로 첫 번째는 아래와 같이 normalize된 $x$를 구할 때의 <code class="language-plaintext highlighter-rouge">x - mu</code> term에 활용할 것이고 동시에 var를 구하기 위한 식에도 적용할 것이다.</p><p>[ \begin{aligned} \hat{x_i} =&amp; \frac{x_i - \mu_\text{batch}}{\sqrt{\sigma_\text{batch}^2 + \epsilon}} \newline \sigma^2_\text{batch} =&amp; \frac{1}{m} \left(x_i - \mu_{\text{batch}} \right)^2 \end{aligned} ]</p><p>위의 두 식에 공통적으로 들어있는 $x_i - \mu_\text{batch}$를 생각해주면 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#step4: calculate variance
</span><span class="n">var</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sq</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#step5: add eps for numerical stability, then sqrt
</span><span class="n">sqrtvar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

<span class="c1">#step6: invert sqrtvar
</span><span class="n">ivar</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">sqrtvar</span>
</code></pre></div></div><p><code class="language-plaintext highlighter-rouge">sq</code>는 위에서 구했던 <code class="language-plaintext highlighter-rouge">x - mu</code>의 제곱이기 때문에 이를 평균낸 것이 분산 공식이다. 위에서 언급했던 것과 마찬가지로 이 식도 <code class="language-plaintext highlighter-rouge">np.mean()</code> 메소드로 대체 가능하다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sq</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p>그리고 step 5에서 <code class="language-plaintext highlighter-rouge">eps</code>를 더하는 형태의 방법은 흔히 컴퓨팅 환경에서 <code class="language-plaintext highlighter-rouge">0</code>으로 division되는 error를 방지하기 위함이다. 나머지 계산을 통해 구하고자 하는 output을 나타내면 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#step7: execute normalization
</span><span class="n">xhat</span> <span class="o">=</span> <span class="n">xmu</span> <span class="o">*</span> <span class="n">ivar</span>

<span class="c1">#step8: Nor the two transformation steps
</span><span class="n">gammax</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span>

<span class="c1">#step9
</span><span class="n">out</span> <span class="o">=</span> <span class="n">gammax</span> <span class="o">+</span> <span class="n">beta</span>
</code></pre></div></div><p><code class="language-plaintext highlighter-rouge">xhat</code>(normalized된 <code class="language-plaintext highlighter-rouge">x</code>), 학습 가능한 parameter인 <code class="language-plaintext highlighter-rouge">gamma</code>, <code class="language-plaintext highlighter-rouge">beta</code>를 통한 scaling과 shifting 과정이다.</p><h2 id="backward-propagation">Backward propagation</h2><p>[ \begin{aligned} \frac{\partial l}{\partial \hat{x_i}} =&amp; \frac{\partial l}{\partial y_i} \cdot \gamma \newline \frac{\partial l}{\partial \sigma_B^2} =&amp; \sum_{i=1}^m \frac{\partial l}{\partial \hat{x_i}} \cdot (x_i - \mu_B) \cdot -\frac{1}{2}(\sigma_B^2 + \epsilon)^{-3/2} \newline \frac{\partial l}{\partial \mu_B} =&amp; \sum_{i=1}^m \frac{\partial l}{\partial \hat{x_i}} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}} \newline \frac{\partial l}{\partial x_i} =&amp; \frac{\partial l}{\partial \hat{x_i}} \cdot \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial l}{\partial \sigma_B^2} \cdot \frac{2(x_i - \mu_B)}{m} + \frac{\partial l}{\partial \mu_B} \cdot \frac{1}{m} \newline \frac{\partial l}{\partial \gamma} =&amp; \sum_{i=1}^m \frac{\partial l}{\partial y_i} \cdot \hat{x_i} \newline \frac{\partial l}{\partial \beta} =&amp; \sum_{i=1}^m \frac{\partial l}{\partial y_i} \end{aligned} ]</p><p>사실 backward propation 과정은 위와 같이 논문에 수식으로 정리되어 있다. 하지만 실제로 이를 코드로 구현하는 작업이 생각보다는 복잡하다. 각각의 식이 유도된 부분을 각 gate마다 천천히 살펴보면 다음과 같다. 우선 가장 말단에 있는 gate부터 backpropagation을 진행하게 되면,</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213634136-116b0839-b86b-4e1f-b3f5-f89fa21661b5.png" width="150" /></p><p>[ \text{out} = \gamma \hat{x} + \beta <br /> ] 위와 같은 연산을 하게 되므로 덧셈 연산 gate의 두 input인 $\gamma \hat{x},~\beta$에 대한 local gradient는 각각 다음과 같이 구할 수 있다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dgammax</span> <span class="o">=</span> <span class="n">dout</span>
<span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p><code class="language-plaintext highlighter-rouge">dbeta</code>를 구하는 과정에서 <code class="language-plaintext highlighter-rouge">np.sum()</code> 메소드가 사용되는 부분이 이해가 잘 가지 않아서 살펴보니, numpy broadcasting은 사실 $\beta$의 크기 그대로를 더하는 것이 아니라, 이 값을 broadcasting하여 <code class="language-plaintext highlighter-rouge">gammax</code>의 dimension에 맞게끔 확장시켜 더하게 된다. 따라서 <code class="language-plaintext highlighter-rouge">dbeta</code>를 구하는 것은 broadcasting을 고려해야하므로 이처럼 표현된 것으로 이해할 수 있었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213634981-7b834205-7d0d-4289-ab0b-b60643a58d7d.png" width="150" /></p><p>그 다음은 곱셈 연산 게이트에 대해서 backpropagation을 진행한다. 이 operation에 대한 backpropagation을 계산하기 전, <code class="language-plaintext highlighter-rouge">dgammax</code>가 이전의 backpropagation value이기 때문에 chain rule에 따라, 해당 value에 local gradient를 곱하여 해당 gate의 각 input에 대해 흘러가는 gradient를 구하게 된다. 사실 <code class="language-plaintext highlighter-rouge">*</code> operation에 대한 gate local gradient는 이전 게시글에서 다룬 것과 같이, gate로 들어오는 backpropagation value에 각자 다른 길로 들어오는 input value를 곱해주면 된다. 이를 테면 위쪽 input은 <code class="language-plaintext highlighter-rouge">xhat</code>이고 아래쪽 input은 <code class="language-plaintext highlighter-rouge">gamma</code>이므로, 위쪽 gate에 대한 backpropagation value는 <code class="language-plaintext highlighter-rouge">dgammax</code>에 <code class="language-plaintext highlighter-rouge">gamma</code>를 곱한 결과가 <code class="language-plaintext highlighter-rouge">dxhat</code>이 되고 반대로 아래쪽 gate에 대한 backpropagation value는 <code class="language-plaintext highlighter-rouge">dgammax</code>에 <code class="language-plaintext highlighter-rouge">xhat</code>을 곱한 결과가 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dxhat</span> <span class="o">=</span> <span class="n">dgammax</span> <span class="o">*</span> <span class="n">gamma</span>
<span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dgammax</span> <span class="o">*</span> <span class="n">xhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p>그리고 앞서 살펴본 바와 같이 <code class="language-plaintext highlighter-rouge">gamma</code> 또한 <code class="language-plaintext highlighter-rouge">beta</code>와 같이 broadcasting되는 값이기 때문에 <code class="language-plaintext highlighter-rouge">np.sum()</code> 메소드를 통해 dimension을 맞춰주게 된다. 이제 학습 가능한 parameter인 <code class="language-plaintext highlighter-rouge">gamma</code>와 <code class="language-plaintext highlighter-rouge">beta</code>에 대한 backpropagation이 끝났고, normalization에서의 backpropagation을 쭉 계산해보면 다음과 같다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213672252-1fa03a1b-e8e6-442b-937a-213a59b452de.png" width="600" /></p><p>가장 오른쪽의 곱셈 연산 gate에 대해서는 앞서 했던 연산과 동일하므로 주어진 코드에서 연산은 다음과 같이 진행된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">divar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dxhat</span><span class="o">*</span><span class="n">xmu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dxmu1</span> <span class="o">=</span> <span class="n">dxhat</span> <span class="o">*</span> <span class="n">ivar</span>
</code></pre></div></div><p>sample variance <code class="language-plaintext highlighter-rouge">divar</code>(inverse variance) 또한 broadcasting 되었으므로 <code class="language-plaintext highlighter-rouge">np.sum</code> 해주고, <code class="language-plaintext highlighter-rouge">dxmu1</code>은 이전의 backpropagation value였던 <code class="language-plaintext highlighter-rouge">dxhat</code>에 <code class="language-plaintext highlighter-rouge">ivar</code>를 곱해준 것과 같다. 여기서 중요한 것은 <code class="language-plaintext highlighter-rouge">dxmu</code>에 대한 계산이 두 개가 필요하다는 것인데, 이는 아래와 같이 <code class="language-plaintext highlighter-rouge">x - mu</code>가 두 갈래로 나뉘어 계산되었기 때문이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213672765-3330f613-8271-4d53-b4ac-c122a53afda7.png" width="150" /></p><p>따라서 이 gate에 대한 backprop을 계산하기 위해서는 분리된 두 input에 대한 gradient를 따로 계산한 뒤에 더해줘야 한다. 위로 가는 <code class="language-plaintext highlighter-rouge">dxmu1</code>은 계산했고 나머지 아래 부분에 대한 공식을 차례로 보면, inverse에 대한 local gradient는 $\frac{d}{dx} \left( \frac{1}{x} \right) = -\frac{1}{x^2}$이기 때문에,</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dsqrtvar</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span><span class="p">(</span><span class="n">sqrtvar</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">divar</span>
</code></pre></div></div><p>코드가 위와 같으며 마찬가지로 square root에 대한 local gradient는 $\frac{d}{dx} \left( x + \epsilon \right) = \frac{1}{2 \sqrt{x + \epsilon}}$이 되기 때문에,</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dvar</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsqrtvar</span>
</code></pre></div></div><p>이처럼 표현 가능하다. $\frac{1}{N} \sum_i x_i$에 대한 부분이 조금 복잡한데, 현재 미분의 대상이 되는 것이 행렬이기 때문에 다음과 같이 정의할 수 있다.</p><p>[ \frac{d}{dx} \left( \frac{1}{N} \sum_i x_i \right) = \frac{1}{N} \begin{pmatrix} 1 &amp; \cdots &amp; 1 \newline \vdots &amp; \ddots &amp; \vdots \newline 1 &amp; \cdots &amp; 1 \end{pmatrix} <br /> ]</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dsq</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span> <span class="o">*</span> <span class="n">dvar</span>
</code></pre></div></div><p>마지막 부분은 $\frac{d}{dx} \left( x^2 \right) = 2x$에서,</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dxmu2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xmu</span> <span class="o">*</span> <span class="n">dsq</span>
</code></pre></div></div><p>이제 드디어 <code class="language-plaintext highlighter-rouge">dxmu2</code>를 구했으므로 뺄셈 연산 gate에 대한 local gradient를 구하게 되면 <code class="language-plaintext highlighter-rouge">mu</code>는 빼주고 <code class="language-plaintext highlighter-rouge">x</code>는 더해주는 process이므로,</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx1</span> <span class="o">=</span> <span class="p">(</span><span class="n">dxmu1</span> <span class="o">+</span> <span class="n">dxmu2</span><span class="p">)</span>
<span class="n">dmu</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dxmu1</span> <span class="o">+</span> <span class="n">dxmu2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div><p>위에서 보는 바와 같이 <code class="language-plaintext highlighter-rouge">dx1</code>은 positive(+), <code class="language-plaintext highlighter-rouge">dmu</code>는 negative(-) 방향이 된다. <code class="language-plaintext highlighter-rouge">dx</code> 또한 <code class="language-plaintext highlighter-rouge">dxmu</code> 계산과 동일하게 <code class="language-plaintext highlighter-rouge">dx2</code> 연산이 추가로 필요한데, 이는 앞서 구했던 matrix의 미분 공식과 같은 공식이 적용된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span> <span class="o">*</span> <span class="n">dmu</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">dx1</span> <span class="o">+</span> <span class="n">dx2</span>
</code></pre></div></div><p>이를 모두 합친 과정이 다음과 같은 backpropagation 코드가 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#unfold the variables stored in cache
</span><span class="n">xhat</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">xmu</span><span class="p">,</span> <span class="n">ivar</span><span class="p">,</span> <span class="n">sqrtvar</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="n">cache</span>

<span class="c1">#get the dimensions of the input/output
</span><span class="n">N</span><span class="p">,</span><span class="n">D</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="n">shape</span>

<span class="c1">#step9
</span><span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dgammax</span> <span class="o">=</span> <span class="n">dout</span> <span class="c1">#not necessary, but more understandable
</span>
<span class="c1">#step8
</span><span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dgammax</span><span class="o">*</span><span class="n">xhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dxhat</span> <span class="o">=</span> <span class="n">dgammax</span> <span class="o">*</span> <span class="n">gamma</span>

<span class="c1">#step7
</span><span class="n">divar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dxhat</span><span class="o">*</span><span class="n">xmu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dxmu1</span> <span class="o">=</span> <span class="n">dxhat</span> <span class="o">*</span> <span class="n">ivar</span>

<span class="c1">#step6
</span><span class="n">dsqrtvar</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span><span class="p">(</span><span class="n">sqrtvar</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">divar</span>

<span class="c1">#step5
</span><span class="n">dvar</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsqrtvar</span>

<span class="c1">#step4
</span><span class="n">dsq</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span> <span class="o">*</span> <span class="n">dvar</span>

<span class="c1">#step3
</span><span class="n">dxmu2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xmu</span> <span class="o">*</span> <span class="n">dsq</span>

<span class="c1">#step2
</span><span class="n">dx1</span> <span class="o">=</span> <span class="p">(</span><span class="n">dxmu1</span> <span class="o">+</span> <span class="n">dxmu2</span><span class="p">)</span>
<span class="n">dmu</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dxmu1</span><span class="o">+</span><span class="n">dxmu2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#step1
</span><span class="n">dx2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span> <span class="o">*</span> <span class="n">dmu</span>

<span class="c1">#step0
</span><span class="n">dx</span> <span class="o">=</span> <span class="n">dx1</span> <span class="o">+</span> <span class="n">dx2</span>
</code></pre></div></div></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/cs231n05"><p id="prev_title"> ❮❮ cs231n 내용 요약 (5) - Neural Network</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/cs231n07"><p id="next_title"> ❯❯ cs231n 내용 요약 (7) - Regularization, Loss function</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div id="gitcus_container" class="gitcus"></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script> <script> document.addEventListener('DOMContentLoaded', function() { const currentTheme = localStorage.theme === 'dark' ? 'dark' : 'light'; const script = document.createElement('script'); script.src = 'https://giscus.app/client.js'; script.setAttribute('data-repo', '6unoyunr/comments'); script.setAttribute('data-repo-id', 'R_kgDOODmwzQ'); script.setAttribute('data-category', 'General'); script.setAttribute('data-category-id', 'DIC_kwDOODmwzc4Cn5wD'); script.setAttribute('data-mapping', 'pathname'); script.setAttribute('data-strict', '0'); script.setAttribute('data-reactions-enabled', '1'); script.setAttribute('data-emit-metadata', '0'); script.setAttribute('data-input-position', 'bottom'); script.setAttribute('data-theme', currentTheme); script.setAttribute('data-lang', 'ko'); script.async = true; script.crossOrigin = 'anonymous'; document.getElementById('gitcus_container').appendChild(script); }); </script>
