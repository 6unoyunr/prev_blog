<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"><meta name="og:description" content="prompt learning"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/219937038-7277341b-efd1-4391-8ab3-12fa5cbc9f01.gif"><meta name="og:image:alt" content="Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"><meta name="og:url" content="http://localhost:4000/blog/CoOp"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"><meta name="twitter:description" content="prompt learning"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/219937038-7277341b-efd1-4391-8ab3-12fa5cbc9f01.gif"> <!-- Search Engine --><meta name="description" content="prompt learning"><meta name="image" content="https://user-images.githubusercontent.com/79881119/219937038-7277341b-efd1-4391-8ab3-12fa5cbc9f01.gif"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"><meta name="author" content="JY"/><meta itemprop="description" content="prompt learning"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/219937038-7277341b-efd1-4391-8ab3-12fa5cbc9f01.gif"><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBDTZMG942"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GBDTZMG942'); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep_learning1" class="navbar-item has-text-grey-light "> DEEP LEARNING THEORY </a> <a href="http://localhost:4000/category/deep_learning2" class="navbar-item has-text-grey-light "> PAPERS & TECHS </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github_blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/CoOp" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/219937038-7277341b-efd1-4391-8ab3-12fa5cbc9f01.gif" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">prompt learning</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>February 19, 2023</b> by <a href="https://6unoyunr.github.io/mycard" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">VLP</span> <span class="tag is-link">Prompt learning</span> <span class="tag is-link">Downstream tasks</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 19 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="들어가며">들어가며…</h1><p>대용량의 <u>pre-trained VL model</u>(CLIP, ALIGN 등등)은 여러 downstream task에서 <u>효과적으로 representation을 transfer</u>할 수 있음을 보여주었다. 특히 zero-shot이나 linear probing에서 보여준 성능은 vision과 language를 함께 활용하는 것이 보다 open world set에 대해 적합한 학습 구조라는 것을 증명하였다. 대부분 one-hot encoding과 같은 <u>discretized label</u>(class를 label로 mapping하는 형태)를 사용하는 것이 traditional representation learning의 트렌드였다면, VLP(Vision language pre-training) task에서는 image와 text를 같은 feature space에 어떻게 하면 유의미한 관계를 가지게끔 위치시킬 수 있을지 연구하게 되었다. 이러한 연구를 토대로 각 downstream task를 해결하기 위한 <u>prompting</u>을 적용하였으며, 여기서 말하는 prompting이란 일종의 ‘<u>텍스트를 구성하는 방법론</u>’이라는 의미가 된다. 이번 포스팅에서 다룰 주제가 prompt learning인 만큼 prompt에 대한 개념이 계속 사용되기 때문에 대략적인 개념만 짚고 넘어가도록 하자.</p><p>자세한 내용은 뒤에서 더 디테일하게 설명할 것이지만 이 논문에서 저자들은 <u>prompt engineering</u>이 challenging하다는 점을 문제로 삼는다. 보통의 딥러닝에서 학습 성능을 높이기 위한 방법으로 hyperparameter를 조정하는 과정을 거치곤 하는데, 이처럼 VLP task에서 prompt를 어떠한 방식으로 만들어내는지에 따라 task별 성능의 등락폭이 너무나도 커져서 무시할 수 없는 전처리 과정으로 간주되었다. 하지만 단순히 단일 task의 성능을 높이기 위해 사실상 <u>무한에 가까운 경우의 수</u>를 가지는 prompt를 모두 테스트하는 것도 굉장히 시간이 오래 걸리는데, 이들 중 가장 좋은 성능을 보이는 prompt를 기준으로 학습을 진행했더라도 다른 task에서는 <u>또다시 prompt engineering 과정을 처음부터 시작</u>해야하는 것이다. 그리고 특정 언어에 대한 representation을 학습하기 위해 prompt를 만드는 과정이 <u>domain expertise</u>를 요구한다는 점이 문제가 된다.</p><p>따라서 이 논문에서는 NLP task에서 제시한 prompt learning 방법론을 사용하여, prompt 자체를 최적화하는 <u>Context Optimization</u>(CoOp)을 제시하였고, CoOp은 prompt의 context word를 학습 가능한 벡터로 간주하여 VL task의 model parameter는 frozen 시킨 채로 각 downstream task에 최적의 prompt를 찾기 위한 학습 과정을 거친다. CoOp을 통해 더 적은 shot(학습 샘플)으로도 hand-crafted prompt의 성능을 따라잡을 수 있었으며 학습이 충분히 진행된 후에는 downstream task의 성능이 대략 $15\%$ 상승한 것으로 나타났다.</p><hr /><h1 id="limitations-of-existing-methods-and-apperance-of-vlp">Limitations of existing methods and apperance of VLP</h1><p>CLIP 논문 리뷰에서도 언급했지만 CLIP이 문제시했던 내용은 충분히 많은 representation을 학습할 수 있는 <u>Web 기반 이미지 데이터셋</u>에 대한 부분이었고, 구체적으로는 <u>왜 기존 image modality에 대한 학습법</u>이 좋은 representation learning 방법이 아니었는지 언급하지 않고 넘어왔었다. ResNet과 Vision Transformer와 같은 연구들에서 확인할 수 있듯이 classification task의 경우 정해진 갯수의 object category가 있고, 각 카테고리에 대한 description은 indexing을 거쳐 discrete label로 간주하게 된다. 예를 들어 <u>CIFAR10</u>과 같은 경우에는,</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934924-6bf23756-4bec-4c2d-ab25-90539e24cdf9.png" width="600" /></p><p>위와 같이 총 $10$개의 클래스를 가진 데이터셋을 사용하는데, 각각의 카테고리 정보(airplane, cat, frog 등등)은 학습 과정에서 단어의 뜻으로 supervision을 주는 것이 아닌 one-hot encoding 형태로 cross-entropy loss를 최적화하는데 사용되었다. ImageNet dataset은 약 1000개의 클래스로 구분되는데, 다음과 같은 클래스들이 포함된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">281</span><span class="p">:</span> <span class="s">'tabby, tabby cat'</span><span class="p">,</span>
<span class="mi">282</span><span class="p">:</span> <span class="s">'tiger cat'</span><span class="p">,</span>
<span class="mi">283</span><span class="p">:</span> <span class="s">'Persian cat'</span><span class="p">,</span>
<span class="mi">284</span><span class="p">:</span> <span class="s">'Siamese cat, Siamese'</span><span class="p">,</span>
<span class="mi">285</span><span class="p">:</span> <span class="s">'Egyptian cat'</span><span class="p">,</span>
</code></pre></div></div><p>이는 완전히 무관한 카테고리인 ‘orange’라던지 ‘mushroom’과의 언어적/맥락적 유사도를 전혀 고려하지 않고 <u>단순히 서로 다른 카테고리일 경우에는 다른 인덱스를 부여</u>하는 형태의 supervision이 될 수 밖에 없다. 이러한 <u>discretized label</u>이 가지는 문제는 다음과 같이 묘사할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934884-25d0d7bd-acf1-4b77-a5e4-eb580320e411.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/219934885-578f5f29-c458-4aa6-94bd-88a2eaeff2e3.png" width="480" /></p><p>만약 <strong>siamese cat</strong>과 <strong>tiger cat</strong>이 있다면 분명 <u>두 사진은 다른 카테고리</u>이지만, 어쨌든 카테고리의 이름에서 볼 수 있듯이 ‘고양이’라는 공통점을 가지고 있고, 무엇보다 두 이미지에서 확인할 수 있는 object의 attribute(동물, 털이 있음, 수염이 있음 등등)가 유사하다고 판단할 수 있다. 그와는 반대로 아래에서 볼 수 있는 <strong>사과 이미지</strong>는 위의 두 사진과 사실상 겹치는 attribute가 없다고 볼 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934886-e477a8f9-9163-48af-afa8-17812c16b808.png" width="400" /></p><p>하지만 기존 classification task에서 사용하는 카테고리별 label은 <u>text가 가지는 연관성</u>을 이미지 특성과 전혀 연관짓지 못한다는 것이다. 그렇기 때문에 실제로 open world set에 대해 적용할 수 있는 다양한 object를 판별하기 위해서는 continous signal인 image를 discrete signal인 one-hot encoding으로 단순하게 mapping을 하는 것이 옳지 않다. 또한 카테고리를 묘사하는 text가 더 <u>자세하면 자세할수록</u> network가 분류해야할 <u>category의 개수는 증가</u>하게 되고 그렇게 되면 정답인 label 이외의 <u>다른 object들에 대한 노드가 방해 요소</u>로 작용할 수 밖에 없다. 기존 computer vision 관련 딥러닝에서 다루던 방법은 visual recognition system을 closed set으로 간주한다는 점이 한계가 되었고, 데이터가 추가되면 기존에 학습했던 데이터에 추가하여 <u>새로운 classifier를 학습해야하는</u> 문제점이 생기게 되었다(보다 관심이 생긴다면 continual learning, domain generalization 관련 논문을 찾아보면 좋다).</p><p>따라서 비교적 최근 pre-training을 discretized label이 아닌 text embedding에 대해 진행하는 VLP model인 CLIP과 ALIGN이 등장하기 시작했고, vision representation learning에서 기본 방식에 비해 zero-shot transfer 성능을 눈에 띄게 높일 수 있었다. 학습 아이디어는 간단한데, 바로 image encoder과 text encoder의 output을 align하는 방식을 사용한 것이다. CLIP과 ALIGN 논문에서는 모두 <u>contrastive loss</u>를 objective function으로 사용했는데, 만약 image를 묘사하는 text랑 서로 <strong>positive pair</strong>(image-text pair를 학습 데이터로 사용했다)라면 <strong>embedding space</strong> 상에서 <u>가까워지도록</u>, <strong>negative pair</strong>라면 <u>멀어지도록</u> 학습하게 된다. 이와 같은 방법으로 CLIP과 ALIGN은 대용량의 데이터셋에 대해 pre-trained된 네트워크를 사용하여 <u>다양한 task</u>에 맞는 <u>prompting</u>을 거쳐 knowledge transfer을 진행하였다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934887-21a4e1bc-569d-4078-870f-45422742032a.png" width="700" /> <img src="https://user-images.githubusercontent.com/79881119/219934888-e0ad3742-7bac-42f8-ad7f-c722a75f515c.png" width="600" /></p><p><strong>Task specific prompting</strong>과정은 위의 그림들 중 좌측(<u>CLIP framework</u>)에서 확인할 수 있는데, classification을 진행할 class의 object name을 사용하여 “A photo of (Class)” 등의 형태(task마다 prompt 형식이 달라짐)로 <u>formatting을 진행</u>한 뒤, text encoder를 거쳐 나온 <strong>텍스트 임베딩</strong>을 <u>classifier weight으로 사용</u>하게 된다.</p><hr /><h1 id="limitations-of-prompt-engineering-in-vlp">Limitations of prompt engineering in VLP</h1><p>사전 학습된 network를 사용하는 VL task에서 성능에 주된 영향을 보여준 것은 적절한 prompt engineering이었다. 그러나 적절한 prompt를 정의하는 것은 쉽지 않았기 때문에 이를 tuning하는 과정이 굉장히 오래 걸리는 작업이었고, 약간의 prompt 변화로도 <u>성능이 크게 좌우</u>되는 task도 존재했다. 예를 들어 Caltech101 dataset과 같은 경우, “a photo of (class)” 대신 “a photo of a (class)”를 썼을 때 무려 성능이 $5\%$ 증가할 정도로 <u>변동이 큰 것</u>을 확인할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934889-04937a61-abee-4e74-b265-93c782e1ce81.png" width="600" /></p><p>게다가 prompt engineering은 task에 대한 prior knowledge도 있어야하며 language model이 작동하는 방식에 대한 domain knowledge가 필요하다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934891-8f949757-edd8-4e9c-8726-41c50737d8bf.png" width="1200" /></p><p>‘texture’라던지, “centered satellite photo”와 같은 specific한 description은 해당 dataset이나 분야에 대해 제대로 알고 있지 않다면 <u>prompt engineering에 적용하기 힘들기 때문</u>이다. 그리고 가장 큰 문제는 앞서 말했던 것처럼 이와 같은 prompt engineering이 실질적으로 <u>optimal한지 확신할 수 없기 때문</u>에 일정 수준의 성능 향상으로만 만족할 수 밖에 없는 상황이다.</p><hr /><h1 id="prompt-engineering-in-vlp-task">Prompt engineering in VLP task</h1><p>위에서 볼 수 있는 여러 문제점들을 해결하기 위해 NLP task에서 비교적 최근 등장한 <u>prompt learning research</u>의 아이디어를 기반으로 CoOp에서는 <u>downstream task에 최적화</u>된 prompt를 찾고자 하였다. 기존의 prompt를 찾는 방식은 최적의 hyperparameter를 찾기 위해 tuning 및 검증하는 단계와 같았지만 CoOp 논문에서 제시하는 방법은 적절한 prompt를 찾는 과정을 자동화하는 것이다. 즉 prompt의 context words를 학습 가능한 vector로 취급한다는 것이다. 다양한 task를 다루기 위해 논문에서는 <u>두 가지 방식을 제시</u>하였다. 첫번째는 <u>unified context</u>로 모든 class에 대해 동일한 prompt를 학습시키는 방법이고 두번째는 <u>각 class마다의 prompt를 학습</u>시키는 방법이다. 대부분 첫번째 방법이 좋은 성능을 보였으나, 몇몇 fine-grained category를 가진 task에서는 두번째 방법이 더 효과적이었다고 한다.</p><hr /><h1 id="contributions">Contributions</h1><p>논문에서 선택한 방법인 <u>prompt learning</u>은 기존의 prompt engineering에서 사용한 방식처럼 text prompt를 discretized explanation(category)로 생각하지 않고 continous signal로 취급한 것과 같다. CoOp의 <u>prompt optimization의 효과</u>를 확인해보기 위해 $11$개의 벤치마크 데이터셋을 사용, 여러 object나 scene 그리고 action 등등 다양한 형태의 category로 분류된 task에서 evaluation을 진행하였다. 결과적으로 논문에서 제시한 본인들의 contribution은 다음과 같다.</p><ul><li>직접 VLP 네트워크에 대해 다루지는 않고, 이를 활용한 downstream application을 연구한 시기적절한 연구라고 생각한다. 그리고 기존의 VLP 방식에서의 비효율적인 문제점을 제시하였다(prompt engineering).</li><li>사전 학습된 VL model을 통해 prompt engineering이 자동화될 수 있게 하기 위해, continous prompt learning 방식을 기반으로 접근하였고 unified/class specific 방식 두 가지를 제시함으로써 보다 다양한 recognition task에 적용될 수 있는 방법을 제시하였다.</li><li>Hand crafted prompt 방식과 linear probe model 방식을 downstream task에 적용했을 때 VLP 네트워크의 representation을 transfer learning이 진행되는 것보다 더 효율적으로 최적화가 가능하며 성능 측면에서도 기존 방법들을 뛰어넘었다. 그리고 VL model에 있어서 domain shift에 보다 robust하다는 특징이 있다.</li></ul><hr /><h1 id="related-works">Related works</h1><h3 id="vision-language-models">Vision Language models</h3><p>결국 <u>prompt learning</u>을 적용한 것은 VLP task의 <u>pre-trained representation</u>을 <u>효과적으로 transfer</u>해서 사용하고 싶기 때문인데, 비교적 최근 연구인 <strong>CLIP</strong>과 <strong>ALIGN</strong>과 같이 text와 image encoder의 결과를 <u>contrastive하게 학습한 형태</u>가 image/text multimodal의 기본 아키텍쳐로 사용된다. 두 연구 모두 web 기반의 대용량 데이터셋을 사용했다는 점과 large minibatch를 기반으로 contrastive learning을 진행했다는 점을 공통점으로 삼을 수 있다. 물론 CLIP과 ALIGN 이전에도 text와 image를 같은 embedding space에 올리고자 했던 연구는 있었지만, text embedding을 추출하는 방식(Word2Vec, TF-IDF 등등)이나 matching하는 방식(metric learning, multi-label classification, n-gram language learning 등등)이 현재 SOTA인 contrastive representation learning based와는 차이가 있다.</p><p>그러나 이 논문에서 밝히는 바는 본인들의 연구는 이러한 기존의 vision-language model의 연구와 방향성이 다르다고 한다. 이는 기존의 VLP task는 <u>image와 text를 동일 embedding space 상에 alignment하는 방법</u>에 대해 초점이 맞춰져 있었다면 이 논문에서는 이미 학습된 <u>pre-trained knowledge를 transfer하는 방식</u>에 대해 초점을 맞춘 것이다. 논문에서는 hand-crafted 방식으로 prompt engineering을 하는 과정을 prompt learning으로 바꾸는 것이 효과적일 것이라고 밝혔다.</p><h3 id="what-is-prompt-learning">What is prompt learning?</h3><p><strong>Large pre-trained language model</strong>(LLM)의 knowledge probing을 하는 방식으로는 ‘<u>빈칸 채우기</u>’ 방식의 cloze texts 방법이 제시되었고, 이는 NLP task에서 <u>prompt learning</u> 연구가 진행될 수 있는 발판이 되었다.</p><p><strong>Knowledge probing</strong>이 빈칸 채우기 방식이라고 했는데, 간단하게도 <u>probing의 기본 컨셉</u>은 주어진 cloze-style의 prompt에 대해 <u>정답을 생성하게끔</u> 하는 것이다. ‘<a href="https://arxiv.org/pdf/1911.12543.pdf">How can we know what language models know?</a>’ 논문에서는 text mining을 통해 여러 prompt를 후보군으로 생성하고, training accuracy를 보이는 prompt를 optimal prompt로 사용하는 방식을 제안하였다. <a href="https://arxiv.org/pdf/2010.15980.pdf">AutoPrompt</a>에서 제시한 방법은 아래 그림에서 볼 수 있듯이 label likelihood에 대해 가장 큰 gradient 변화를 주는 token을 searching한 뒤(<u>gradient based search</u>라고 부른다) 이를 prompt generation에 사용하였다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934893-cec8afe9-afe0-49c9-b10b-bd8b0b98e477.png" width="800" /></p><p>이 논문에서는 <u>continous prompt</u> 방식을 사용하게 되는데, 이 방법의 문제점이라면 기존 discrete token에 대해 text embedding space에서 찾는 것보다 학습된 word가 <u>정확히 어떤 prompt를 나타내는지</u> 시각화할 수 없다는 점이다. 그럼에도 불구하고 저자들이 continous prompt 방식을 사용한 것은 VLP task의 주된 목적은 명확한 prompt embedding을 추출하는 것이 아니라, VL model을 <u>downstream task</u>에 사용했을 때 좋은 성능을 보이는 prompt를 <u>tuning하는 과정을 자동화</u>하기 위함이다.</p><hr /><h1 id="method">Method</h1><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934894-0538a799-821f-4650-9ad6-9139a0b6255d.png" width="800" /></p><p>방법론은 굉장히 심플하다. 단순히 기존의 prompt engineering 부분을 학습 하능한 context vector로 설정하고 이를 최적화하는 과정을 사용한다. 물론 VLP 모델 자체를 학습하는 것과는 orthogonal하다는 저자들의 말과 같이 학습 과정은 CLIP baseline framework에서 step (2)에서 (3) 과정과 같다. 각 <u>downstream task</u>에 대해서는 <u>supervision이 있기 때문</u>에 context를 최적화할 수 있는 것이다.</p><h3 id="clip-baseline">CLIP baseline</h3><p>네트워크 구조는 CLIP을 사용하였는데, 구조를 간단히 확인해보자면 <u>vision encoder</u>와 <u>language encoder</u> 각각 있는데 vision encoder는 ResNet50과 같은 <strong>CNN baseline</strong>과 ViT와 같은 <strong>transformer baseline</strong>을 사용하였으며 language encoder로는 <strong>transformer</strong>를 사용하엿다. CLIP의 텍스트 인코딩 방식은 BPE를 사용한다. 학습 과정은 related work에서 간단하게 설명했었지만 다시 한번 설명하면 다음과 같다. Batch 단위의 image-text pair를 가지고 matched pair(contrastive learning에서의 positive pair)에 대해서는 cosine similarity를 최대화하고 unmatched pair(contrastive learning에서의 negative pair)에 대해서는 cosine similarity를 최소화하는 방향으로 학습한다. 다양한 image/text representation에 대해서 학습시키기 위해 CLIP은 web 기반 대용량($400M$)의 paired dataset을 학습에 사용한다.</p><p>CLIP의 주된 <u>contribution</u> 중 하나가 바로 <u>zero-shot inference</u> 성능이 높다는 것인데, CLIP은 웹 기반으로 <u>다양한 text prompt</u>에 대해 학습이 되어있기 때문에 다양한 category를 가지는 classification dataset에 대해 <strong>downstream task</strong>를 수행할 수 있다. 예컨데 $f$가 image encoder에 image $x$를 통과시켜 얻은 feature이며, $(w_i)_{i=1}^K$를 각 class에 대한 description $t_i$를 text encoder에 통과시켜 얻은 weight라고 생각하면 된다. $K$는 downstream task의 클래스 개수를 의미하고 prompt는 ‘a photo of a (class).’와 같이 설정하여, (class)라 명시된 부분에 ‘cat’, ‘dog’와 같이 class 이름이 들어가게 된다. 그런 뒤 prediction은 cosine similarity를 기반으로 softmax probability를 사용하게 된다. cosine similarity를 일종의 score라고 보면 된다(<u>유사도</u>가 높을수록, 해당 class일 <u>확률이 증가</u>하는 구조).</p><p>[ p(y = i \vert x) = \frac{\exp (\cos (w_i, f) / \tau)}{\sum_{j=1}^K \exp(\cos (w_j, f)/\tau)} ]</p><p>$\tau$는 <u>temperature parameter</u>로 CLIP pre-train 과정에서 학습되는 parameter이다. 기존 classifier가 <u>closed-set visual concepts</u>(정해진 class를 구분하는 task로서 학습됨)에 대해서만 discrete label을 학습하는 구조였다면, CLIP은 <u>open-set visual concept</u>를 유기적으로 학습할 수 있다는 것이 <u>high-capacity network</u>를 구성할 수 있는 방법이 되었다.</p><h3 id="context-optimization">Context Optimization</h3><p>그러나 위의 방법에서 볼 수 있듯이 ‘a photo of a (class)’와 같은 prompt는 사람이 직접 각 task에 대해 <u>좋은 성능을 보이는 prompt를 찾는</u> tuning 과정이 필요하다. CoOp 논문에서는 이를 자동화할 수 있는 방법으로 다음과 같이 두 방법들을 제시한다.</p><p><strong>Unified Context</strong></p><p>모든 class에 대해서 같은 context를 공유하는 방식이다. Text prompt $g(\cdot)$에 주어지는 prompt는 다음과 같은 형태로 정의해볼 수 있다.</p><p>[ t = (V)_1(V)_2 \cdots (V)_M (\text{CLASS}) ]</p><p>$(V)_m$으로 표시된 부분이 각각 특정 word의 embedding이라고 가정하면 된다(CLIP에서는 $512$의 dimension을 가진다). $M$은 <u>사용할 word embedding의 갯수</u>가 되며, 이는 hyperparameter로 정해주게 된다. Prompt $t$를 text encoder에 통과하면 각 class에 대한 <u>classification weight vector</u>를 구할 수 있고, <u>prediction probability</u>는 위에서 봤던 식과 동일하게 구할 수 있다.</p><p>[ p(y = i \vert x) = \frac{\exp (\cos (g(t_i), f) / \tau)}{\sum_{j=1}^K \exp(\cos (g(t_j), f)/\tau)} ]</p><p>그런데 사실 <u>최적의 context 구조</u>가 “a photo of (class)” 형태일 수도 있지만, “a photo of (class), a type of object”일 수도 있기 때문에 <u>학습 가능한 prompt</u>를 다음과 같이 지정해줄 수도 있다.</p><p>[ t = (V)_1 \cdots (V)_{\frac{M}{2}}(\text{CLASS})(V)_{\frac{M}{2}+1} \cdots (V)_M ]</p><p>이처럼 prompt는 <strong>latter cell</strong>을 채울 수도 있고, <strong>termination signal</strong>을 통해 더이상 채우지 않을 수도 있게끔 학습할 수 있다.</p><p><strong>Class-specific context</strong></p><p>앞서 언급한 방법은 모든 class에 대해 <u>동일한 context</u>를 학습하게 되고, 다른 방법으로는 각 <u>class 마다의 context를 학습</u>하는 방법이 있다. 서로 다른 class index $i$와 $j$에 대해,</p><p>[ (V)_1^i(V)_2^i \cdots (V)_M^i \neq (V)_1^j(V)_2^j \cdots (V)_M^j ]</p><p>위와 같이 <u>서로 다른 context를 학습</u>하는 것이다. 이 방법은 일반적인 task와는 다르게 <u>fine-grained classification</u>이 필요할 때 효과적이었다고 한다.</p><hr /><h1 id="experiments">Experiments</h1><p>저자들이 실험한 데이터셋은 총 $11$개로, ImageNet, Caltech101, Oxford-Pets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, DTD, EuroSAT 그리고 UCF101을 사용했다고 한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934896-5b60da1e-4aad-4333-aa57-133fc7aaaf76.png" width="800" /></p><p><u>사용된 dataset의 statistics</u>는 위와 같았다. Hand-crafted prompt로 사용한 prompt는 연구에서 ablation을 통해 <u>가장 좋은 성능을 보이는 prompt</u>를 적용했을때다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934897-83e27c9f-e872-4f31-8354-e88cb744e457.png" width="600" /></p><p>$11$개의 dataset에 대해 <u>CoOp 방식을 사용한 평균 결과</u>는 위와 같이 나와있다. ‘end’라고 표시된 것이 CLASS description을 마지막에 넣은 context를 최적화했을 때가 되고 ‘mid’라고 표시된 것이 CLASS description을 중간에 넣은 context를 최적화했을 때가 된다. CSC는 class specific하게 학습했을 경우인데, 대체로 경향성을 보게 되면 <u>unified prompt</u>를 적용했을 때가 성능이 좋은 것을 볼 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934899-7721601c-223b-4336-b857-76a4d4035639.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/219934901-c0484884-3fa8-4620-84e3-ea55dc8b34be.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/219934902-11a25846-78f8-4266-b61d-e9a5ebcd20f8.png" width="400" /></p><p>물론 모든 경우에 <strong>unified prompt</strong>가 좋은 성능을 보이지는 않았고 일부 dataset에 대해서는 sample 수가 증가할수록 <u>오히려 CSC가 더 좋은 성능</u>을 보인 경우도 있었다. 이 논문의 가장 main contribution이라고 생각하면, zero-shot CLIP에 대해 여러 dataset의 fine-tuning(Linear probing) 과정에서4-shot 이전까지는 few-shot 성능이 zero-shot 성능 이상으로 보장되지 않았던 것이 limitation으로 제시되었는데, prompt learning을 통해 <u>few-shot 성능</u>이 얼추 <u>zero-shot 성능 이상으로 올라가는</u> 경향성을 확인할 수 있다는 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934903-f53b86ca-f618-4fef-8be9-829627e7c7fc.png" width="600" /></p><p>위의 그래프를 보게 되면 <u>장점이 더 명확</u>해지는데, Zero-shot CLIP과 비교했을 때 CoOp 방식에 <u>$16$-shot을 적용한 few-shot network</u>가 적게는 $1.24\%$, 많게는 $45.97\%$의 성능 향상을 보이는 것을 알 수 있다. 물론 오히려 성능이 하락한 경우(Food101)도 있지만, $11$개의 dataset 중 $10$개의 dataset에 대해 <u>성능 향상을 보이는 것</u>을 확인할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934905-2383c97b-1547-4d97-a356-93514afbabe7.png" width="700" /></p><p>그리고 <u>CoOp을 통한 학습</u>이 위와 같은 <u>domain shifting 상황</u>에 대해서 보다 robustness를 가지는 것을 확인할 수 있다. 특히 ImageNet-R, ImageNet-Sketch 등의 dataset에 대해서는 단순히 source에 대한 성능 향상에 대한 경향성보다는 <u>더 큰 폭으로 성능이 좋아질 수 있는 것&lt;&gt;을 볼 수 있다.</u></p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934906-b68c65df-b638-40cd-b97f-b6b3a5304b34.png" width="700" /></p><p>그리고 실험 결과를 보면 context length를 $16$만큼 사용했는데, 이에 대한 <strong>ablation</strong>과 <u>vision backbone</u>에 따른 <u>CoOp의 경향성</u> 또한 확인하였다. 기존 zero-shot CLIP이 보였던 성능 추이에 비슷하게 나오는 것을 확인할 수 있으며, backbone 구조에 무관하게 <u>CoOp 방식이 성능 향상에 잘 적용될 수 있다</u>는 점을 보여주었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934908-7e1ff3ff-e917-4602-ba88-00ff341c9812.png" width="800" /></p><p>위는 appendix에 각 dataset에 따라 학습된 context와 가장 유사한 vector를 나타낸 것이다. Continous prompt learning을 사용했기 때문에 학습된 vector가 특정 단어를 나타낼 수는 없지만, 위를 통해 간접적으로나마 학습된 prompt와 유사한 word를 찾을 수 있다. 사실 이걸 보면서 느낀 점은 prompt를 continous하게 학습하게 되면 기존의 text 구조 체계가 무너질 수 밖에 없기 때문에, 성능을 높이는 방법으로는 CoOp이 적절할 수는 있으나 <u>image/text와의 관계성을 보여주기엔 한계쩜이 많다는 것</u>이다. 저자들이 related works를 작성하는 과정에서 본인들의 연구가 <u>VLP와는 동떨어진 연구라고 주장했던 이유</u>가 바로 이 때문이 아닐까 조심스럽게 추측해본다.</p><hr /><h1 id="limitation-in-coop-and-appearance-of-cocoop">Limitation in CoOp and appearance of CoCoOp</h1><p>CoCoOp에는 치명적인 문제가 존재한다. 이는 바로 학습 과정에서 context가 <u>downstream task에 overfitting</u>이 되다보니, in-domain class에 대해서는 좋은 성능을 보이지만 <u>비슷한 distribution</u>을 가지는 <u>out-of domain class</u>에 대해서는 낮은 성능을 보인다는 점이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934909-7b186c22-5b2c-4431-abb6-059bf1d07357.png" width="900" /></p><p>예를 들어 SUN397 dataset에 존재하는 class category인 ‘Arrival gate’나 ‘Cathedral’과 같이 존재하는 class에 대한 accuracy는 zero-shot에 비해 학습된 prompt가 더 좋은 성능을 보이지만,</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934910-961725f8-8edb-43f1-aa47-9649558a6220.png" width="900" /></p><p>‘Wind farm’이나 ‘Trail railway’와 같이 비슷한 distribution(<u>scene understanding</u>이라는 관점에서) hand crafted prompt를 사용하는 zero-shot baseline에 비해 오히려 성능이 나빠지는 것을 확인할 수 있다. 즉 최적화된 text prompt가 특정 dataset을 기준으로 <u>seen class</u>에 대해서만 <u>overfitting</u>되는 문제가 발생한다.</p><p>사실 이러한 문제는 어느 정도 당연히 제시될 수 밖에 없는 것이 앞서 CoOp 논문 experiment 과정에서 마지막에 언급한 Appendix 표에도 잘 드러나 있는데, 학습된 prompt와의 nearest neighbor을 시각화했을 때 합리적인 word나 문장이 전혀 생성되지 않고 사실상 <u>이미지의 문맥과는 거의 동떨어진 description</u>이 나오는 것을 확인할 수 있었다. 애초에 논문에서 <u>유연한 context</u>를 실험해보기 위해 CLASS prompt를 문맥 중간에 넣거나 뒤쪽에 넣는 식으로 variation을 주었지만, 이러한 과정이 text domain에 대한 intuition에 아무런 영향도 끼치지 못했다는 것처럼 보인다.</p><hr /><h1 id="conditional-context-optimization">Conditional Context Optimization</h1><p>논문에서는 이러한 <u>weak-generalization 문제</u>를 해결하기 위해 image captioning과 비슷한 방법을 사용한다. Input image가 ‘<u>학습되는 prompt</u>’에 guidance를 줄 수 있게끔 meta-network를 구성하고, 이를 통해 prompt가 class에 overfitting되는 문제를 정규화하는 것이다. Image captioning에서도 instance에 따른 optimization이 class shift 문제에 대해 보다 robust한 학습이 가능하다고 밝혔다. Prompt가 최적화하는 과정에서 text encoder의 영향만 받다 보니 image representation의 transfer이 약해진다고 판단한 듯하다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934912-418fd3ed-99a5-4f2d-b677-77a68e085ba5.png" width="900" /></p><p>따라서 image encoder(ViT, ResNet)의 class token을 기반으로 meta network를 학습하여, 각 image sample instance에 대해 <u>conditioning된 meta-token</u>을 생성하는 방법을 사용하였다. Context tokens($v_1, v_2, \cdots v_M$)이 오롯이 특정 dataset에 대한 prompt 최적화가 되면 overfitting이 될 수 있기 때문에, context token으로 하게끔 일반화 가능한 정도의 prompt만 학습하고, 나머지 각 image에 대한 정보는 $\pi$가 줄 수 있게 <u>token</u>을 <u>lightweight meta network</u>로 conditioning한다는 것이다. 컨셉을 보고 생각했던 것은 context token을 학습하는 과정은 <u>잘 짜여진 도화지를 만드는 작업</u>이고, $\pi$를 학습하는 과정은 좋은 representation을 <u>그릴 수 있는 팔레트를 구성</u>하는 작업이라는 것이다. 기존의 CoOp이 맨땅에서 그럴듯한 prompt를 만들어내고자 하다보니 overfitting될 수 밖에 없었고, CoCoOp은 conditioning을 통해 text encoder와 image encoder의 <u>역할을 분리</u>함으로써 정규화가 가능해진다고 해석되었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934914-24332975-a9da-4efe-99f1-10a30fa67162.png" width="1100" /></p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934915-aaf4b8b2-224c-480b-b973-263cc98ca375.png" width="1100" /></p><p>두 예시에 대해 각각의 <u>CoOp vs. CoCoOp</u> 결과는 위와 같다. 위의 결과는 class specific prompt에 대한 성능이 CoOp와 CoCoOp이 얼추 비슷하게 나온다는 것을 보여주고, 아래의 결과는 unseen class prompt에 대한 성능이 CoCoOp이 훨씬 좋다는 것을 보여준다. CoOp에서는 zero-shot 성능보다 $10\%$나 떨어지는 경우에 대해서도 성능이 오히려 올라가는 결과를 보여주었다.</p><hr /><h1 id="conditional-context-optimizationcocoop">Conditional Context Optimization(CoCoOp)</h1><p>사실 논문을 보면 알 수 있듯이 <u>related works</u>나 <u>method</u>가 원래 본인들 연구였던 CoOp paper에서 살짝만 바꾼 수준이다. 뭐 본인들 paper을 그대로 reference로 작성하였기 때문에 굳이 tackle을 걸 필요는 없지만 나도 나중에 논문을 쓰면 저렇게 쓰고 싶다는 생각을 해보았다. 사실 원래 연구에 그냥 meta-network만 추가한 것과 같아서 experiment setting도 쉬워보였기 때문이다.</p><p>아무튼 그렇기 때문에 나머지 수식은 모두 동일하고 meta network를 통한 meta token $\pi$에 대한 식만 살펴보면 다음과 같다. 저자들이 가장 단순하게 먼저 생각했던 것은 $M$개의 context token에 대해 <u>따로 학습되는</u> $M$개의 neural networks를 설계하는 것이었다. 그러나 $M\times$의 neural network를 보두 학습하는 것은 CoOp 기준으로 $16$개의 network를 학습하는 과정이 되기 때문에 <u>너무 heavy</u>하고, <u>parameter를 적게 사용할 수 없는 방법</u>이 되기 때문에 $M$개의 context vector에 모두 공통적으로 더해질 수 있는 token을 생성하는 <u>meta network를 구성</u>하였다.</p><p>파라미터 $\theta$를 학습 가능한 파라미터로 가지는 Meta-Net($h_\theta(\cdot)$)에 대해 input image embedding $f = E_I(x)$에 대한 context vector는 다음과 같이 구성할 수 있다. 우선 앞서 CoOp 논문에서 사용했던 수식에 추가로 설명하면,</p><p>[ t = (V)_1(V)_2 \cdots (V)_M (\text{CLASS}) ]</p><p>정해진 갯수의 학습 가능한 prompt vector를 다음과 같이 설계하고, meta token $\pi = h_\theta(f)$를 각 vector에 더한 conditioned vector $V_m(f) = (V)_m + \pi$를 prompt로 대체할 수 있다.</p><p>[ t(f) = (V_1(f),~V_2(f), \cdots ,~V_M(f), (\text{CLASS}) ]</p><p>그렇게 되면 기존의 prediction probability는 다음과 같이 수정된다.</p><p>[ p(y = i \vert x) = \frac{\exp (\cos (g(t_i(f)), f) / \tau)}{\sum_{j=1}^K \exp(\cos (g(t_j(f)), f)/\tau)} ]</p><p>학습 과정에서는 meta network의 <u>parameter $\theta$</u>와 <u>context vector</u> $V_m$이 함께 gradient based로 update된다.</p><p>Meta network는 정말 심플하게 2개의 layer를 가지는 structure로, Linear-ReLU-Linear의 MLP로 구성했다고 한다. 보다 복잡한 구조는 future work로 남기겠다고 했는데, 여기에 더 복잡한 구조를 써서 유의미한 결과를 얻어내는 것 자체는 큰 contribution이 안될 것 같다(<del>논문 주제 하나 또 잃었네</del>).</p><hr /><h1 id="experiments-1">Experiments</h1><p>실험에 사용한 dataset은 기존 CoOp 연구에서의 $11$개의 dataset을 그대로 사용하였다. 실험 결과 확인한 accuracy 평균은 다음과 같다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934917-5e774c08-85e1-45ab-be82-293f41344704.png" width="600" /></p><p>CoCoOp을 사용한 것이 New(unseen class), H(Base + New) 모두 CoOp보다 좋은 결과를 보여주었다. 물론 CLIP이 unseen class에 대한 zero-shot 성능은 제일 좋았으나, base dataset에 대한 성능이 $11\%$ 차이가 난다는 점에서 CoCoOp 방법이 seen class와 unseen class <u>모두에 적용될 수 있는 방법</u>이라는 것을 보여준다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934919-f8061919-1e32-4526-96b4-9ab6a3803c51.png" width="800" /></p><p>각 dataset에 대한 unseen class와 base class 성능 비교는 위와 같다. Base class에 대해서는 CoOp 성능이 더 높은 경우가 많았는데, 이는 <u>overfitting</u> 덕분이라는 분석이 있기 때문에 유의미하지 않았고 <u>unseen class</u>에 대해서는 CoCoOp이 <u>기존 방법을 모두 뛰어넘었다</u>는 점이 주목할만한 점이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934920-76be2e2f-152d-4995-a844-7116db4e7d85.png" width="800" /></p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/219934923-1506df2d-ae29-4475-9cd1-5e9c582481a9.png" width="800" /></p><p>Generalization 성능에 대해서 언급한 만큼 domain generalization에 대한 실험도 빠질 수 없는데, 실제로 CoCoOp이 source domain과 target domain의 차이가 커짐에도 CoOp보다 robust한 성능을 보였다고 한다.</p><hr /><h1 id="결론">결론</h1><p>CoOp 논문, CoCoOp 논문 둘 다 <u>prompt learning을 기반</u>으로 <u>CLIP downstream task</u>의 성능을 높이고자 한 방법론을 다룬다. CoOp에서는 NLP task의 여러 prompt learning 기법 중에서 최적화 과정에 적용할 수 있는 <u>continous prompt learning</u> 방법을 사용, <u>downstream task의 성능을 높이는데 집중</u>했으며 CoCoOp에서는 CoOp에서 성능을 높이면서 놓친 <u>seen class에 대한 overfitting 문제</u>를 다룸으로써 unseen class에 대한 <u>generalization 방법</u>도 meta-network를 통해 제시하였다.</p><p>NLP task에서 적용되던 domain generalization 방법을 VL network에 적용하면서 VLP 연구와 orthogonal하게 독자적인 논문들을 작성했다는 점이 contribution이 될 것 같으며, 사실 CoOp이나 CoCoOp은 성능에서도 충분히 좋은 논문이지만 ‘이렇게 써야 논문을 쓸 수 있겠구나’라는 생각을 하게 된 paper라고 생각된다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/BLIP"><p id="prev_title"> ❮❮ Bootstrapping Language-Image Pre-training(BLIP, BLIP2) 논문 리뷰</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/ffalgorithm"><p id="next_title"> ❯❯ 딥러닝의 체계를 바꾼다! The Forward-Forward Algorithm 논문 리뷰 (1)</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/deep_learning2">PAPERS & TECHS</a></li><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github_blog">GITHUB BLOG</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/first_diary">차별화된 삶</a></li><li> <a href="http://localhost:4000/blog/reasoning">추론 모델은 정말로 뛰어난가?? 오픈소스 엑사원 딥(EXAONE-Deep), QWQ, 딥시크(Deepseek-R1) 비교 실험해보기</a></li><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
