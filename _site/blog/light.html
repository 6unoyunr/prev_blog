<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)"><meta name="og:description" content="Light weight convolutional neural networks"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/235352966-2120920e-4d5f-4f2a-830b-97ebeddca9bc.jpg"><meta name="og:image:alt" content="Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)"><meta name="og:url" content="http://localhost:4000/blog/light"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)"><meta name="twitter:description" content="Light weight convolutional neural networks"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/235352966-2120920e-4d5f-4f2a-830b-97ebeddca9bc.jpg"> <!-- Search Engine --><meta name="description" content="Light weight convolutional neural networks"><meta name="image" content="https://user-images.githubusercontent.com/79881119/235352966-2120920e-4d5f-4f2a-830b-97ebeddca9bc.jpg"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | About light weight CNNs(MobileNet 시리즈들)"><meta name="author" content="JY"/><meta itemprop="description" content="Light weight convolutional neural networks"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/235352966-2120920e-4d5f-4f2a-830b-97ebeddca9bc.jpg"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/light" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">About light weight CNNs(MobileNet 시리즈들)</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/235352966-2120920e-4d5f-4f2a-830b-97ebeddca9bc.jpg" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Light weight convolutional neural networks</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>December 08, 2022</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">AI</span> <span class="tag is-link">deep learning</span> <span class="tag is-link">CNN</span> <span class="tag is-link">light weight</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 7 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><hr /><h1 id="경량화-네트워크">경량화 네트워크</h1><p>Deep learning의 발전은 하드웨어와 함께 시작되었고, 충분히 좋은 성능을 가지는 CPU 혹은 GPU 없이는 뛰어난 추론 능력을 가진 네트워크를 fully 활용할 수 없는 것이 사실이다. <br /> 그러나 언제까지나 무거운 서버컴에서 딥러닝을 돌릴 순 없기 때문에 딥러닝 네트워크의 경량화, 간소화가 필수적이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088788-348d060b-19ba-432c-8b74-6578d47becb7.svg" width="650" /></p><p>따라서 경량화 네트워크의 목적은 두 가지로 나눌 수 있다.</p><ol><li>CNN을 applicable하게 만들고, 이를 모바일 device나 embedding system에 적용이 가능하게끔 한다.</li><li>경량화 과정에서 accuracy나 performance를 적어도 유지, 가능하다면 improve하고 싶다.</li></ol><p>경량화 자체는 어렵진 않다고 생각할 수 있다. 그러나 Deep neural network의 구조를 좀 더 간결하게 구성한다던지 filter kernel의 채널 수를 줄여 가용 파라미터 수를 줄인다는 식의 접근이 있지만, 함부로 구조나 파라미터를 축약하게 되면 학습 가능한 representation이 제한되기 때문에 이마저 그리 간단하지 않다. <br /> 따라서 이러한 네트워크 구조를 제안한 여러 논문들을 살펴보고, 어떤 식으로 파라미터 수를 줄일 수 있었는지 확인해보도록 하자.</p><hr /><h1 id="mobilenet">MobileNet</h1><p>유명한 논문 중 하나인 <a href="https://arxiv.org/abs/1704.04861">MobileNet</a>은 google로부터 제안된 방법이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088800-7ac3c32b-e720-43ce-b42b-efe2992d27d0.png" width="650" /></p><p>바로 depthwise seperable convolution인데, 이는 depth-wise convolution과 point-wise convolution을 서로 연결한 구조가 된다. <br /> 이름에서 알 수 있듯 depthwise convolution과 pointwise convolution은 콘볼루션이 적용되는 영역에 대한 내용인데, 흔히 우리가 처리하고자 하는 image tensor는 각 레이어를 통과하며 하나의 batch 당 $C \times H \times W$로, 각각 채널, 높이, 너비를 가지는 3차원의 데이터로 구성된다. 그리고 이러한 이미지 텐서에 적용되는 필터의 크기는 $C \times D_K \times D_K$로, 각각 채널, 커널의 높이, 커널의 너비를 가지는 3차원의 weight로 구성된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088798-c6daff25-46dc-4b2b-b080-875bc57e585c.png" /></p><p>따라서 위의 그림을 참고하게 되면(실제 mobilenet 논문에 첨부된 사진을 인용하였다), $D_K$로 표시된 dimension이 커널의 높이와 너비에 대해 $M$은 필터가 적용될 데이터의 채널 수가 되고, $N$은 필터의 개수가 된다. notation에 일부 혼란이 있을 수 있어 미리 말하자면 그림에서의 $M$이 내가 언급한 $C$와 같은 값이라고 생각해주면 된다. <br /> 연산 후에는 데이터가 $N \times H’ \times W’$이 된다. <br /> 이런 식으로 convolution이 진행되는데, 각 커널 필터의 파라미터 수를 확인해보면, [ {D_K}^2 \times M \times N ] 이 되고, 이러한 커널이 각 레이어의 convolution마다 존재하기 때문에 parameter 수가 dimension value $D_K$에 따라 급격히 증가하는 구조가 된다. 심지어 segmentation과 같이 high level computer vision에서는 이러한 부분이 네트워크 경량화에 큰 bottleneck으로 작용한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088807-6ae8a0c4-9464-4da2-ba85-6a4aeef33b69.png" width="450" /> <img src="https://user-images.githubusercontent.com/79881119/209088809-677c3e21-6da7-4a64-b450-4a9f67223ab3.png" width="450" /></p><p>따라서 이를 위와 같이 두 개로 분리하자는 것이 mobilenet에서 제시한 방법이다. 좌측에 보이는 $D_K \times D_K \times 1$ 크기의 커널을 $M$개 만듦으로써 <u>각 데이터의 채널 별로 연산</u>을 진행하고자 하는 것이 depthwise convolution이고, 우측에 보이는 $1 \times 1 \times M$ 크기의 커널을 $N$개 만듦으로써 <u>각 데이터의 픽셀 별로 연산</u>을 진행하고자 하는 것이 pointwise convolution이다. Depthwise convolution을 제시한 <a href="https://arxiv.org/abs/1610.02357">논문</a>에서는, feature map(데이터)에서의 채널 간의 상관관계와 spatial(같은 채널에서의 픽셀 간) 상관관계가 완전히 분리될 수 있다고 가설을 세웠다. <br /> 따라서 depthwise convolution은 computational complexity를 줄일 수 있지만 channel 사이의 correlation을 관장할 수는 없고, 이러한 문제를 pointwise convolution을 통해 채널 간 상관관계를 채워주는 느낌이다. 이러한 가설을 토대로 분리한 두 convolution step에 대한 parameter수는, [ ({D_K}^2 \times M) + (M \times N) &lt; {D_K}^2 \times M \times N ] 위와 같이 감소할 수 있는 것이다. <br /> MobileNet 구조는 신기하게도 pooling layer를 전혀 적용하지 않았는데, 그 대신 depth-wise seperable convolution을 stride 2를 적용해서 sub-sampling하는 방식을 채택하였고, 전체 layer 수는 28개다.</p><hr /><h1 id="relu6">ReLU6</h1><p>기존 CNN 구조를 보면 각 convolution block에서의 activation function으로 대부분 ReLU()를 사용한다. 물론 현재는 ReLU()도 이런 저런 문제들 때문에 잘 안쓰이긴 하지만 그래도 조상격인 스테디셀러나 마찬가지다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088813-1d95c642-fdb8-4d3c-a79b-b41f6556355b.png.png" /></p><p>그러나 ReLU의 경우 value가 양수라면 모든 값을 내보낼 수 있기 때문에 <u>상한선이 없다</u>는 문제점이 생긴다. 이게 왜 큰 문제가 될 수 있냐면, 값의 상한선이 없다면 이를 표현할 메모리를 한정적으로 사용할 수 없다는 것이다. 이러한 문제에서 제시된 것이 바로 fixed point 관점, training 관점에서 모두 좋은 평가를 받은 ReLU6 activation function이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088819-ea3c1e5e-b846-4273-9e2a-fba3c1877e4c.png" /></p><p>딥러닝 모델 최적화에 있어 fixed point로 변환해야 하는 경우가 생기는데, 6으로 상한선을 두게 되면 3개의 bit만 가지고 모든 가짓수를 표현 가능하기 때문에 최적화 관점에서 큰 도움이 된다. 또한 상한선을 두게 되면 딥러닝 모델이 학습할 때 퍼져있는 feature들을 더 일찍 학습할 수 있다는 <a href="http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf">분석</a>도 존재한다. 여러 테스트를 통해 상한선을 결정할 때, 6이 가장 성능이 좋았다는 결과를 토대로 ReLU6를 제안했다고 한다.</p><hr /><h1 id="mobilenet-v2">MobileNet v2</h1><p>유명한 논문의 다음 논문인 <a href="https://arxiv.org/abs/1801.04381">MobileNet-v2</a>도 보도록 하자.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088821-fae1de5e-b3d0-4bf2-b344-3fbe24a082fc.png" width="200" /> <img src="https://user-images.githubusercontent.com/79881119/209088825-0dd7fd6b-0c8f-4a83-b04f-98fd747364d9.png" width="300" /></p><p>가장 왼쪽에 보이는 그림이 MobileNetv1의 framework이고, 중앙과 우측의 두 구조가 바로 MobileNetv2의 framework다. <br /> 핵심부터 미리 말하자면 MobileNetv2에서는 <strong>inverted residual</strong> 구조를 사용했다. MobileNetv2의 좌/우 block은 모두 pointwise($1 \times 1$) + Convolution + ReLU6로 시작한다. 그리고 depthwise convolution을 수행할 때 왼쪽 block은 stride 1을 적용하여 spatial dimension을 그대로 가져가고, 오른쪽 block은 stride 2를 적용하여 downsampling한다. 세번째에 다시 pointwise convolution을 적용하는데, 이때는 activation function을 적용하지 않는다. Downsizing이 될 경우 skip connection이 이루어질 수 없기 때문에 skip connection(residual connection)이 없고 왼쪽 block에는 residual connection이 있는 것을 확인할 수 있다. <br /> 대강 구조를 확인해보았는데, 이를 좀 더 풀어서 설명하자면 expansion convolution으로 채널 수를 뻥튀기 시키고, 거기에 depthwise convolution을 적용한 뒤에 이를 pointwise convolution으로 production하는 구조와 같다. 즉 맨 앞단의 $1 \times 1$ convolution은 expanding 역할, 뒷단의 $1 \times 1$ convolution이 projection 역할을 한다. 또한 projection 뒤에는 activation function을 적용하지 않은 이유는 ReLU를 통한 feature 왜곡을 최소화하기 위함이라고 한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088829-4773c43a-319f-4561-b1cb-899e359e8445.png" /></p><p>그림을 참고하면 우측이 inverted residual 구조를 나타낸 것이고 좌측이 원래의 residual 구조를 나타낸 것이다. 채널 수가 확장되는 차이가 있다. 좌측에 보이는 구조가 곧 ResNet 구조라 보면 되고, 우측이 MobileNetv2에서 사용한 구조라 보면 된다.</p><hr /><h1 id="mobilenet-v3">MobileNet v3</h1><p>유명한 논문의 다음 논문의 다음 논문인.. <a href="https://arxiv.org/abs/1905.02244">MobileNet-v3</a>도 보도록 하자. <br /> 우선 심플하게 해당 논문은 inverted residual 구조를 고대로 사용했고, MobileNet v1에서의 pointwise, depthwise 개념이랑 MobileNet v2의 inverted residual은 유용하다고 판단하여 계속 사용한다. <br /> 다만 조금 달라진 점이라면 새로운 activation function을 제시했다는 점 그리고 SE(Squeeze and Excitation network) 모듈을 사용했다는 점이 될 수 있겠다. 각각 순서대로 설명하면,</p><h2 id="h-swish">h-swish</h2><p>h-swish는 swish function의 개량된 버전인데, 솔직히 본인은 swish도 잘 몰라서 찾아봤다. 다들 그렇다고 답해주길 바라고 있다. [ \text{swish}(x) = \frac{x}{1+e^{-x}} ] 그래서 찾아봤는데 오잉? 그냥 시그모이드에다가 $x$ 곱한 녀석이더라. 생각보다 별볼일 없는 친구라 생각이 들었다. 암튼 sigmoid를 요즘은 잘 안쓰니까, 어찌 보면 sigmoid를 조금 바꿔놓은 swish라는 애를 ReLU 식에도 적용이 가능하지 않을까? 라는 생각이다. 는 바로 적용. [ \text{h-swish}(x) = x\frac{\text{ReLU6}(x+3)}{6} <br /> ]</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088832-0c395635-00d3-4c61-ad85-ccd8569df4c4.png" /></p><p>pytorch에서는 hardswish란 이름으로 모듈이 있는 듯하다. 참고해보실 분들은 참고!</p><h2 id="sesqueeze-and-excitation">SE(Squeeze and Excitation)</h2><p>사실 <a href="https://arxiv.org/abs/1709.01507">이 친구</a>는 굉장히 유명하기도 하고 그래서 읽어보는 걸 추천한다. 내용도 엄청 복잡복잡하지 않고 간단명료하게 딱 그림, 설명 이 정도면 대강 어떤 목적으로 구현된 모듈인지 이해할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/209088835-5de42d5d-57b9-406e-a3da-a8986ebc2da2.png" /></p><p><a href="https://arxiv.org/abs/1506.02025">STN</a> 논문을 읽어보신 분들은 아실 수도 있는데, 얘도 궁극적으로 추구하고자 하는 것은 모듈을 사용함으로써 자동적으로 layer 별로 attention이 최적화가 되는 걸 바라는 것이다. 보통 우리가 어떠한 feature map을 쓸 때 해당 feature map의 어느 채널이 유용한 정보를 담고 있는지 신경 쓰지 않기 때문에 효율적인 학습이 어렵다는 관점에서 나온 모듈인데, MobileNet에서 pointwise convolution이 채널 간의 연관성을 되살려주는 역할을 하고 있다는 가정 하에 요런 보조적인 장치 하나가 성능에 큰 효과를 줄 수 있다는 것이다. <br /> 심지어 그림을 보면 알 수 있지만 weight를 도출하는 방식은 단순히 squeeze(spatially average)한 다음에 함수를 통과시키는 구조다. 매우 간단쓰</p><h2 id="nasneural-architecture-search">NAS(Neural Architecture Search)</h2><p>딥러닝 네트워크를 만들 때 네트워크 구조 짜는 거나, 하이퍼 파라미터 튜닝하는 거나 이 모든게 자동화가 완전히 되지 못한다. AutoML은 이러한 디자인/경험적인 문제를 해결하고자 하는 process라 보면 된다.</p><ul><li>Search space : 시도할 아키텍쳐의 집합</li><li>Search algorithm : search space를 어떤 식으로 explore할 것인지. Randomly하게 할 수도 있고 Bayesian optimization 관점으로 접근할 수도 있고 아니면 강화 학습 등등…</li><li>Evaluation strategy : NAS 알고리즘을 어떤 방식으로 학습할 것인지. 흔히 하는 Training/Validation 작업을 할 것인지 혹은 fully-supervision, few-shot 등등…</li></ul><p>대표적인 용어에 대해 설명하자면 위와 같다.</p><p>NAS 방법이 엄청 다양하고, 이거에 대해서 한나절 설명하자니 lightweight 모델에 대한 내용이 부수적이게 될 것 같아 여기서 멈추도록 하겠다.. 암튼 NetAdapt 알고리즘을 통해 network 구조를 최적화했다고 한다. 그래서 그런지 MobileNetv3 논문 제목부터가 “Searching for MobileNetV3”.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/efficient"><p id="prev_title"> ❮❮ Efficient deep neural networks에 대하여</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/gan"><p id="next_title"> ❯❯ Generative adversarial networks(GAN)에 대하여</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div id="gitcus_container" class="gitcus"></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script> <script> document.addEventListener('DOMContentLoaded', function() { const currentTheme = localStorage.theme === 'dark' ? 'dark' : 'light'; const script = document.createElement('script'); script.src = 'https://giscus.app/client.js'; script.setAttribute('data-repo', '6unoyunr/comments'); script.setAttribute('data-repo-id', 'R_kgDOODmwzQ'); script.setAttribute('data-category', 'General'); script.setAttribute('data-category-id', 'DIC_kwDOODmwzc4Cn5wD'); script.setAttribute('data-mapping', 'pathname'); script.setAttribute('data-strict', '0'); script.setAttribute('data-reactions-enabled', '1'); script.setAttribute('data-emit-metadata', '0'); script.setAttribute('data-input-position', 'bottom'); script.setAttribute('data-theme', currentTheme); script.setAttribute('data-lang', 'ko'); script.async = true; script.crossOrigin = 'anonymous'; document.getElementById('gitcus_container').appendChild(script); }); </script>
