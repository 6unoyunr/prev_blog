<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여"><meta name="og:description" content="Zero-shot, Multimodal learning"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png"><meta name="og:image:alt" content="Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여"><meta name="og:url" content="http://localhost:4000/blog/clip"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여"><meta name="twitter:description" content="Zero-shot, Multimodal learning"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png"> <!-- Search Engine --><meta name="description" content="Zero-shot, Multimodal learning"><meta name="image" content="https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | CLIP(Learning transferable visual models from natural language supervision)에 대하여"><meta name="author" content="JY"/><meta itemprop="description" content="Zero-shot, Multimodal learning"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png"><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBDTZMG942"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GBDTZMG942'); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep_learning1" class="navbar-item has-text-grey-light "> DEEP LEARNING THEORY </a> <a href="http://localhost:4000/category/deep_learning2" class="navbar-item has-text-grey-light "> PAPERS & TECHS </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github_blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/clip" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">CLIP(Learning transferable visual models from natural language supervision)에 대하여</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/213900254-5da22304-43ab-457c-9533-0927b0d859df.png" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Zero-shot, Multimodal learning</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>January 22, 2023</b> by <a href="https://6unoyunr.github.io/mycard" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Multimodal</span> <span class="tag is-link">Zero shot</span> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 20 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="what-is-modality">What is modality?</h1><p>논문 소개에 앞서 <u>multimodal</u>이라는 의미에 대해 짚고 넘어가기 위해 먼저 ‘modality’의 의미에 대해 확인해보자. 일반적으로 modality가 가지는 의미는 크거나 작게 구분이 가능하다. 만약 확률 분포에 대해서 생각해보면, modality는 <u>하나의 probability density function</u>를 나타내며, 이때 multimodal은 <u>서로 다른 peak</u>(local maxima)를 가지는 두 개 이상의 mode를 의미한다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213900325-9960901d-7940-4231-834b-c9052dc051e7.png" width="400" /></p><p>하지만 위의 내용은 단순히 확률 분포를 가정한 수학적 접근(해석)에 대한 이야기이고, 실제로 우리가 <strong>딥러닝</strong>에서 이야기하고자 하는 부분은 <u>sensory</u>(감각)과 맞닿아 있는 경우가 많다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213900373-ac1c69a4-5919-4a9d-a020-375e67560f91.png" width="400" /></p><p>인간에게 있어 감각에는 여러 modality(크게 해석해서, 형태나 모양이라고 보면 된다)인 청각, 촉각, 시각 등등이 있으며 이러한 다양한 감각들이 상호작용하여 맞닿아있는 중간 부분이 사람으로 하여금 <u>추론의 근거</u>로 사용된다. 즉 multimodal이란 소통의 방식이 될 수도 있고, 진화심리학에 따른 <u>특정 행동양식의 기준</u>이 되기도 한다. <br /> 예를 들어, Verbal(언어)에 있어서는 단어 각각의 의미를 서술하는 <u>Lexicon</u>이 있고, Speech의 한 부분을 담당하며 각 단어의 유기적인 관계를 표현하는 <u>Syntax</u>(문법)이 있기도 하며 어떠한 context가 진행되는 사람 간의 관계, 시간과 장소 등 언어의 의미 분석에 대해 다양한 요소를 고려하는 <u>Pragmatics</u>(화용론)도 있다. <br /> 이처럼 언어 이외에도 Visual(시각) 요소에는 <u>gesture</u>, <u>body posture</u>나 <u>proxemics</u>(인간의 사회적이나 문화적인 환경 및 공간과의 관계를 통해 정의된 신체 언어) 등이 있다. 더불어 사람과 사람 간의 eye contact, facial expression도 이러한 <strong>modality</strong>의 한 형태로 볼 수 있다. <br /> 앞서 설명한 내용은 내부분 정보를 수집하는 각각의 신체 기관 및 정보 전달의 수단과 관련된 구성 요소들에 살펴보았고, 이제 조금 더 좁은 의미의 modality에 대해 정의하면 다음과 같다.</p><hr /><h1 id="modality-in-computational-environment">Modality in computational environment</h1><p>Modality는 무언가가 경험되거나 발생하는 방식 및 수단이다. 따라서 <u>컴퓨팅 환경에서 이해하는</u> modality는 특정 형태의 정보를 의미하기도 하며, 정보가 저장되는 대표적 형태로 이해할 수 있다. 예를 들어 컴퓨터에 어떤 이미지가 저장된다고 했을 때, <strong>‘Image’ 자체</strong>를 modality로 볼 수도 있고 보다 상세하게 분류해서 ‘jpeg’, ‘bmp’, ‘png’ 등 <strong>이미지 저장 및 압축 방식</strong>에 따라 modality를 구분할 수도 있다. <br /> 앞서 본 인간의 감각과 관련된 sensory modality나 수학적 확률 분포에 근거한 probability modality와 그 결을 함께하여, modality(혹은 medium, media)는 어떤 방법이나 기구를 통해 정보를 저장하고 소통하는 것을 의미한다. 즉 <u>통산 및 의사 소통의 시스템</u>으로 이해할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213900749-965f0ba0-5878-41e5-b53a-c3e5b08230cc.png" width="800" /></p><p>세상에는 <u>여러 종류의 communities</u>(단체)로 인식된 사회/문화 상의 <strong>플랫폼들</strong>이 잇고, modality는 그 상에서 다양한 방식을 통해 정의되는 정보나 소통 방식에 대한 분류이다. <u>deep learning</u>과 크게 관련이 있는 <strong>Natural language</strong>(spoken or written), <strong>Visual</strong>(images, videos) 그리고 <strong>Auditory</strong>(voice, sounds and music)가 그러한 분류 중 하나에 속하며, 이때의 multimodal이란 두 개 이상의 modality를 함께 활용하여 어떠한 task를 풀어나가는 것을 의미한다. 단순히 우리가 알고 있는 text와 image를 함께 사용하는(완전히 결이 다른 modality) task뿐만 아니라, <u>modality만 구분된다면</u>(ex. 사람의 목소리 + 악기 연주) 모두 multimodal로 간주할 수 있다.</p><hr /><h1 id="learning-transferable-visual-models-from-natural-language-supervision">Learning Transferable Visual Models From Natural Language Supervision</h1><p>앞서 multimodal 설명을 굳이 길게 하고 이제야 논문 리뷰를 시작하는 이유는 이 paper의 main contribution 중 하나가 ‘<u>text와 image 간의 유의미한 관계</u>‘를 찾는 것에 집중했기 때문이다. <del>무려 48쪽이나 되는 분량을 잡아먹는</del> 이 논문은 워낙 유명하기 때문에 다양한 블로그에서 리뷰를 했고, 유튜브를 찾아봐도 강의 영상이 정말 많은 것을 알 수 있다. 본인이 이 글에서 집중하고자 하는 것은 <a href="https://arxiv.org/abs/2103.00020">이 논문</a>이 가지는 <u>paper</u>나 <u>방법론</u>에서의 장단점, 그리고 contribution이다.</p><hr /><h1 id="nlp-task의-발전과-cv-task의-한계점">NLP task의 발전과 CV task의 한계점</h1><p><strong>Computer vision task</strong>와 <strong>Natural language processing(NLP)</strong> deep learning은 발전 방향에 있어 차이가 있었다. 처음에 그 시작은 CNN(Convolutional Neural Network)를 baseline으로 recurrent neural network이 개발되었지만, attention mechanism 및 transformer가 소개되며 기계 번역과 관련된 task의 성능이 크게 향상되었고, <u>transformer의 encoder 및 decoder 구조를 활용</u>한 GPT나 BERT model이 발전하며 <strong>많은 downstream task</strong>의 성능이 크게 향상될 수 있었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213901161-30db60a8-bb44-46c8-9f42-ea2e45de38fe.png" width="400" /> <img src="https://user-images.githubusercontent.com/79881119/213901191-b4994362-af6a-4685-bcf0-9a9924ec3b1a.png" width="400" /></p><p>컴퓨터 비전과는 다르게 이러한 대규모 언어 모델이 발전할 수 있었던 가장 큰 이유 중 하나는 <u>데이터셋 수집의 용이성</u>이었다. 보통 영상(image)과 관련된 task를 해결하려면 이에 맞는 domain을 정의하고, 각 domain에 맞는 distribution을 한정하여 이에 맞는 데이터를 수집 및 라벨링하는 과정을 거치게 된다. 예를 들어 강아지를 분류하는 task를 수행하고자 한다면, 분류하고자 하는 강아지의 사진과 각 사진에 대해 강아지 품종을 매칭하는 과정을 거친다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213901272-c17978e2-d803-4e0f-b7d8-4b80fa4ca7cb.png" width="800" /></p><p>하지만 여기서 <strong>image dataset</strong> 수집의 한계가 드러난다. 만약 분류하고자 하는 강아지 품종이 수천/수만 가지가 된다면 이에 맞는 index labeling이 되어야하고, 무엇보다 만약 사진에 강아지보다 background 비중이 높다거나(사진마다 object의 크기가 다르기 때문), 강아지 여러 마리가 포함된 사진 등등 <u>데이터셋 분포의 일관성 및 퀄리티</u>를 보장할 수 없기 때문이다. 또한 위의 그림처럼 열심히 데이터를 모아서 distribution을 맞춘 dataset을 구성했는데, 추가적으로 ‘시바견’을 구분해야한다면 해당 class에 대한 <u>데이터셋 구축을 처음부터</u> 해야하기 때문이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213901370-3d8a3d22-f504-4aaf-bcc4-74cfa9f3cf82.png" width="800" /></p><p>이는 단순히 웹 상에서 <strong>다량의 데이터셋</strong>을 획득하고, 획득한 다양한 <u>text prompt</u>를 <u>token 단위로 supervision에 활용</u>할 수 있는 NLP 모델과 큰 차이가 있다. 이렇듯 computer vision에서 퀄리티, 분포를 모두 고려한 유의미한 데이터셋 구축 과정을 ‘<u>gold labeling</u>‘이라 부르며, 이러한 gold label 없이는 <strong>computer vision task</strong>에서 deep learning의 좋은 성능을 기대하기 힘들다. <br /> 또한 NLP와 CV에서 차이가 발생하는 것이 new task에 대한 zero-shot 및 few-shot 성능에 대한 부분이다. 앞서 설명했던 것과 같이 자연어 모델의 경우 웹 상에서 다량의 데이터셋을 구축 가능하며 분포가 무한에 가까운(continous signal) 이미지와는 다르게 언어는 인간이 사용하는 vocabulary나 문장 내에서 대부분의 형태나 variation이 있기(discrete signal) 때문에 <u>대용량의 데이터를 통해 학습된 대용량 네트워크</u>는 다른 <strong>task</strong>에도 쉽게 적응이 가능하다.</p><hr /><h1 id="restricted-form-of-supervision">Restricted form of supervision</h1><p>이러한 SOTA computer vision system의 한계점(미리 정의한 object category에 따른 데이터만을 학습할 수 있고, 실제로 데이터 수집 단계에서 이러한 방법 이외에는 사용할 수 없다는 것)을 이 논문에서는 해결해야할 문제로 제시하였다. 이를 해결하지 못한다면 <u>NLP 모델</u>이 가지는 <strong>generality</strong>(일반화 성능), <strong>usability</strong>(다양한 downstream task에 활용할 수 있음)을 CV에서는 영원히 가져갈 수 없다는 것이다. <br /> 따라서 CLIP 논문에서는 <u>image 학습에 raw text를 함께 활용</u>하여, image supervision 자체를 학습하는 것보다는 text prompt와의 관계성을 통해 <strong>text representation</strong>과 <strong>image representation</strong>을 연결하는 방식을 사용했다. <br /> 이는 데이터 수집 단계에서도 한계가 있었던 기존 computer vision 접근 방식에도 적용될 수 있는데, 웹에서 이미지를 수집하고 이에 맞는 <u>class categorize</u>를 진행하는 것보다는, 웹 상에서 자주 검색되거나 반복되는 <strong>text prompt</strong>를 기반으로 이를 묘사하는 이미지를 검색하게 되면 <u>굳이 domain distribution을 고려한 dataset 구축 없이도</u> 학습이 가능하기 때문이다. 예를 들어 위키피디아 상에서 많이 언급된 단어들 중 하나가 ‘<u>귀여운 고양이</u>‘라면, 해당 text를 기반으로 크롤링한 image를 ‘귀여운 고양이’라는 text와 연관짓는 학습만 진행하면 되기 때문이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213902189-7c92b1f4-776c-4334-a980-0a461eedbb02.png" width="800" /></p><p>기존 방식과 비교한 형태는 위의 그림과 같다. 데이터셋 구축 단계에서 task를 정의하는 부분이 빠지고, <u>image/text의 유기적 학습</u>을 위한 <strong>text prompt</strong>를 설정하고 이를 토대로 데이터를 수집하는 과정이 추가되었다. <strong>Text prompt</strong>에 대한 데이터를 수집했기 때문에 기존의 <u>task based dataset 수집</u>과는 다르게 라벨링에 <strong>추가 과정</strong>이 들어가지 않는다.</p><hr /><h2 id="task-robust-representation-learning-with-clip">Task-robust representation learning with CLIP</h2><p>사실 이미지와 텍스트 사이의 유기적인 관계를 학습하고자 한 연구는 CLIP 이전에도 존재했었다. <u>머신러닝 기반</u>으로 접근했던 방식으로는 <strong>content based image retrieval</strong>(특정 문서와 함께 주어진 이미지를 기반으로 nouns, adjectives를 예측하는 task), <strong>caption prediction with manifold learning</strong> 그리고 <strong>multimodal deep Boltzmann machine</strong>을 활용한 low level image와 text tag feature의 학습이 있다. 이후에도 CNN based approach나 Transformer based approach로 접근한 <u>deep learning 기반</u>의 방법들이 제시되었지만, 여전히 ‘<u>image representation learning</u>‘이라는 관점에서 natural language 방식들은 <u>zero-shot learning</u> 성능을 끌어올리지 못하고 있었다. 이렇듯 CLIP 이전의 deep learning 방식들은 대부분 JFT-300M dataset과 같이 대용량 데이터셋을 활용하는 방식이라던지, 관련된 prompt를 instagram hashtag로 사용하여 대용량의 dataset으로 학습하는 등 앞서 설명했던 기존 computer vision 방식의 <strong>gold-label</strong>과 <strong>unlimited amounts of text prompt</strong> 기반으로 수집한 데이터셋 중간 과정에 놓인 연구들이다. 여전히 한정된 supervision을 가지고 있으며, 고전적인 softmax classifier에 의존한 학습 때문에 representation의 유연한 학습과 zero-shot 성능을 제한하는 요소가 되었다. <br /> 이러한 weakly-supervised method와는 다르게 <a href="https://arxiv.org/abs/2006.06666">VirTex</a>, <a href="https://arxiv.org/abs/2008.01392">ICMLM</a> 그리고 CLIP 논문이 가장 큰 insight를 얻었던 <a href="https://arxiv.org/abs/2010.00747">ConVIRT</a>의 경우에는 이 논문의 방향성과 비슷하게 language 정보를 사용하여 image representation을 학습한다. 하지만 이러한 연구들에서는 <u>대용량의 데이터를 사용해보지 않았다</u>는 점에서 CLIP 저자들이 <u>ConVIRT</u>(참고로 ConVIRT는 medical diagnosis와 관련된 task라 데이터셋이 한정적이다)의 컨셉을 그대로 이용하되, 대량의 WebImageText dataset을 활용하여($400M$) scratch부터 contrastive representation learning을 진행한 것이다. <br /> 학습 과정에서 저자들은 가장 효율적인 학습이 가능한 방법론을 탐색했으며, 결론적으로는 자연어 모델인 <u>GPT 시리즈</u>와 같이 <u>다양한 task에 적용 가능한</u> 네트워크를 학습시킬 수 있었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213903087-38687ef4-47e8-43ef-ad4c-0d5b8c209277.png" width="600" /></p><p>Image-captioning baseline인 transformer language 모델을 학습하는 것보다 <u>bag of words</u> prediction(BoW)을 학습하는 것이 더 효율적이고, bag of words prediction과 CLIP(constrastive learning)을 함께 활용한 것이 <strong>zero-shot</strong> 성능을 높이는 데에 있어 약 $12$배의 학습 효율이 나타났다고 한다.</p><hr /><h1 id="several-approaches">Several approaches</h1><p>앞서 길게 설명하고 넘어오긴 했지만, 해당 논문에서 <strong>approach</strong>를 분류하여 설명한 부분이 사실상 <u>main contribution</u>을 알아보기에 가장 적합한 구간이라고 생각되어 각 요소마다 간단하게 요약하고 넘어가도록 하겠다.</p><h2 id="nlp-supervision">NLP supervision</h2><p>기존의 <u>text와 image pair를 학습했던 연구</u>들은 대부분 unsupervised, self-supervised, weakly-supervised 등의 방법을 토대로 <u>natural language supervision</u>을 사용했기 때문에 image representation 학습에 text 정보를 직접 연관지을 순 있었으나 NLP가 가지는 정보를 제대로 활용하지 못했다. 하지만 NLP는 training 과정에서 $1$ to $N$ mapping이 필요없으며, categorized된 labeling에 비해 활용할 수 있는 정보가 많다는 장점이 있다. 따라서 이 논문에서 집중하고자 한 내용은 NLP를 supervision으로 활용해서 image를 통해 text를 예측하는 형태로 image/text multimodal learning을 진행하자는 기존 concept에서 벗어나, <strong>NLP representation</strong>에 <strong>image representation</strong>을 연결만 시켜주자는 concept을 사용하였다.</p><h2 id="dataset-수집">Dataset 수집</h2><p>MS-COCO, Visual Genome 그리고 YFCC100M과 같은 데이터셋은 수가 현저히 적거나, 대용량의 데이터셋이라 하더라도 image의 metadata(ex. image filename)을 이용한 정보 추출이 어려웠기 때문에 대용량 데이터셋 구축이 힘들었다. <br /> 따라서 NLP supervision에서 수집하는 web based 방식에서 motivation을 얻어, 인터넷 상에서 많이 사용되는 query(text prompt)를 augmentation하여 $500,000$ 만큼 준비했으며, 각 query에 맞는 (image, text) pair를 최대 $20,000$ 씩 구축하였다. 결국 목표로 했던 <strong>GPT-2 training dataset</strong>인 ‘<u>WebText</u>‘의 총 갯수에 필적하는 대용량 데이터셋인 ‘<u>WebImageText</u>‘를 구할 수 있었다.</p><h2 id="selecting-an-efficient-pre-training-method">Selecting an efficient pre-training method</h2><p>그러나 데이터셋 크기가 커진만큼 학습에 걸리는 시간 또한 길어졌기 때문에, 빠른 수렴을 위한 효율적인 pre-training method를 찾는 것이 중요했다. 저자들은 이 부분에서 시행착오를 겪은 여러 방법들에 대해 언급한다.</p><h4 id="jointly-trained-an-image-cnn-and-text-transformer-from-scratch">Jointly trained an image CNN and text transformer from scratch</h4><p>VirTex 논문에서와 같이 접근한 경우가 된다. 하지만 이 방법을 사용하게 되면 위에서 봤던 그래프(transformer based learning)와 같이 간단한 baseline인 <strong>ResNet과 BoW encoder를 사용</strong>했을 경우보다 수렴 성능이 낮은 것(약 $3$배)을 확인할 수 있다.</p><h4 id="bag-or-words-with-simpler-baseline">Bag or Words with simpler baseline</h4><p>하지만 BoW 방식과 transformer 방식의 차이는 구조(parameter 개수, CNN and transformer)에 있을 뿐 결론적으로 두 방식 모두 공통적으로 각 이미지에 대해 exact words를 찾고자 하는 <u>NLP supervision concept</u>에서 벗어나지 못했다. <br /> 이는 단순히 이미지를 표현하는 방식을 단일의 description에 한정지었기 때문인데, 예를 들어 다음과 같은 이미지가 있다고 생각해보자.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213903991-7ee64c92-1253-4f0e-b06d-0119cade64cb.png" width="500" /></p><p>이 사진을 묘사하는 text prompt는 단순히 ‘고양이 두마리가 있는 이미지’만 있는 것이 아니라, ‘서로 좋아하는 두 고양이’, ‘초원에 서 있는 두 고양이’ 등 <u>관련된 text embedding space</u>에서 <strong>유사한 영역에 놓일 수 있는</strong> 모든 representation의 총체가 될 수 있다. 결국 contrastive learning(관련이 있는 text embedding과의 유사성을 올리고, 관련이 없는 text embedding과의 유사성을 낮추는) 방식이 더 효율적인 학습 효과를 불러왔으며 바로 이 방법이 CLIP에서 사용한 학습법이다.</p><h2 id="contrastive-learning">Contrastive learning</h2><p>만약 $N$개의 (image, text) pair의 batch가 있다고 해보자. CLIP은 $N \times N$의 가능한 (image, text) pair에 대한 <strong>prediction</strong>을 진행한다. 정답이 되는 text prompt는 batch 내에 image당 하나씩 총 $N$개 있기 때문에, $N \times N$ prediction에서 $N$은 positive pair가 되고 나머지 $N^2-N$은 negative pair가 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213904183-1ff81e1a-0a92-42f2-8742-bc87e5dd69e6.png" width="500" /></p><p>위의 그림과 같이 각 sample embedding에 대한 <u>symmetric cosine similarity</u>를 matrix로 표현 가능한데, 이를 normalize한 cross-entropy loss를 최적화한다고 생각하면 된다. Batch 단위에서 similarity를 기반으로 cross-entropy loss를 사용하는 형태는 multi-class $N$-pair loss, InfoNCE loss 등의 연구로부터 소개되었고, 이 논문에서도 <strong>같은 메커니즘</strong>을 사용하였다.</p><h2 id="training-from-scratch">Training from scratch</h2><p>데이터셋 개수가 $400M$이므로 overfitting을 걱정할 필요가 없다. 따라서 네트워크를 pre-training하는 과정에서 따로 representation과 관련된 initializing 없이 바로 scratch부터 학습을 진행하였으며 crop 이외에는 data augmentation을 진행하지 않았다. 또한 multi-modal embedding space로 mapping하는 과정에서 non-linear projection을 진행하지 않았는데, 이는 self-supervised learning에서 사용되던 방식과는 다르게 여기에서는 <u>해당 training efficiency</u>와 관련된 method를 사용할 필요가 없었기 때문에 연산량을 줄인 것으로 보인다. 그리고 entropy 조절에 사용되는 temperature normalizing hyperparameter $\tau$는 직접 설정하게 되면 ablation 진행이 어려웠기 때문에 학습 가능한 parameter로 설정하였다.</p><h2 id="network-selection">Network selection</h2><p>BoW baseline으로 사용된 ResNet-50을 CLIP에서도 사용했으며, 이에 <a href="https://arxiv.org/pdf/1812.01187v2.pdf">ResNet-D</a> 논문을 통해 제시된 다양한 improvement 또한 encoder로 사용하였다. 또한 image embedding의 dimension을 맞추기 위해 feature extraction의 global average pooling을 attention pooling layer로 대체하였다. <br /> ResNet 구조 이외에는 ViT(Vision Transformer)에서 조합된 patch와 position embedding에 추가 layer normalization을 더한 것 이외에는 동일한 구조를 사용하였다. <br /> Text encoder로는 <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>를 사용하였고, base size로는 $12$개의 layers의 $512$의 channel dimension width를 가지는 네트워크를 사용하였다(attention head는 $8$개).</p><hr /><h1 id="results">Results</h1><p>사실 이 논문이 방대하게 길어진 이유 3가지는 다음과 같다.</p><ol><li>Related works가 너무 많다.</li><li>저자들이 주장하는 contribution이 방대하다.</li><li>컨셉 하나에 대해 설명하는 과정(빌드업)이 너무 크다.</li></ol><p>그러다보니 실험 결과도 자연스럽게 길어지게 되었고, 논문을 모두 읽으면서 파악하기에는 다소 무리가 있지 않나 싶다. 따라서 비교적 <u>주요한 내용들만 위주로</u> figure를 참고해서 작성해보았다.</p><h2 id="zero-shot-with-prompt-engineering">Zero-shot with prompt engineering</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213904974-bec5109c-1bf0-4a67-8347-91b0c299e048.png" width="400" /></p><p>우선 CLIP이 주로 다룬 내용은, <strong>자연어 모델</strong>과 같이 <strong>이미지 모델</strong> 또한 충분한 데이터셋을 기반으로 representation learning이 진행된다면, 이에 따라 <u>새로운 task에 대한 performance</u>도 <strong>zero-shot</strong>으로 진행될 수 있음을 증명하는 것이었다. 새로운 task(classification)에 대해 downstream task를 진행하는 과정은 다음과 같다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905079-04fba334-510c-4708-ab1e-a83b8c1bc098.png" width="500" /></p><p>여기서 저자들은 단순히 class 이름을 prompt로 사용하는 것이 아닌, ‘<u>A photo of a {object}</u>’ 식의 prompt engineering을 각 dataset 특성에 맞게 진행하여 성능을 올릴 수 있었다고 한다. 학습에 사용되는 (image, text) pair 자체가 object를 표현하는 단어로 구성된 것이 아니라, <u>image scene을 설명하는 description</u>이기 때문에 학습된 <strong>domain distribution</strong>과 <strong>class name distribution</strong>에 차이가 있다는 것이 주된 분석이었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905043-21a4ccaa-184a-4d46-8655-c98ac5ffd531.png" width="500" /></p><p>$N$개의 class에 대한 text prompt $N$개 와의 similarity를 구할 수 있고, 이 중 가장 높은 유사도의 index가 해당 image의 prediction이 되는 구조다. 실제로 <strong>prompt engineering</strong>을 통해 <u>classification 성능</u>(average)이 올라가는 것을 확인할 수 있다.</p><h2 id="zero-shot-vs-fully-supervised-baseline">Zero-shot vs fully-supervised baseline</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905261-bb97911c-d308-46b4-b911-242a256b5bcd.png" width="350" /></p><p>ResNet-50을 baseline study에 사용했기 때문에 linear probe와의 성능 비교가 가능한데, 위의 표에 있는 총 $27$개의 dataset을 fully-supervision에 사용한 결과보다 $16$개의 dataset에서 더 좋은 성능(positive $\Delta$)를 보여주는 것을 확인할 수 있다.</p><h2 id="zero-shot-vs-few-shot">Zero-shot vs Few-shot</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905359-aff9e1f2-6790-4283-9644-f744974df527.png" width="500" /></p><p>마찬가지로 여러 few-shot methods에 대해 <u>일정 수의 샘플을 제공</u>했을 때 중에서 가장 최고의 성능을 보인 그래프끼리 비교하면 zero-shot CLIP이 4-shot method에 필적하는 결과를 보여주고, 심지어 <u>BiT-M, SimCLRv2</u> 등 기존 방식은 <strong>16-shot</strong>을 진행했음에도 <strong>Zero-shot CLIP이 획득한 정확도</strong>를 넘기기 어려운 것을 확인할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905487-ccc29be3-832e-4a68-9fff-c82e28244278.png" width="500" /></p><p><u>CLIP의 zero-shot</u> 성능에 필적할 만한 성능을 보이기 위한 few-shot sample의 수를 나타낸 표를 보면 대부분의 dataset에 대해서 평균 $20.8$의 sample이 필요한 것을 알 수 있으며, 이를 토대로 CLIP method가 가진 <strong>data efficiency</strong> 및 <strong>task robustness</strong>에 대해 확인할 수 있다.</p><h2 id="zero-shot-performance--linear-probe">Zero shot performance &amp; Linear probe</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905589-43e5a2ec-1605-45b9-b596-fc0278564a4d.png" width="500" /></p><p>또한 논문에서는 단순히 CLIP이 ‘텍스트와 이미지만 대충 유사하게 connection’한 것이 아니라, classifier 기반의 linear probe system에서 학습 가능한 image representation을 image encoder가 동일하게 학습이 가능하다는 점을 위의 그래프를 통해 보여준다. <u>Zero-shot 성능이 좋지 않다면</u> 마찬가지로 <u>Linear probe 성능도 좋지 않다</u>는 경향성을 보여줌으로써, classification 학습이 어려운 task에 대해 <strong>CLIP도 성능이 낮아지는 결과</strong>를 보여주었다.</p><h2 id="zero-shot-performance-with-network-scale">Zero shot performance with network scale</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905665-6b1e381e-eae6-4b74-8742-6596d334146f.png" width="500" /></p><p>또한 <strong>기존 방식들</strong>(ex. EfficientNet)에서 주장했던 경향성과 동일하게 image encoder의 representation power가 늘어날수록(network의 scale이 커질수록) 이에 따른 학습된 <u>CLIP의 zero-shot 성능이 높아지는 것</u>을 알 수 있다.</p><h2 id="clip-is-also-a-good-representation-learner">CLIP is also a good representation learner</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905803-ed18c7b2-8dfb-4017-8290-d026b85e3ae2.png" width="1000" /></p><p>학습 과정에서의 <strong>GFLOPS/Image</strong>에 따른 <strong>Average score</strong> 비교는 위의 그래프와 같다. 위의 결과 그래프는 zero-shot을 비교한 것은 아니고 실제로 CLIP이 pre-trained 과정에서 학습한 representation이 <u>Linear probe</u> 성능에 얼마나 도움이 되는지 확인한 절차이다. 실제로 그래프상 가장 좋은 성능을 보인 CLIP network와, 함께 비교한 network 중 가장 높은 representation learning 성능을 보인 <u>EfficientNet-NoisyStudent</u>와 여러 데이터셋에 대해 성능 비교를 하였다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213905957-a00dd72b-9d84-44c5-a202-b33e76587155.png" width="350" /></p><p>물론 <u>ViT(L/14) 기반으로 학습된 CLIP</u>이기 때문에 당연히 더 좋겠다고 예상하긴 했지만 아무튼 $27$의 dataset 중에서 $21$ 만큼의 데이터셋에 대해 더 좋은 성능(positive $\Delta$)를 보여주었다.</p><h2 id="robustness-to-natural-domain-shift">Robustness to natural domain shift</h2><p>물론 supervision을 활용한 deep learning network들의 성능이 충분히 좋아지긴 했으나, 여전히 distribution shift와 같은 문제에 취약한 면이 있다. 예를 들어 단순히 <u>ImageNet dataset</u>에 대해 학습된 네트워크는 각 class의 object의 특징에 대해 <strong>유의미한 feature</strong>를 학습 및 인식하기보다는, <u>training dataset</u>의 <strong>in-distribution</strong>을 neural network에 fitting하는 형태로 학습이 진행된다는 것이다. ImageNet dataset에 대해 $7$가지의 natural distribution shift가 일어난 dataset인 <a href="https://arxiv.org/abs/1902.10811">ImageNetV2</a>, <a href="https://arxiv.org/pdf/1905.13549v2.pdf">ImageNet Sketch</a>, <a href="https://arxiv.org/pdf/1906.02168v3.pdf">Youtube-BB and ImageNet-Vid</a>, <a href="https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html">ObjectNet</a>, <a href="https://arxiv.org/abs/1907.07174">ImageNet Adversarial</a> 그리고 <a href="https://arxiv.org/pdf/2006.16241v3.pdf">ImageNet Rendition</a>에 대해 validation을 진행하였다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213913187-9e5293a6-e6fd-4442-86da-29541a8c0ce3.png" width="700" /></p><p>사실 이 figure에 대한 설명이 논문에 없는 부분이 잘 이해가 가질 않지만 일단 이 부분에 있기 때문에 해석하는 바로는 CLIP의 zero-shot 모델들이 linear probe를 통해 domain transfer를 진행했을때, 보다 ImageNet의 성능을 잘 유지하는 것으로 보아 domain shift 현상에 잘 대처한다고 볼 수 있다. 하지만 위의 figure가 main은 아니고, 다음 figure를 보면 왜 <u>CLIP model</u>이 <strong>zero-shot</strong> 및 <strong>domain shift</strong> task에서 game changer로 등장할 수 있었는지 실험적 증거를 확인할 수 있다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213913448-cd6058d5-26bc-47f1-927c-4bb18ae42be9.png" width="1000" /></p><p>이상적인 상황에서는 domain shift가 일어나더라도 ImageNet dataset에 대한 성능과 domain shifted dataset에서의 성능이 동일해야한다($y = x$). 단순히 ImageNet에 대해서 학습한 경우 그 성능이 $2 \sim 30\%$ 감소하는 것을 확인할 수 있지만, zero-shot CLIP 모델의 경우 $7$개의 natural domain shifted dataset의 성능 평균이 <u>최대 약 $75\%$나 상승할 수 있음</u>을 보여주었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213913670-3f082bde-b940-4d35-ba77-4029d3b233f5.png" width="1000" /></p><p>여기서 의문이 들 수도 있는 점은, 단순히 zero-shot을 기반으로 성능을 측정하는 것과 dataset에 <u>specific</u>하게 <strong>fine-tuning</strong>한 <u>logistic regression classifier</u>에 유의미한 차이가 생길 수 있지 않을까에 대한 부분이다. 결과를 보게 되면 ImageNet에 대해 fine-tuning한 경우 original dataset에 대해서는 $9.2\%$의 괄목할만한 정확도의 향상이, ImageNet의 데이터셋 수집 방식과 유사했던 ImageNetV2에 대해서는 그에 비해 절반 정도의 정확도의 향상이 이루어졌고, Video dataset인 Youtube-BB와 ImageNet-Vid를 제외한 데이터셋에 대해서는 성능이 떨어지는 결과가 나타났다. <br /> 사실 그냥 text encoder를 포함한 CLIP model로 <u>zero-shot classification</u>을 진행했다면, 위에서 열심히 실험했던 내용에 기반하여 ‘text가 guidance를 주기 때문에 domain에 robust한 예측을 할 수 있다’라고 결론을 내릴 수 있지만, 놀라운 점은 classifier를 통해 downstream task를 fine-tuning했을 때도 domain shift에 대한 정확도가 유지될 수 있다는 점이다. <strong>이 부분에 대해서는 저자들이 명확하게 그 근거를 설명하지 못한 채</strong> 넘어갔다. <br /> 저자들이 추가로 확인한 실험 중 하나는 transfer dataset의 class들은 항상 $1000$-way classifier가 적용되는 ImageNet과 일치하지 않는다는 점 때문에 문제가 생긴 점이었는데, 이전 과정에서는 이를 ImageNet class의 하위 항목에 해당되는 prediction에 대한 max-pooling으로 진행하였다. 이러한 방식은 정확하지 않았으며, 저자는 이러한 방식 대신 CLIP에서 사용할 수 있는 방법인 ‘class name을 prompt화 시킨 text embedding’을 사용하였다. 그 결과 Video based dataset인 Youtube-BB, ImageNet-Vid의 성능 향상이 있었으며 ObjectNet의 약간의 성능 향상만이 있었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213914563-51f46a4c-8b7e-4d08-86e4-b79b67b7e6ec.png" width="400" /></p><p>위의 그림을 통해 Zero-shot CLIP에 Few-shot 만큼의 supervision이 더해질수록, fully-supervised learning에 비해 가지던 robustness가 줄어드는 것을 확인할 수 있다. 즉 학습에 사용되는 dataset sample이 많아질수록 <u>in-distribution</u>에 fitting되어 기존의 zero-shot CLIP model에 비해 같은 ImageNet performance에 대한 robustness가 감소한다고 할 수 있다.</p><h2 id="comparison-to-human-performance">Comparison to Human Performance</h2><p>사람으로 하여금 zero-shot 성능을 측정하기란 굉장히 어렵다. 하지만 이런 상황에서도 zero-shot 혹은 few-shot task performance를 비교할 수 있는 방법이 있는데, 바로 Oxford-IIT Pets dataset($37$개의 강아지 혹은 고양이 종류 맞추기)에 대한 것이다. 아무리 사람이라도 강아지나 고양이의 모든 종에 대해 알고 있지는 않을 것이다. 특정 이미지를 보여주고, 해당 종에 대해서 알고 있다면 종류를 대답하면 되고 그렇지 않다면 ‘I don’t know’를 대답하면 된다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213914909-19070974-15ed-476d-b066-2acbf47ac437.png" width="600" /></p><p>어느 정도 직관으로 이해할 수 있지만, 사람은 보통 $1$개의 이미지가 주어지면(one-shot) 해당 종에 대한 분류 정확도가 올라가지만, 추가로 $n$개의 이미지를 보여주더라도 해당 종에 대한 분류 정확도가 올라가지는 않는다. 이를 표현하는 문장으로는 ‘humans know what they don’t know’인데, 본인이 모르는 것에 대해서는 명확하기 때문에 한 번 해당 내용을 학습하는 것으로 충분하다는 뜻이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213914869-d1f0cf60-7da0-4929-a566-938483f2fb9c.png" width="600" /></p><p>따라서 <u>딥러닝 네트워크로 하여금</u> 사람과 같이 <strong>few-shot</strong> 성능이 prior knowledge를 활용하여 유의미한 결과를 이끌어내지 못하는 점이 유사하고, 이를 찾아내는 것이 <strong>CLIP의 중요한 발전</strong>이 될 것이라고 언급한다. 또한 위의 그래프에서 볼 수 있듯, 각 종에 대한 zero-shot 성능을 인간과 비교했을 때, 서로 어렵게 느끼는 breed에 대한 경향성이 어느 정도 유사한 것을 확인할 수 있다.</p><h2 id="data-overlap-analysis">Data overlap analysis</h2><p>앞서 WebImageText 데이터셋을 구축하는 과정에서 <u>대량의 웹 이미지</u>를 가져와 학습한다고 했는데, 여기서 발생할 수 있는 문제는 validation에서 사용할 일부 dataset 또한 web 상에서 획득할 수 있는 sample에 해당되기 때문에 이러한 validation set이 pre-training dataset에 들어가게 된다면(<u>leak into</u>) 문제가 될 수 있다. 이를 방지하는 한 가지 방법은 네트워크를 학습시키기 전에 사전에 duplicated image(validation set과 동일한 샘플)을 모두 제거하는 작업이 필요하지만, 결국 <u>모든 dataset에 대해 검증을 거쳐야한다</u>는 점이 time-consuming하다. <br /> 이러한 brute-force 방식 대신, 저자들은 overlap이 발생하는 방식을 정의하고 다음과 같은 과정을 통해 overlap을 확인하였다.</p><ol><li><p>각 validation dataset에 대해 duplicate detector(논문의 appendix에 있다고 한다)를 각 example에 대해 수행한다. 그런 뒤 nearest neighbor를 직접 조사한 뒤, 특정 threshold(sample간 거리 metric)를 설정하여 Overlap(threshold보다 큰 유사성을 가지는 샘플)과 Clean(threshold보다 낮은 유사성을 가지는 샘플)로 subset을 구성한다. Data의 clean한 정도(겹치는 데이터셋이 없음을 의미한다)를 확인하기 위해 All(Overlap + Clean)에 대한 Overlap의 비율을 구한다.</p></li><li><p>그런 뒤 All, Overlap, Clean에 대해 CLIP RN50x64에 대한 zero-shot accuracy를 계산한다. (All accuracy) - (Clean accuracy)가 의미하는 바가 contamination에 의한 accuracy 차이가 되므로, 이 값이 양수라면 얼마나 overlapping data에 의해 over-fitting 되었는지에 대한 정보를 반영한다.</p></li><li><p>Overlap의 양이 적은 경우가 있기 때문에 binomial significance test를 추가로 진행한다. Clean accuracy를 null hypothesis로 잡고 Overlap subset에 대한 one-tailed $p$-value(greater)를 얻는다. 추가로 $99.5\%$의 Clopper-Pearson confidence intervals 계산을 Dirty subset에 진행한다.</p></li></ol><h4 id="binomial-distribution-test">Binomial distribution test</h4><p>실험 결과를 확인해보기 전에 Binomial distribution test에 대해서 수식을 통해 이해하면 다음과 같다. 우선 $H_0$라는 test hypothesis가 있고, 이를 null hypothesis라고 부르도록 하자. 우리가 진행할 test에서 null hypothesis는 clean accuracy이므로, 적은 양의 <u>overlapping dataset을 제외하고 진행한</u> zero-shot validation 성능을 의미한다.</p><p>[ H_0 = \pi = \pi_0 <br /> ]</p><p>정확도는 $0 \sim 1$의 값을 가지게 되므로, 이를 CLIP model이 overlapping되지 않은 데이터셋을 기준으로 특정 샘플에 대해 ‘정확하게 분류할 확률’이라고 하자. <br /> Overlap dataset이 $n$개 있고, 그 중 샘플 $k$만큼이 CLIP model에 대해 정확하게 분류될 확률이라면 다음과 같이 표현할 수 있다.</p><p>[ \text{Pr}(X = k) = \begin{pmatrix} n \newline k \end{pmatrix} p^k (1-p)^{n-k} ]</p><p>우리는 Overlap dataset 내부의 확률 $p$로 하여금 이 값보다 커질 $p$-value를 구하고 싶다. 왜냐하면 그래야만 <u>overfitting을 가정할 수 있기 때문</u>이다.</p><p>[ p = \sum_{i=0}^{k} \text{Pr}(X = i) = \sum_{i = 0}^k \begin{pmatrix} n \newline i \end{pmatrix} \pi_0^i (1-\pi_0)^{n-i} ]</p><p>해당 $p$를 구하는 one-tail test는 위와 같다. Sucess의 개수가 $n\pi_0$가 되는 것이 기준점이 되기 때문이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/213917253-bea7c88d-647d-408d-b0c8-a059d9744ae1.png" width="1000" /></p><p>결과 그래프는 위와 같다. 총 $35$개의 dataset에 대해 실험을 진행한 결과, Overlapping보다 Clean subset의 accuracy가 높은 $9$개의 dataset에 대해서는 overlap이 없다고 결론을 내렸다. 결과적으로는 대부분의 데이터셋에 대해서 overlap이 심하지 않았고, overall accuracy가 크게 shifted된 부분은 $0.1\%$의 threhold 기준으로 $7$개의 dataset이 있었다. 이 중 Bonferroni correction을 기준으로 $2$개의 dataset이 가장 큰 것으로 판별되었다(Birdsnap, Country211).</p><hr /><h1 id="결론">결론</h1><p>논문에서는 뒤에 추가로 논문이 가지는 한계점이나 appendix를 통해 설명하지 못한 디테일한 구현 방법에 대해서 소개한다. 사실 위에서 정리한 내용만 해도 거의 20 page에 가깝기 때문에 이 논문이 <del>지나치게 길다는 사실</del>이 더 와닿는다. <br /> 아무튼 길게 보아 이 논문이 가지는 contribution 중 가장 메인이 되는 부분을 정리해보면 다음과 같다.</p><ol><li>Text model에서 사용되던 대용량 dataset을 구축하여 image model을 효율적으로 학습하였다.</li><li>유의미한 text to image 관계를 학습할 수 있었다. 즉, multimodal을 가능케 했다.</li><li>Zero-shot transfer learning의 성능을 비약적으로 향상시켰다.</li></ol><p>그리고 실험 내용을 기반으로 확인한 추가 contribution은 다음과 같다. 이 부분은 사실 main contribution이 아닌 sub contribution에 해당된다.</p><ol><li>Linear probe나 logistic regression시 성능을 보아, representation learning이 어느 정도 잘 진행됨을 알 수 있다.</li><li>Domain shift에 robust한 모델을 학습하였다.</li><li>단순히 무작정 dataset을 모은 것이 아니라, dataset의 overlapping 문제도 분석하였고, 이를 통해 성능 향상이 overfitting에 의한 결과가 아님을 증명하였다.</li></ol><p>다만 다시 한번 말하지만 <u>논문이 너무 길고</u>, 솔직히 몇몇 분석은 이 paper 이후 연구로 넘겼어도 충분하지 않았나 싶다..</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/pytorch2"><p id="prev_title"> ❮❮ Pytorch 2.0에서 어떤 점이 변했을까??</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/maskclip"><p id="next_title"> ❯❯ MaskCLIP 논문 리뷰</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/deep_learning2">PAPERS & TECHS</a></li><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github_blog">GITHUB BLOG</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/first_diary">차별화된 삶</a></li><li> <a href="http://localhost:4000/blog/reasoning">추론 모델은 정말로 뛰어난가?? 오픈소스 엑사원 딥(EXAONE-Deep), QWQ, 딥시크(Deepseek-R1) 비교 실험해보기</a></li><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
