<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | About GAN supervsed alignment(GAN-gealing)</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | About GAN supervsed alignment(GAN-gealing)"><meta name="og:description" content="GAN gealing"><meta name="og:image" content="https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif"><meta name="og:image:alt" content="Welcome to my blog | About GAN supervsed alignment(GAN-gealing)"><meta name="og:url" content="http://localhost:4000/blog/gangeal"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | About GAN supervsed alignment(GAN-gealing)"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | About GAN supervsed alignment(GAN-gealing)"><meta name="twitter:description" content="GAN gealing"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif"> <!-- Search Engine --><meta name="description" content="GAN gealing"><meta name="image" content="https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | About GAN supervsed alignment(GAN-gealing)"><meta name="author" content="JY"/><meta itemprop="description" content="GAN gealing"><meta itemprop="image" content="https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif"><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> function changeGiscusTheme () { function sendMessage(message) { const iframe = document.querySelector('iframe.giscus-frame'); if (!iframe) return; iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app'); } sendMessage({ setConfig: { theme: localStorage.theme } }); } /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; /* 토글 이벤트 리스너 */ mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; changeGiscusTheme(); }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/gangeal" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">About GAN supervsed alignment(GAN-gealing)</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://user-images.githubusercontent.com/79881119/210159076-23125484-b1a0-40a4-a676-23518861a952.gif" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">GAN gealing</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>January 01, 2023</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Generative model</span> <span class="tag is-link">AI</span> <span class="tag is-link">Deep learning</span> <span class="tag is-link">GAN</span> <span class="tag is-link">GAN gealing</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 14 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="논문-요약-및-소개">논문 요약 및 소개</h1><p>이번에 소개할 논문은 이번에 연구실에서 논문을 준비하면서 여러 생성 모델 분야에 대해서 살펴보다가 흥미있게 봤던 <u>GAN-supervised learning</u>이다. 이전 리뷰들은 여러 논문들을 전반적으로 살펴봤었는데, 이번에는 <a href="https://arxiv.org/abs/2112.05143">이 논문</a> 하나만 집중해서 볼 예정이다. 사실 이름만 봐서는 단순히 generative network를 학습하는 것처럼 느껴지지만, GAN-supervised learning이라는 의미는 ‘GAN’이 만들어내는 이미지를 supervision으로 사용함과 동시에 최적화하겠다는 의미가 된다. <br /> <a href="http://vis-www.cs.umass.edu/papers/PAMI_congeal.pdf">Classical congealing</a> method에 대해서 간단히 말하자면 다음과 같다.</p><hr /><h1 id="what-is-congealing">What is congealing?</h1><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/210159138-2413884d-8cae-4102-9f4d-abf4589c980a.png" width="700" /></p><p>자주 봤던 MNIST(숫자 손글씨) 데이터셋이다. 단순하게 이를 0과 1로 구성된 데이터셋이라고 생각해보자. 같은 사람이 동일한 숫자를 쓰더라도, 해당 숫자는 약간의 픽셀 이동이나 기울기, 숫자의 가로 세로 크기 등등 다양한 형태를 가질 수 밖에 없다. 즉 단일 지표를 나타낼 수 있는 데이터셋의 분포 자체가 퍼져있다는 느낌인 것이다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/210159184-7d57918a-2cd4-41c7-bd61-4bd1ada9111e.png" width="500" /></p><p>위의 그림은 각각 $j^{th}$ image인 $I^j$가 transformation matrix $U^j$에 대해 변형된 이미지 $I^{j\prime}$를 의미하고, 해당 이미지에서의 각 pixel 위치에 대한 index $i$에 대한 지표화를 통해 각 이미지 상에서 동일한 위치 $i$를 $x_i^j$로 표현한 것이다. 모든 transformed digit image를 쌓아두고, 같은 위치의 픽셀들에 대해 empirical entropy(entropy estimation)을 진행한다.</p><p>[ \hat{H}(x_i) = -\left( \frac{N_0}{N} \log _2 \frac{N_0}{N} + \frac{N_1}{N} \log _2 \frac{N_1}{N} \right) ]</p><p>위의 식에서 notation $N_0$과 $N_1$은 각각 이미지 상에서 같은 픽셀 위치에서의 흰색 부분($1$)과 검은색 부분($0$)의 빈도를 나타낸 것이다. 이를 전체 이미지 개수로 나눠주기 때문에 확률값으로 매핑이 되는 것이다. 최적화하고자 하는 방향은 이 엔트로피를 최소화하여, <u>최대한 모든 이미지에 대한 transformation이 같은 형태의 이미지를 만들 수 있게끔</u> 하는 것이다.</p><hr /><h1 id="다시-돌아와서-abstract">다시 돌아와서, abstract</h1><p>이러한 framework 및 ideation 과정을 확장하여 만약 unalign된 데이터셋에 대해 <u>균일한 target mode를 생성할 수 있다면</u>, STN(Spatial Transform Network)를 학습할 수 있는 <strong>supervision</strong>으로 GAN의 mode를 사용할 수 있지 않을까?라는 아이디어가 된다. <br /> 실제로 이 방법을 사용하여 기존 SOTA 기준 precise correspondence(aligned된 부분의 일치하는 정도)가 약 $3 \times$ 증가했으며, 이 방법은 augmented reality나 image editing 혹은 다양한 GAN downstream task에서 학습될 수 있는 이미지 전처리에 활용될 수 있음을 보여주었다.</p><hr /><h1 id="why-gan-gealing">Why GAN-gealing?</h1><p>사실 이 부분은 논문의 introduction 부분을 정리하고자 만든 파트인데, 굳이 제목을 ‘왜 꼭 GAN-gealing이어야만 했는가?’로 지은 이유는, MNIST에서 사용된 기존 congealing 방식을 적용하기엔 복잡한 RGB 이미지에 대해서는 힘들기 때문이다.<br /> <u>Visual alignment</u>는 워딩에서 볼 수 있듯이 시각적인 정적 영상/동적 영상에 대해 correspondence(서로 다른 이미지에서 동일한 사물 찾기 혹은 위치 적용하기 등)을 찾거나 registration하는 문제로, 주로 computer vision에서 많이 다루는 <u>optical flow</u>, <u>3D matching</u>(depth estimation과 관련됨), <u>medical imaging</u> 혹은 <u>tracking and augmented reality</u>에 적용될 수 있다. <br /> 물론 최근 연구들로 하여금 pairwise alignment(image A로부터 image B)의 alignment는 쉬워졌지만, 여전히 모든 데이터셋에 대한 alignment는 크게 관심받지 못한 분야인 것은 여전하다. 예를 들어, 모든 데이터셋에 대해 자동으로 annotation을 해준다던지, 모든 이미지에 대해서 keypoint를 찾아낸다던지 등등 일반화된 <u>reference frame</u>이 필요한 경우에 대해서는 해결되지 못한 것이다. <br /> 그리고 FFHQ, AFHQ 그리고 CelebA-HQ와 같이 이미 얼굴 인식이나 crop/transform을 통해 <u>align된 이미지에</u> 대해서 generative model이 학습되었을 경우에 <u>더 높은 quality의 샘플을 만들어낼 수 있다</u>는 것이 여러 연구를 통해 입증되었기 때문에, 보다 global keypoint reasoning이나 alignment가 필요하게 되었다. <br /> 이 논문에서는 binary digit과 같은 단순한 modality에서만 적용될 수 있는 기존 congealing 방식의 아이디어에서 보다 넓은 dataset에 적용될 수 있는 GAN-gealing 방식을 제시한다. <br /> 뒤에서 보다 자세히 설명할 것이지만 간단하게 개요만 잡고 가자면 우선 사전 학습된 <u>GAN</u>의 생성 모델을(학습은 unaligned dataset에 대해 진행) 기반으로 random sample과 target sample을 spatial transformer networks(STN)의 supervision으로 활용, <u>STN의 파라미터</u>와 <u>target sample</u>을 동시에 최적화한다. 즉 STN의 학습 과정은 GAN과는 독립적으로(exclusively) 진행된다.</p><hr /><h1 id="related-works">Related works</h1><h2 id="gan-모델의-학습">GAN 모델의 학습</h2><p>GAN 종류의 generative model은 classification이나 segmentation, 그리고 representation learning이나 3D graphics에 이르기까지 다양한 computer vision 분야에서 활동되고 있다. 이처럼 해당 논문에서는 deep generative networks의 pre-trained된 joint distribution를 활용하고자 하는 것이다. 그러나 기존 GAN의 학습 과정과 같이 GAN 구조에서 먼저 이미지를 생성한 뒤, 이를 기반으로 discriminative network를 학습하는 것이 아니라, GAN-generated image와 discriminative model을 동시에 학습하는 형태가 바로 GAN-gealing이다. 조금 헷갈릴까봐 말하자면, generative network를 학습하는게 아니라 ‘GAN generated image’ 자체를 최적화하게 된다. 단순히 GAN이 생성한 이미지를 supervision으로 사용한 학습 형태가 되며, 그 어떤 픽셀 단위의 augmentation이나 domain knowledge에 따른 생성 데이터의 후처리를 진행하지 않고 사용한다.</p><h2 id="joint-image-set-alignment">Joint image set alignment</h2><p>이미지의 <u>평균</u>이라는 개념은 동일한 semantic content를 가지고 있는 image set에서의 통합적인 alignment를 시각화하기 위해 사용되었다. 대표적으로 지금 리뷰하고 있는 논문의 기초가 되는 congealing에서 unsupervised joint alignment(위에서 언급했듯, 단순히 image 간의 엔트로피를 최소화하는 방향으로 최적화를 진행)하는 방식이 이와 같다. 이러한 방법은 binary digit과 같이 형태가 잘 잡혀있는 데이터 구조에 대해서는 잘 작동하지만, 보다 다양한 형태의 데이터에 대해서는 쉽지 않다. 이런 문제들을 해결하기 위해 이후 연구에서는 고차원의 데이터셋을 low-rank의 subspace로 projection하는 연구나, image를 color, appearance 그리고 shape의 feature로 factorize해서 사용하는 등 데이터 단순화를 통한 연구가 진행되었다. <br /> 모든 연구들은 모든 image를 하나의 mode(데이터셋이 projection되는 <u>또다른 subspace</u>라고 생각하면 됨)로 표현 가능하다고 가정한다. 또다른 연구에서는 joint visual alignment 그리고 clustering 기법을 user-driven data를 통해 해결하였다. Unsupervision으로 alignment를 하는게 아니라 bounding box supervision을 활용하고, object 카테고리에 묶이는 여러 mode에 대해 cluster를 진행한다. 이를 자동화한 방식도 있지만, 특정 domain에만 한정되는 문제가 있다. 이렇게 각 이미지의 optimization을 해결하려한 연구 중에서 warping을 network가 예측할 수 있게 했던 연구가 있고, large-scale collection에 대한 clustering과 alignment를 가능케 했지만, 여전히 simple color transformation에 국한된다는 문제가 있다. 이러한 제약은 complex dataset에 대해서 paper가 가지는 assumption을 무력화한다는 단점으로 작용한다.</p><h2 id="spatial-transformer-networks">Spatial transformer networks</h2><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/210162591-a1184fcc-93ff-47f9-bec4-ca6c21ae6cde.png" width="700" /></p><p>다음으로 소개할 내용은 이 논문에서 GAN supervision으로 학습하고자 하는 주요 프레임워크이자, deep learning based data processing 관련 논문으로 유명한 <a href="https://arxiv.org/pdf/1506.02025.pdf">STN</a>이다. Deep learning framework를 사용하여 학습 가능한 geometric transformation의 파라미터를 학습하는 과정이다. Warpining에 사용되는 몇 개의 parameter를 딥러닝을 통해 학습하고, sampling grid를 input image에 대해 생성하고 warpining하는 과정을 미분 가능한 연산으로 정의하여 학습이 가능하게끔 하였다. STN framework를 모듈로 활용하여 여러 discriminative task에서 좋은 성능을 확인할 수 있었으며, robust filter learning, view-synthesis 그리고 3D representation learning에서 활용되기도 하였다. 이러한 방법들은 공통적으로 STN을 사용하여 또다른 task를 간접적으로 쉽게 만들고자 작용하는 constraint였지만, 이 논문에서는 STN을 직접 학습시키기 위해 GAN network를 사용한다는 점이 다르다.</p><hr /><h1 id="gan-기반-supervised-learning">GAN 기반 supervised learning</h1><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/210162569-06a79464-3c31-48f9-9831-ce2ddb04e6a2.png" width="800" /></p><p>이제 본격적으로 해당 논문의 학습 방법에 대해 자세히 살펴보도록 하자. 설명에 사용할 supervision pair $(x,~y)$는 기존 supervised learning에서 사용했던 바와 같이 source-target 관계에 놓여있다고 생각하면 된다. 우리는 <u>'STN'</u>을 학습시킬거니까, <strong>source image</strong>는 정렬이 안 된 이미지(위의 그림에서 아무렇게나 위치한 고양이)라고 생각하면 되고, <strong>target image</strong>는 정렬이 된 이미지(위의 그림에서 우측과 같이 잘 정렬된 고양이)라고 보면 된다. 이렇게 정렬된 데이터셋을 구성하는 과정에서 GAN generator가 생성한 이미지를 사용하게 되는 것이다. <br /> 먼저, $x$는 미리 학습된 GAN generator로부터 임의로 생성하는 sample이다. 그리고 $y$는 $x$의 latent code에 추가적으로 가해진 latent manipulation를 통해 생성된다.이렇게 pair는 STN network $f_\theta : x \rightarrow y$를 학습하는 supervision으로 작용한다.</p><p>[ \mathcal{L}(f_\theta,~y) = l(f_\theta(x),~y) <br /> ]</p><p>여기서의 $l$은 reconstruction loss이다. 말 그대로 정렬되지 않은 데이터 $x$를 정렬된 데이터 $y$로 만들고자 하는 것. 일반적인 supervised learning에서는 여기서 데이터 $x$와 $y$가 고정이지만, 신기하게도 GAN supervised learning에서는 고정된 데이터를 STN($f_\theta$) 최적화에 사용하는 것이 아니라 <u>target이 되는 $y$ 또한 end-to-end로 최적화된다</u>. 학습이 끝나고 나면 STN network $f_\theta$를 사용하여 GAN generated image가 아닌 real input에 대해 테스트하게 된다.</p><h2 id="dense-visual-alignment">Dense visual alignment</h2><p>그렇다면 도대체 어떤 방식으로 congealing을 진행해야 GAN-supervision을 사용할 수 있을지 알아보도록 하자. 이제부터 GANgealing이라고 불리는 알고리즘에 대한 엄밀한 분석이 들어간다. <br /> GANgealing은 가장 먼저 latent variable generative model $G$를 unaligned input dataset에 대해 학습하는 것으로 시작된다. 이때, input이 되는 latent vector는 $w \in \mathbb{R}^{512}$로 표현한다. 왜 $z$가 아니냐고 물어본다면, 나중에 말하겠지만 여기서 사용되는 generator는 styleGAN 기반이기 때문이다. 암튼 이렇게 학습된 $G$가 있다면, 여기서 source가 되는 input sample은 그냥 알고 있는 latent space $w \sim \mathcal{W}$에서 추출하고, generator의 생성된 샘플 $x = G(w)$로 정의하면 된다. 이제, 반대로 target image가 되는 $G(c)$를 정의한다. 여기서 $c$는 latend vector로 input에 사용된 $w$와 같은 dimension이 되며, 이를 STN의 target으로 사용하게 된다. 여기서 드는 의문점은 ‘$w$는 랜덤으로 뽑으면 되는데 $c$는 source가 되는 G(w)에 맞는 latent를 골라야하는데 어떻게 해요?’인데, 결국 앞서 말했던 것과 같이 $c$를 STN network 학습과 더불어 최적화하는 것이 목적이고, generator $G$는 input $c$에 대해 미분 가능한 함수이므로 gradient descent 방식을 사용해 학습 가능하다.</p><p>[ \mathcal{L}_\text{align}(T,~c) = l(T(G(w)), G(c)) <br /> ]</p><p>여기서 $T(\cdot) = f_\theta(\cdot)$으로 생각하면 된다. $l$은 두 이미지 사이에 작용되는 distance metric이 된다. 그림 상으로는 <u>perceptual loss</u>를 사용한 듯하다. 아무튼 이걸 latent vector $c$에 대해서 최소하하는 과정은 결국 <u>latent $c$로 하여금</u> 모든 생성된 이미지 데이터 $x$가 <u>spatial transformer($T$)를 통해 가까워질 수 있는 샘플</u>을 찾고자 하는 과정과 같다. 즉 $G(c)$는 모든 이미지 데이터 $x$의 alignment를 생성하고자 하는 목적으로 작용한다는 것. 만약 처음 $c$가 이런 조건을 만족하지 못하고 $T$를 통해 도저히 도달할 수 없는 이미지를 만들어낸다면, 이는 loss function의 작용에 의해 자동으로 최적화될 것이고, 같은 iteration을 계속 반복하다보면 어느샌가 $c$는 평균 이미지를 잘 생성할 수 있게 된다는 것. <br /> 이러한 단순한 접근은 상당히 그럴듯해 보이지만 dataset의 diversity가 적은 경우에만 가능하다는 단점이 있다. 왜냐하면 아무리 학습되더라도 모든 input image $x$는 같은 constant latent $c$에 의해 생성된 이미지 $G(c)$를 타겟으로 학습되기 때문에 최적화에 한계가 있다는 것이다. <br /> 아마 GAN inversion에 대해 실험해본 사람이면 알겠지만, 무작위로 sampling한 latent를 최적화하는 것보다는 <u>input image에 대해 가까운 sample를 기반으로 최적화하는 것</u>이 생성 이미지의 퀄리티를 높이기 좋은 걸 알 수 있을 것이다. 바로 요 아이디어를 적용하였다. 단순히 same target $G(c)$를 모든 randomly sampled image $G(w)$에 사용하는 것이 아니라, pose 및 orientation은 샘플 간에 유지하면서 디테일한 appearance는 $G(w)$를 가져가고 싶었기에, 이를 적용한 latent를 최적화한다. 사실 이 부분은 내가 작성했던 image manipulation글과 StyleGAN based GAN inversion 글을 보면 이해가 빠를텐데, StyleGAN에서 이미지를 만들어내는 방식이 <u>low resolution부터 latent를 constant에 입혀가면서 coarse style부터 점차 fine detail까지 생성하는 과정</u>이다. 따라서 적절한 부분에서 target vector $c$와 input sample을 생성할 때 사용한 random latent $w$를 조합하면, $\text{mix}(c, w) \in \mathbb{R}^{512}$가 곧 $c$의 전반적인 형태를 유지한 채로 $w$가 만드는 이미지의 디테일을 살리고자 하는 것이다.</p><p>[ \mathcal{L}_\text{align}(T,~c) = l(T(G(w)), G(\text{mix}(c,~w))) ]</p><p>실제 실험에서는 StyleGAN2를 generator로 사용했으며, 이를 사용함에 따라 StyleGAN이 가지고 있는 style-pose disentanglement를 활용할 수 있었다고 한다. 각 샘플마다의 생성되는 target image $G(\text{mix}(c, w))$를 style mixing을 사용하여 만들게 되고, 여기서 $c$는 synthesis generator의 첫 부분(coarse feature)을 담당하여 sample의 pose를 결정하게 되고, $w$는 later layer에 적용(fine feature)하여 texture를 결정하게 된다. 이러한 mixing point를 결정하는 것 또한 ablation을 진행하였다고 한다.</p><h2 id="stn-parameterization">STN parameterization</h2><p>STN 구조를 다시금 생각해보자. 논문을 읽지 않았다면 유감이지만, STN(spatial transformer network)는 다양한 연구로 뻗어가기 좋은 insight가 되기 때문에 아직 읽지 않았다면 꼭 읽어보기를 추천한다. 아무튼 spatial transformer 함수 $T$는 input으로 image를 받고, input image에 대한 sampling grid $g \in \mathbb{R}^{H \times W \times 2}$를 추출한다. 이때, 사용하고자 하는 목적에 따라 $g$의 parameter를 주게 된다. Gangealing 논문에서는 회전, scaling, 가로축 및 세로축 이동에 대한 transform을 주는 방식과, unconstrained(자유로운 변형)을 테스트해보았고, 결국 구성한 $T$는 <u>unconstrained STN</u>에 <u>similarity STN</u>(앞서 말했던 회전, scaling 등등)을 준 방식이 된다. 이러한 방식으로 저자들이 구성한 STN은 horizontal flip과 같은 변형도 가능하도록 학습되었다고 한다. <br /> Unconstrained $T$를 사용함에 있어서, “<u>total variational(TV)</u>” regularizer를 통한 정규화가 효과적이었다고 한다. Total variation은 보통 input에 대한 gradient를 구하고, 이에 대한 합(혹은 평균이 될 수도 있음)의 supremum으로 정의되는데, 그냥 간단히 표현하면 다음과 같다.</p><p>[ TV(x) = \sum_n \vert y_{n+1} - y_n \vert <br /> ]</p><p>여기서 total variance 거리를 정의하는 방식은 경우마다 다르고, 이 논문에서 regularizer로 sampling grid $g$가 투머치 변형되지 않게끔하기 위해 다음과 같은 loss를 최적화하는 과정에서 사용했다.</p><p>[ \mathcal{L}_\text{TV} (T) = \mathcal{L}_\text{Huber} (\Delta_x g) + \mathcal{L}_\text{Huber} (\Delta_y g) ]</p><p>식에서 알 수 있듯이, $g$의 각 방향($x,~y$)에 대한 partial derivative를 계산한다. 이에 추가로, regularizer $g$가 identity transformation으로부터 크게 벗어나지 않게 하기 위해 파라미터에 대한 norm을 다음과 같이 준다.</p><p>[ \mathcal{L}_I (T) = \parallel g \parallel^2_2 ]</p><h2 id="parameterizationn-of-c">Parameterizationn of $c$</h2><p>앞서 target이 되는 이미지를 만들기 위해 사용되는 latent vector $c$를 언급했었는데, 이 $c$를 단순히 미분 가능한 generator network $G$를 통해 연산된 backpropagation을 진행하는 것이 아니라, 조금 다른 방법을 쓴다. 그대신, $\mathcal{W}$ space의 top-$N$ principal direction의 linear combination으로 정의하는 형태로 parameterize한다.</p><p>[ c = \bar{w} + \sum_{i=1}^N \alpha_i d_i <br /> ]</p><p>여기서 principal direction은 우리가 많이 알고 있는 PCA(주성분 분석)을 통해 추출된 축과 같은 의미다. 위의 식에서 $\bar{w}$는 $w$ vector의 empirical mean이고, $d_i$는 $i$번째 principal direction이다. 그리고 $\alpha_i$는 학습 가능한 스칼라 coefficient로, 주성분 방향 $N$개에 대해 각각 따로 적용되는 파라미터라고 보면 된다. 이런 방식으로 $c$를 최적화하는 이유는 StyleGAN의 $\mathcal{W}$ space가 표현력이 뛰어나다는 특징을 가지기 때문이다. 만약 추가로 constraints를 주지 않고 $c$를 최적화하게 되면 <u>target image의 품질이 저하</u>되고, natural image(실제 이미지)가 분포하는 <u>manifold로부터 멀어지는 경향성</u>을 보인다. 주성분 축 $N$의 개수를 줄인다는 것은, 그만큼 $c$의 feasible direction을 줄이는 것이 되고, 이를 다르게 풀어쓰면 보다 $\mathcal{W}$-mean space에서의 constraint를 주는 것과 같기 때문에 바로 앞서 언급한 manifold 문제를 해결할 수 있다. <br /> 최종으로, 이 논문에서 사용한 GANgealing objective는 다음과 같이 정리할 수 있다.</p><p>[ \mathcal{L}(T,~c) = \mathbb{E}_{w \sim \mathcal{W}} (\mathcal{L}_\text{align} (T, c) + \lambda_\text{TV} \mathcal{L}_\text{TV} (T) + \lambda_I \mathcal{L}_I (T)) ]</p><p>실험에 사용했던 hyperparameter인 람다는 $\lambda_\text{TV} = 1000,~2500$이고, loss weight $\lambda_I$는 1이다.</p><h1 id="joint-alignment-and-clustering">Joint alignment and clustering</h1><p>GANgealing을 지금까지 설명해오면서 묘사했던 것은 LSUM Bicycles나 Cats와 같은 여러 multimodal data에 대해 잘 적용할 수 있다는 것이다. 그러나 LSUN Horses와 같은 몇몇 데이터셋처럼 data에서 <u>single mode로 표현되기 힘들 정도로 다양한 pose를 가진 경우</u>가 있다. 이러한 어려운 상황에 대처하기 위해 GANgealing 기법은 단일 target latent $c$에 대해 학습하는 것이 아닌, clustering algorithm에 적용될 수 있다. <br /> 예를 들어 우리가 학습시키고자 하는 $c$ vector의 개수를 $K$라고 하자. 각 latent constant $c$는 data의 특정 mode를 포착하게 되므로(여기서 mode는 쉽게 이해하자면, 특정 데이터가 가지고 있는 feature를 풀어쓴 것으로 이해하면 된다) 여러 개의 $(c_k)_{k=1}^K$를 학습하는 것이 dataset에 대한 여러 mode를 학습할 수 있는 방법이 된다. 앞서 언급했던 것과 같이 $c$를 최적화하는 방식은 $\mathcal{W}$ space의 top-$N$ principal direction의 linear combination으로 학습되므로, 각 $c_k$에 대해서 상승되는 $\alpha$ coefficients가 학습된다. 이와 유사하게 논문에서 사용한 방식은 $K$개의 개별적인 Spatial Transformer $T_k$가 있고, 각 spatial transformer가 각 mode를 학습하는 형태가 된다. 이런 식으로 수정한 GANgealing 구조는 각 cluster에서 모든 이미지 사이의 dense correspondence를 학습할 수 있게 된다. 여기서 중요한 건 그럼 각 $c_k$와 그에 상응하는 $T_k$ pair가 특정 mode에 대해 최적화되게 guidance를 해줘야하는데,</p><p>[ \mathcal{L}_\text{align}^K (T, c) = \min_k \mathcal{L}_\text{align} (T_k, c_k) <br /> ]</p><p>이를 위해 hard-assignment step을 주어 각 mode에 따라 unaligned image를 align할 수 있게 하였다. <br /> $K = 1$인 케이스는 계속 설명해왔던 unimodal case에 속한다고 생각하면 된다. Test time에는 input fake image $G(w)$를 상응하는 cluster index인 $k^* = \arg \min_k \mathcal{L}_\text{align} (T_k,~c_k)$를 찾고, 이에 상응하는 spatial transformer $T_{k^*}$로 warping하게 된다. 그러나 문제가 되는 것은 바로 실제 이미지에 대해서 연산을 해야할 경우인데, 왜냐하면 mode assignment을 하는 과정은 앞서 말했던 바와 같이 $\mathcal{L}_\text{align}$을 계산해야하고, 이는 <u>input image</u>가 아닌 input image를 생성한 <u>$w$ vector</u>를 알아야하기 때문이다. <br /> 사실 처음에 이 부분이 이해가 잘 안됐는데, 식을 다시 살펴보니 $K$에 대해 clustering을 한 목적 자체는 여러 modality에 대한 최적화를 위함이고, 각 $c$는 결국 input image에 대한 latent $w$와 함께 mixing 및 최적화되는 구조가 되므로 최종적으로는 real image에 대한 연산 과정에서도 latent variable을 알아야한다는 것이다. 이를 해결하는 가장 직관적인 방법으로는 GAN inversion이 있고, 이는 앞서 작성했던 <a href="https://6unoyunr.github.io/blog/imagemanipulate">GAN 관련 게시글</a>에서도 언급했었지만, image $x$에 가까운 샘플을 만들 수 있는 latent variable $w$를 찾는 것이 목적이다. 하지만 StyleGAN은 FFHQ(얼굴 이미지) 데이터셋으로 학습되었기 때문에 <u>얼굴이 아닌 dataset</u>의 정확한 GAN inversion은 상당히 challenging하고 느린 작업이다. <br /> 따라서 저자들은 조금 다른 방법을 사용했는데, 그것은 바로 input image의 cluster assignment를 classification 결과로 낼 수 있는 네트워크를 학습한 것이다. 간단하게도 input fake image에 대한 target cluster $k^*$를 데이터셋으로 사용 가능하므로 $(G(w),~k^*)$를 하나의 supervision으로 학습하게 되면 굳이 GAN inversion을 통하지 않고서도 임의의 이미지에 대해 assignment가 가능해진다. Classifier는 spatial transformer의 weight를 사용하되, warping head를 classification head로 사용하여 최적화한다. 이러한 방식을 통해 fake sample에 대해 최적화된 sptial transformer, classifier 모두 real image에 좋은 일반화 성능을 보여주었다.</p><p align="center"> <img src="https://user-images.githubusercontent.com/79881119/210498660-f6d0b978-b4b6-49e3-9569-9ce755c9b37b.png" width="1000" /></p><hr /><h1 id="끝으로">끝으로…</h1><p>여기까지가 GANgealing에 대한 설명이었고, 실험 내용이나 결과에 대해선 언급하지 않고 끝내도록 하겠다. 사실 최근에 계속 논문을 읽고자하는 방향은 논문에서 구체적으로 어떤 실험을 통해 어떤 결과를 내었는지보다는, 이 논문의 아이디어를 디벨롭하는 과정에서 사용된 insight(related works, preliminary)와 수학적 배경이 조금 더 중요하다고 생각했기 때문이다. 아무튼 결론적으로 말하자면 이 논문은 GAN을 학습한 연구가 아닌 GAN을 이용해 STN을 학습, 이를 통해 효과적으로 unaligned real image를 alignment할 수 있는 방법을 제시했다는 것이 주된 contribution이 될 것 같다. 굉장히 흥미로운 주제였고 광범위하게 사용될 수 있을 것 같다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/trnmultimodal"><p id="prev_title"> ❮❮ Transformer와 Multimodal에 대하여</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/clip"><p id="next_title"> ❯❯ CLIP(Learning transferable visual models from natural language supervision)에 대하여</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="6unoyunr/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/201962047?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/reasoning">추론 모델은 정말로 뛰어난가?? 오픈소스 엑사원 딥(EXAONE-Deep), QWQ, 딥시크(Deepseek-R1) 비교 실험해보기</a></li><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
