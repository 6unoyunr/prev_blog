<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:description" content="Discord bot, Chatting bot"><meta name="og:image" content="https://github.com/junia3/junia3.github.io/assets/79881119/2234213e-ad88-430d-8f8b-150ebae3169a"><meta name="og:image:alt" content="Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:url" content="http://localhost:4000/blog/chatbot"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="twitter:description" content="Discord bot, Chatting bot"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://github.com/junia3/junia3.github.io/assets/79881119/2234213e-ad88-430d-8f8b-150ebae3169a"> <!-- Search Engine --><meta name="description" content="Discord bot, Chatting bot"><meta name="image" content="https://github.com/junia3/junia3.github.io/assets/79881119/2234213e-ad88-430d-8f8b-150ebae3169a"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="author" content="JY"/><meta itemprop="description" content="Discord bot, Chatting bot"><meta itemprop="image" content="https://github.com/junia3/junia3.github.io/assets/79881119/2234213e-ad88-430d-8f8b-150ebae3169a"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a> <!--<hr class="navbar-divider"> <a class="navbar-item"> Report an issue </a> --></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/chatbot" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">(LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/2234213e-ad88-430d-8f8b-150ebae3169a" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Discord bot, Chatting bot</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>September 20, 2023</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Automatic system</span> <span class="tag is-link">Chatting</span> <span class="tag is-link">LLM</span> <span class="tag is-link">NLP</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 13 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="과연-내가-만든-봇은-정말로-크롤러였던-것인가">과연 내가 만든 봇은 정말로 크롤러였던 것인가?</h1><p>본인은 크롤링을 단 한번도 수행해보지 않은 채로 디스코드 봇을 통해 긱뉴스를 자동으로 업데이트하겠다는 당찬 포부(?)를 담고 지난 글에서 디스코드 봇을 열심히 만들었었다. 굉장히 흡족한 상태로 침대에 누워서 구글 검색에 “크롤링”을 치자 바로 다음과 같은 글이 등장했다. 혹시라도 읽어보실 분들은 매우 추천하는 글이라서 링크 올려드림(<a href="https://velog.io/@mowinckel/%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81-I">글 링크</a>).</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/213d7b26-82f7-4d8a-9b41-f351b76e1c42" width="800" /></p><p>말 그대로 현존하는 파이썬 모듈 가져다가 대충 홈페이지 프론트에서 주어진 정보만 야금야금 빼내서 가져오는 <code class="language-plaintext highlighter-rouge">selenium</code> 그리고 <code class="language-plaintext highlighter-rouge">BeautifulSoup</code> 기반의 크롤러는 사실상 크롤러라고 부르면 안된다는 것이었다. 뒤통수를 한 대 맞은 기분이었지만 결국 내가 내린 결론은 <strong><em>내가 만든 디스코드 봇은 크롤러가 아니라는 것</em></strong>이다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/ad388a09-0d29-4c55-b8c9-9ea8a2f2c5a2" width="600" /></p><p>백엔드의 ‘백’자도 제대로 짚어보지도 않은 오만한 내 자신을 반성하고자 만든 봇 이름을 조금 수정하기로 했다. 원래는 <code class="language-plaintext highlighter-rouge">crawler</code> 라는 이름을 당당하게 붙였는데 이제는 그러면 안된다는 생각에..</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/71e515f5-d3ec-46ec-beda-aecbccca741a" width="500" /></p><p>이제부터 이 친구 이름은 <strong>‘기어다니는 고양이’</strong>로 정했다. 암튼 크롤러는 아님. 그런데 아무리 생각해도 너무 자존심이 상하는 문제였다. 나는 나름 인공지능을 연구한다고 대학원까지 등록금을 몇백만원씩 내며 다니고 있는데, 내 전공을 살려서 자격을 잃은 고양이 친구의 위상을 다시 올리고 싶었다. 원래 계획은 단순한 긱뉴스 크롤러였는데 내가 어쩌다가 이렇게 됐는지는 모르겠다.</p><hr /><h1 id="무료로-사용할-수-있는-언어-모델은-없을까">무료로 사용할 수 있는 언어 모델은 없을까?</h1><p>세상엔 수많은 챗봇이 존재한다. 챗봇은 모두 Large Language Model(LLM)을 기반으로 한다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/b1b3705a-e3c3-43b0-ba22-63939decffc7" width="700" /></p><p>챗지피티를 포함하여 여러 기업들의 언어 모델은 각각 API 프라이싱 정책이나 open source 유무에 따라 많은 차이가 있다. 나는 아직 돈이 없는 대학원생이기에 <strong><em>굳이 돈을 내고 써야하는</em></strong> 챗지피티 서비스 대신 조금 더 현실적인 친구를 도입하기로 했다. 바로 meta에서 최근 공개한 오픈 소스 LLM 중 하나인 <a href="https://github.com/facebookresearch/llama">Llama</a>이다. 놀랍게도 Llama2는 오픈 소스이므로 누구나 마음대로 가져다가 파인 튜닝이 가능하다. 하지만 Llama2도 거대 언어 모델인 이상 리소스의 지옥을 벗어날 수는 없었다. 가장 적은 수의 파라미터가 $7B$만큼 필요한데, 이게 어느 정도의 수치냐면 완벽하게 사용하기 위해서는 28GB의 GPU RAM이 필요하다. 대형 모델인 70B를 쓰려면 이보다 많은 양의 GPU가 필요한데, 현실적으로 우리가 chat이 가능한 디스코드 봇을 만들자고 이 정도 사이즈의 언어 모델을 쓸 수는 없지 않은가?</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/2bc0f108-4484-4714-8fb2-b1d30a337293" width="700" /></p><p>그렇기 때문에 GPU RAM 대신 CPU를 사용해서 인퍼런스가 가능한 오픈 소스를 활용하고자 했다.</p><hr /><h1 id="llama-2-with-cpp--python">LLAMA-2 with cpp + python</h1><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/549a9258-6fbb-491c-9e57-99898b45b62c" width="600" /></p><p><a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>, <a href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a> 정말 세상에는 똑똑한 사람들이 많다. 대형 언어 모델을 low level과 high level로 접근할 수 있는 방법을 제안하여, 간단하게는 GPU 메모리가 많지 않은 데스크톱과 노트북을 포함한 윈도우/맥 기반의 여러 디바이스에서 돌릴 수 있게 C++로 경량화시킨 것이다. 어떻게 이 정도의 경량화가 가능한 것일까?</p><p>대형 모델은 일반적으로 비싼 GPU를 필요로 하는데 사실 GPU는 큰 메모리 대역폭과 계산 능력 때문에 딥러닝에서는 유리하지만, 메모리 대역폭이 종종 inference 단계에서 bottleneck으로 작용하게 된다. 왜냐하면 실제 연산을 목적으로 한다면 HBM 메모리(RAM)에서 온칩 메모리로 옯겨야 하기 때문이다. 또한 LLaMa 가중치를 위한 램 사용량에 있어서 Quantization(양자화)가 중요한데, 이때 precision과의 적당한 타협선을 찾아 양자화를 통해 모델을 저장하는 데 필요한 메모리 양을 줄여 표준 데이터센터 GPU 및 고급 소비자 GPU에 메모리에 맞출 수 있게 한다. 사실상 딥러닝에서 distillation이 가능한 이유/더 좋은 성능이 나오곤 하는 이유와 관련된다고도 생각해볼 수 있다. 진짜 이제는 가벼운 라마가 등장해버린 것이다. 얼마나 AI가 보급되기 편한 세상이 왔는가? Install하는 방법은 정말 간단하다. 참고로 본인은 우분투라서 우분투 기준으로 작성하고 있기는 하지만, 아마도 윈도우에서도 가능하지 않을까 싶다. 물론 위의 깃허브 링크를 들어가보면 윈도우에서도 빌드하는 과정을 리드미에 적어뒀기 때문에 그대로 따라하면 된다.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>llama-cpp-python
</code></pre></div></div><p>본인은 별다른 추가 작업 없이 위의 코드만으로도 해당 모듈을 사용하는데 큰 문제가 없었다. 리드미에 나와있는 모델 사용법 예시는 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"./models/7B/llama-model.gguf"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="s">"Q: Name the planets in the solar system? A: "</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s">"Q:"</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">],</span> <span class="n">echo</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">llama_cpp</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">ctypes</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_backend_init</span><span class="p">(</span><span class="n">numa</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Must be called once at the start of each program
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">params</span> <span class="o">=</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_context_default_params</span><span class="p">()</span>
<span class="c1"># use bytes for char * params
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_load_model_from_file</span><span class="p">(</span><span class="sa">b</span><span class="s">"./models/7b/llama-model.gguf"</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_new_context_with_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">max_tokens</span> <span class="o">=</span> <span class="n">params</span><span class="p">.</span><span class="n">n_ctx</span>
<span class="c1"># use ctypes arrays for array params
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_token</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">))()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_tokenize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="sa">b</span><span class="s">"Q: Name the planets in the solar system? A: "</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">,</span> <span class="n">add_bos</span><span class="o">=</span><span class="n">llama_cpp</span><span class="p">.</span><span class="n">c_bool</span><span class="p">(</span><span class="bp">True</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">llama_cpp</span><span class="p">.</span><span class="n">llama_free</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</code></pre></div></div><p>위의 코드는 high level, 아래 코드는 low level 형식의 파이썬 코드가 되겠다. 굳이 저자가 high level로 옮겨놓은 코드를 아래와 같이 쓸 필요는 없기 때문에, 위의 코드 형태를 그대로 사용하도록 하겠다.</p><hr /><h1 id="chat에-최적화된-모델-가져오기">Chat에 최적화된 모델 가져오기</h1><p>Llama cpp를 사용하는데 있어서 pretrained 형태는 기존 meta에서 오픈한 모델이 아닌 <code class="language-plaintext highlighter-rouge">.gguf</code> 를 찾아서 다운받으면 된다. <a href="https://huggingface.co/TheBloke">Huggingface</a>에 들어가보면 잘 나와있다. 참고로 ‘채팅’이 자연스럽게 가능한 언어 모델은 description 상에서 chat이란 단어가 붙어있어야한다. 본인은 <code class="language-plaintext highlighter-rouge">llama-2-13b-chat.Q5_K_M.gguf</code>를 사용하였다. 파일을 다운받아서 적당한 경로에 위치시킨뒤 불러오면 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"./models/llama-2-13b-chat.Q5_K_M.gguf"</span><span class="p">)</span>
</code></pre></div></div><p>위에서 설명한 대로 해당 llm에 query를 QnA 형태로 집어넣게 된다. 그런데 여기서 주의할 점은 프롬프트 엔지니어링이다.</p><hr /><h1 id="chatbot에-프롬프트-엔지니어링-하기">Chatbot에 프롬프트 엔지니어링 하기</h1><p>본인이 설계한 봇은 crawling_kitty라는 이름을 가지고 있고 고양이라는 정체성이 있어야하는데, 이를 무시한다면 예컨데 “<strong>너의 이름이 뭐니?</strong>” 라는 질문에 “<strong>전 라마인데용.</strong>” 이라는 생뚱맞은 대답이 돌아올 수 있다. 이러한 문제를 없애기 위해 기본적으로 query에 들어갈 프롬프트 엔지니어링이 간단하게 필요하다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bot_prompt</span> <span class="o">=</span> <span class="s">"You are a bot named kitty and 5-year old. Please answer gently.</span><span class="se">\n</span><span class="s">"</span>
</code></pre></div></div><p>또한 QnA 형태로 대화를 주고 받는 과정에서 ‘이전 대화의 내용’이 이후의 대화에도 어느 정도 영향을 끼쳐야 한다. 예를 들면 만약 챗봇이 현재 question에 대한 정보만 가진다면</p><ul><li>Me : From now on, translate my word into English</li><li>Bot : Okay, go ahead.</li><li>Me : 나는 딥러닝 연구를 좋아하고, 초밥 먹는 것을 좋아한다.</li><li>Bot : 정말 좋은 취미 생활이네요!</li></ul><p>위와 같이 영어로 번역해달라는 이전 지시를 무시한 채로 이후 답변을 작성하게 되는 것이다. 이를 In-context learning 능력이라고 부르는데 일단 이건 무시하자. 암튼 메모리 문제로 많은 대화 내용을 한번에 전달시키지는 못하지만 어느 정도는 앞뒤 맥락을 포함시켜서 query를 만들어야한다는 뜻이다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat_log</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
<span class="n">log_maxlen</span> <span class="o">=</span> <span class="mi">3</span>
</code></pre></div></div><p>따라서 queue 구조를 만들고, 대화 로그의 최대 길이를 설정한 뒤 지속적으로 대화 내용을 업데이트하도록 했다.</p><hr /><h1 id="discord-bot-커맨드-메소드-만들기">Discord bot 커맨드 메소드 만들기</h1><p>이제 남은 것은 명령어 형식을 만드는 것이다. 기존 어프로치랑 동일하게 슬래시 + chat + “내가 질의할 내용”을 입력하면 해당 내용이 프롬프트 엔지니어링을 거쳐서 model로 들어가게 된다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bot</span><span class="p">.</span><span class="n">command</span><span class="p">()</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">chat</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
    <span class="n">chatting</span> <span class="o">=</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">chat_log</span><span class="p">)</span>
    <span class="n">current_chat</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Q : </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">.</span><span class="se">\n</span><span class="s">A : "</span>
    <span class="n">chatting</span> <span class="o">+=</span> <span class="n">current_chat</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="n">bot_prompt</span><span class="o">+</span><span class="n">chatting</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s">"Q:"</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">],</span> <span class="n">echo</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s">"choices"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"text"</span><span class="p">].</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">replace</span><span class="p">(</span><span class="s">"A : "</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">chat_log</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">log_maxlen</span><span class="p">:</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_chat</span><span class="o">+</span><span class="n">answer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">popleft</span><span class="p">()</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_chat</span><span class="o">+</span><span class="n">answer</span><span class="p">)</span>

    <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</code></pre></div></div><p>chat_log의 모든 내용들을 <code class="language-plaintext highlighter-rouge">chatting</code> 이라는 하나의 str로 조인한다. 그리고 현재 질의 응답 내용에서 사용자의 query를 Question 형태로, 그리고 모델이 채울 곳은 Answer 형태로 빈칸으로 남긴다. Output은 질의 응답을 챗봇이 맘대로 채우지 못하게 <code class="language-plaintext highlighter-rouge">Q:</code>나 <code class="language-plaintext highlighter-rouge">줄넘김</code>이 등장하면 멈추도록 한다. 답변은 앞서 함께 넣어주는 string 전체를 반환하게 되므로 이 중 현재 질의 응답에 대한 내용만 따로 추출한 뒤, Answer 내용만 따로 빼낸다.</p><p>그리고 만약 현재 대화 로그의 길이가 최대 길이라면 가장 먼저 들어와있는 대화 내용을 <code class="language-plaintext highlighter-rouge">popleft()</code>로 지워내고, <code class="language-plaintext highlighter-rouge">append()</code>를 통해 현재 질의 응답을 다음 query에 사용하게 된다. 만약 본인이 디스코드 봇을 사용하지 않고 있는 상황이라면, 다음 paragraph를 참고하면 된다.</p><hr /><h1 id="discord-bot말고-단순히-파이썬에서-챗봇-형태-구현해보기">Discord bot말고, 단순히 파이썬에서 챗봇 형태 구현해보기</h1><p>앞서 사용한 구조를 거의 그대로 사용할 것이다. 그런데 이번에는 디스코드에서 input을 받아오는게 아니라 파이썬 자체 메소드인 <code class="language-plaintext highlighter-rouge">input()</code>을 사용해볼 것이다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="n">bot_prompt</span> <span class="o">=</span> <span class="s">"You are a bot named kitty and 5-year old. Please answer gently.</span><span class="se">\n</span><span class="s">"</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s">"./models/llama-2-13b-chat.Q5_K_M.gguf"</span><span class="p">)</span>
<span class="n">chat_log</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
<span class="n">log_maxlen</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">def</span> <span class="nf">chat</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">chatting</span> <span class="o">=</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">chat_log</span><span class="p">)</span>
    <span class="n">current_chat</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Q : </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">.</span><span class="se">\n</span><span class="s">A : "</span>
    <span class="n">chatting</span> <span class="o">+=</span> <span class="n">current_chat</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="n">bot_prompt</span><span class="o">+</span><span class="n">chatting</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s">"Q:"</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">],</span> <span class="n">echo</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s">"choices"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"text"</span><span class="p">].</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">replace</span><span class="p">(</span><span class="s">"A : "</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">answer</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">"You &gt;&gt; "</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">chat</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Bot &gt;&gt; </span><span class="si">{</span><span class="n">answer</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div><p>결과는 대충 다음과 같다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/1693dad6-9a4c-4579-836b-a94e5abf8015" width="600" /></p><p>얼추 대화가 진행되고 있는 것을 볼 수 있다. 대충 각자 컨셉에 맞는 프롬프트 엔지니어링을 통해 사용하면 되지 않을까 싶다. 잘 보이지 않을까 싶어 대화 내용만 따로 빼면 다음과 같다.</p><ul><li>나 : What’s your name?</li><li>봇 : Meow, my name is Kitty! <em>purr</em></li><li>나 : Hello, nice to meet you Kitty!</li><li>봇 : Meow meow! <em>bats eyelashes</em> Purrr… Hi there! I’m so happy to meet you too! <em>twirls tail</em> Are you here to play?</li></ul><p>움.. 컨셉에 너무 잡아먹힌 친구가 되어버린 기분이랄까. 프롬프트 엔지니어링을 좀 더 해야겠다.</p><hr /><h1 id="optional-챗봇으로-논문-찾는-기능-넣기">(Optional) 챗봇으로 논문 찾는 기능 넣기</h1><p><code class="language-plaintext highlighter-rouge">Selenium</code>을 사용하여 구글 검색을 대신 해주고, 가장 관련도 높은 아카이브 논문을 찾아주는 기능을 추가했다. 이건 LLM을 활용한 챗봇은 아니고 단순히 모듈을 사용한 간단한 기능이다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bot</span><span class="p">.</span><span class="n">command</span><span class="p">()</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">paper</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
    <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="sa">f</span><span class="s">"알겠습니다! </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">에 대한 아카이브 논문을 찾아볼게요 🤗"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">found</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">baseUrl</span> <span class="o">=</span> <span class="s">'https://www.google.com/search?q='</span>

        <span class="k">if</span> <span class="s">"paper"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">query</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">baseUrl</span> <span class="o">+</span> <span class="n">quote_plus</span><span class="p">(</span><span class="n">query</span><span class="o">+</span><span class="s">" paper"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">baseUrl</span> <span class="o">+</span> <span class="n">quote_plus</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="n">driver</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        <span class="n">driver</span><span class="p">.</span><span class="n">implicitly_wait</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">html</span> <span class="o">=</span> <span class="n">driver</span><span class="p">.</span><span class="n">page_source</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="s">'html.parser'</span><span class="p">)</span>

        <span class="c1"># Extract search results
</span>        <span class="n">result_links</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">search_results</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'.yuRUbf'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">search_results</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">search_results</span><span class="p">:</span>
                <span class="n">result_title</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">'h3'</span><span class="p">)</span>
                <span class="n">result_link</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">'a'</span><span class="p">)[</span><span class="s">'href'</span><span class="p">]</span>
                <span class="k">if</span> <span class="s">"arxiv.org"</span> <span class="ow">in</span> <span class="n">result_link</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">result_link</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result_links</span><span class="p">:</span>
                        <span class="n">result_links</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_link</span><span class="p">)</span>
                        <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="sa">f</span><span class="s">"### 제목: </span><span class="si">{</span><span class="n">result_title</span><span class="p">.</span><span class="n">text</span><span class="si">}</span><span class="se">\n</span><span class="s">### 링크: </span><span class="si">{</span><span class="n">result_link</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                    <span class="n">found</span> <span class="o">=</span> <span class="bp">True</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">found</span><span class="p">:</span>
                <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="s">"검색 결과가 없습니다 🥲"</span><span class="p">)</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="s">"검색 결과가 없습니다 🥲"</span><span class="p">)</span>
    
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s"> 🥲 ..."</span><span class="p">)</span>
</code></pre></div></div><p>그냥 혹시라도 필요한 사람이 있을까 싶어 만들었다. 결과는 다음과 같다.</p><hr /><h1 id="디스코드-봇-실행-모습">디스코드 봇 실행 모습</h1><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/ec8aad2b-6323-4ffc-b80c-16088bb673b5" width="600" /> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/79f8dc82-945c-4ea7-8796-2d17f5560628" width="600" /></p><p>쿼리와 관련된 아카이브 논문들을 찾아서 링크와 함께 보내준다. 그리고 대화도 제대로 이어나가는 모습을 볼 수 있다. 근데 영어로 대화해야 좀 자연스럽게 대화가 된다..</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/5565c5c8-c614-4c0b-81b6-da8e2bb53bfd" width="600" /></p><p>ㅋㅋㅋㅋㅋㅋ 아니야 넌 잘 못할거야.. 그냥 영어로 하자 우리</p><hr /><h1 id="echo-옵션-제외하기">Echo 옵션 제외하기</h1><p>사실 <code class="language-plaintext highlighter-rouge">Echo</code> 옵션을 제외하게 되면, 오직 prompt 이후의 답변에 대해서만 리턴할 수 있다. 그래서 굳이 위에서 했던 것처럼 output하드 코딩할 필요 없이 다음과 같이 옵션을 조정하게 되면</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bot</span><span class="p">.</span><span class="n">command</span><span class="p">()</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">chat</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
    <span class="n">chatting</span> <span class="o">=</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">chat_log</span><span class="p">)</span>
    <span class="n">current_chat</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Q : </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">.</span><span class="se">\n</span><span class="s">A : "</span>
    <span class="n">chatting</span> <span class="o">+=</span> <span class="n">current_chat</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="n">bot_prompt</span><span class="o">+</span><span class="n">chatting</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s">"Q : "</span><span class="p">],</span> <span class="n">echo</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s">"choices"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">"text"</span><span class="p">]</span>

    <span class="k">print</span><span class="p">(</span><span class="n">chatting</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">chat_log</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">log_maxlen</span><span class="p">:</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_chat</span><span class="o">+</span><span class="n">answer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">popleft</span><span class="p">()</span>
        <span class="n">chat_log</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_chat</span><span class="o">+</span><span class="n">answer</span><span class="p">)</span>

    <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>

</code></pre></div></div><p>같은 기능을 보다 심플하게 구현할 수 있다. 연산상의 로드 차이는 전혀 없음.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/crawlerbot"><p id="prev_title"> ❮❮ 특정 홈페이지에서 원하는 정보 크롤링하는 디스코드 봇 만들기</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/chatbotgpu"><p id="next_title"> ❯❯ (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/79881119?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
