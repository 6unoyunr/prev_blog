<!DOCTYPE html><html><head><head> <!-- Include Meta Tags Here --><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="viewport" content="width=device-width, height=device-height, initial-scale=1 user-scalable=no, shrink-to-fit=no"><meta content='#000000' name='theme-color'/><meta name="keywords" content="AI, Developer, Research engineer"><title>Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기</title><!-- Open Graph general (Facebook, Pinterest & Google+) --><meta name="og:title" content="Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:description" content="Discord bot, Chatting bot"><meta name="og:image" content="https://github.com/junia3/junia3.github.io/assets/79881119/97ca9bcd-4822-4782-8ab6-4ddac8068abe"><meta name="og:image:alt" content="Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:url" content="http://localhost:4000/blog/chatbotgpu"><meta name="article:author" content="https://www.facebook.com/"><meta name="og:site_name" content="Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="og:type" content="website"> <!-- Twitter --><meta property="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="twitter:description" content="Discord bot, Chatting bot"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@"><meta name="twitter:image:src" content="https://github.com/junia3/junia3.github.io/assets/79881119/97ca9bcd-4822-4782-8ab6-4ddac8068abe"> <!-- Search Engine --><meta name="description" content="Discord bot, Chatting bot"><meta name="image" content="https://github.com/junia3/junia3.github.io/assets/79881119/97ca9bcd-4822-4782-8ab6-4ddac8068abe"> <!-- Schema.org for Google --><meta itemprop="name" content="Welcome to my blog | (LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기"><meta name="author" content="JY"/><meta itemprop="description" content="Discord bot, Chatting bot"><meta itemprop="image" content="https://github.com/junia3/junia3.github.io/assets/79881119/97ca9bcd-4822-4782-8ab6-4ddac8068abe"> <!-- Global site tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KFNS88G1GM"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-KFNS88G1GM'); </script><title>Welcome to my blog</title><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href=/assets/external/font-awesome-4.7.0/css/font-awesome.css><link rel="stylesheet" href="/assets/css/style_dark.css"><link rel="stylesheet" href="/assets/css/style.css"> <script src="https://kit.fontawesome.com/6a97161b76.js" crossorigin="anonymous"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5834956759419182" crossorigin="anonymous"></script><link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png"><link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#FFFFFF"></head><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\[','\]'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="load"> <img src="/assets/images/loading.gif" alt="loading"></div><script > const loading_page = document.getElementById("load"); window.onload = function(){ loading_page.style.display = 'none'; } </script></head><body><nav class="navbar is-black is-fixed-top" role="navigation" aria-label="main navigation" id="navbar"><div class="container"> <!-- logo or branding image on left side --><div class="navbar-brand"> <a class="navbar-item" href="http://localhost:4000/"> <strong>Welcome to my blog</strong> </a><div class="navbar-burger" data-target="navbar-menu"> <span></span> <span></span> <span></span></div></div><!-- children of navbar-menu must be navbar-start and/or navbar-end --><div class="navbar-menu has-background-black" id="navbar-menu"><div class="navbar-end"> <a class="navbar-item " href="http://localhost:4000/">HOME</a> <a class="navbar-item" href="http://localhost:4000/#about">ABOUT</a> <a class="navbar-item" href="http://localhost:4000/#contact">CONTACT</a> <a class="navbar-item " href="http://localhost:4000/cv">CV</a> <a class="navbar-item " href="http://localhost:4000/blog">POST</a><div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> CATEGORY </a><div class="navbar-dropdown has-background-black is-left"> <a href="http://localhost:4000/category/deep%20learning" class="navbar-item has-text-grey-light "> DEEP LEARNING </a> <a href="http://localhost:4000/category/development" class="navbar-item has-text-grey-light "> DEVELOPMENT </a> <a href="http://localhost:4000/category/github%20blog" class="navbar-item has-text-grey-light "> GITHUB BLOG </a> <a href="http://localhost:4000/category/paper%20review" class="navbar-item has-text-grey-light "> PAPER REVIEW </a> <!--<hr class="navbar-divider"> <a class="navbar-item"> Report an issue </a> --></div></div><input id="darkmode_switch" class="mh_toogle" type="checkbox"> <label for="darkmode_switch" class="material-icons-sharp mh_toggle_btn"></label></div></div></div></nav><!-- Bulma Navbar JS --> <script> document.addEventListener('DOMContentLoaded', function () { /* Get all "navbar-burger" elements */ var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0); /* Check if there are any navbar burgers */ if ($navbarBurgers.length > 0) { /* Add a click event on each of them */ $navbarBurgers.forEach(function ($el) { $el.addEventListener('click', function () { /* Get the target from the "data-target" attribute */ var target = $el.dataset.target; var $target = document.getElementById(target); /* Toggle the class on both the "navbar-burger" and the "navbar-menu" */ $el.classList.toggle('is-active'); $target.classList.toggle('is-active'); }); }); } }); </script> <script> /* 스타일 파일들 */ const defaultTheme = [...document.styleSheets].find(style => /(style.css)$/.test(style.href)); const darkTheme = [...document.styleSheets].find(style => /(style_dark.css)$/.test(style.href)); /* 스위치, 현재 테마 상태 불러오기 */ let mode = document.getElementById("darkmode_switch"); const current = localStorage.theme; /* 기존 상태에 따라 스위치 체크해주기 */ mode.checked = current === 'dark'; /* 체크된 거에 따라서 스타일 지정해주기 */ darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; mode.addEventListener('click', function(){ localStorage.theme = mode.checked ? 'dark' : 'light'; darkTheme.disabled = mode.checked !== true; defaultTheme.disabled = mode.checked === true; }); </script> <span class="bar"></span><section class="hero is-fullheight has-text-centered" id="post"><div class="hero-body"><div class="container"> <a href="/blog/chatbotgpu" class="has-text-black" id="title"><h1 class="title has-text-centered is-2 has-text-weight-semibold ">(LLama2) GPU에서 돌아가는 나만의 디스코드 챗봇 만들기</h1></a><hr class="has-background-black"><div class="columns is-variable is-5"><div class="column is-6"><figure class="image is-16by9 has-shadow"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/97ca9bcd-4822-4782-8ab6-4ddac8068abe" alt="" id="post-image"></figure></div><div class="subtitle column is-5 has-text-left-desktop has-text-left-fullhd has-text-left-tablet has-text-center-mobile"><p id="description" class="content is-small has-text-weight-medium is-uppercase">Discord bot, Chatting bot</p><p class="subtitle is-6 is-uppercase has-text-weight-normal has-text-black-ter">Published on <b>September 24, 2023</b> by <a href="https://github.com/6unoyunr" target="_blank"><b class="has-text-link"><u>JY</u></b> </a></p><p class="subtitle is-uppercase"> <i class="fa fa-tags"></i> <span class="tag is-link">Automatic system</span> <span class="tag is-link">Chatting</span> <span class="tag is-link">LLM</span> <span class="tag is-link">NLP</span></p><p class="subtitle is-uppercase"><i class="fa fa-clock"></i> <b class="has-text-link"> 7 min </b>READ</p></div></div><div class="content has-text-justified-desktop has-text-justified-fullhd has-text-justified has-text-justified-tablet has-text-left-mobile"><p><h1 id="gpu에서-동작하는-챗봇-구현하기">GPU에서 동작하는 챗봇 구현하기</h1><p>이전 글에서 Llama-2-cpp를 사용하여 CPU로도 동작하는 챗봇을 구현했었다. 이번에는 리소스가 있다는 가정 하에, 보다 빠르게 입력된 prompt에 대한 답변을 처리할 수 있는 GPU를 활용한 챗봇을 만들어보기로 하였다. 마찬가지로 Llama-2를 사용하였으며, Llama-2의 경우에는 meta에 신청서만 제출하면 네트워크를 다운받을 수 있는 링크가 주어진다. 우선 <a href="https://github.com/facebookresearch/llama">Llama-2 github</a>에 들어간다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/75d65b67-a6b9-4f1d-ab53-b41de523a97d" width="800" /> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/77f26735-339b-479d-8326-f1dc5288ca48" width="800" /></p><p>다운받고자 한다면 accept를 받아야한다는 소리가 나온다. 경험상 양식을 채우고 이메일 수신을 기다렸을때 빠르면 5분 안에 바로 승인이 났다. 그리고 링크를 받게되면 이메일로 오게 되는데, 이때의 링크를 잘 저장해두자. 참고로 링크의 유효기간은 24시간이기 때문에, 승인 받고나서 ‘나중에 다운받아야지’하면 안된다. 당일에 모델을 다운받지 않으면 바로 해당 링크는 무용지물.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/ff901970-2c54-4a5e-8560-a357d20bd190" width="800" /></p><p>빨갛게 가려놓은 부분이 바로 링크다. 만약 본인이 이메일로 해당 내용을 회신받지 못했다면 넉넉하게 기다렸다가 다시 승인 요청을 보내보는 것을 추천한다. 아무튼 다운받는 법은 정말로 간단하다. 우선 Llama 깃허브를 클론한 뒤에 다운로드 스크립트를 켠다.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/facebookresearch/llama.git
<span class="nb">cd </span>llama
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>
bash download.sh
</code></pre></div></div><p>제대로 진행했다면 다음과 같은 문구가 뜨는데,</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/17e9b043-2491-497a-9714-5f149b05f041" width="500" /></p><p>바로 여기에 아까 받은 이메일의 링크를 복붙해서 넣으면 된다. 이후 다운로드 받는 과정은 output을 보면서 차근차근 따라하면 된다. 본인은 가능한 용량인 7B, 13B만 다운받았다.</p><hr /><h1 id="llama-2-챗봇-모델과-discord-bot-코드-연결하기">Llama-2 챗봇 모델과 discord bot 코드 연결하기</h1><p>우선 알아야할 점은 Llama-cpp와 Llama-2 GPU 버전은 모델이 동작하는 형태가 다르다. Llama-2 챗봇은 다음과 같은 input prompt 구조를 가져야한다. 예컨데 1명이 챗봇을 사용하는 상황을 가정하겠다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt_for_llama2</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dialog</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[{</span><span class="s">"role"</span> <span class="p">:</span> <span class="s">"system"</span><span class="p">,</span> <span class="s">"content"</span> <span class="p">:</span> <span class="s">"챗봇이 어떠한 방식으로 대답했으면 좋겠는가 작성"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"role"</span> <span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span> <span class="p">:</span> <span class="s">"Query #1"</span><span class="p">},</span>
    <span class="p">...]</span>
<span class="p">]</span>
</code></pre></div></div><p>엄밀하게 따지면 prompt는 <code class="language-plaintext highlighter-rouge">List[Dialog]</code> 형태이지만 간단하게는 리스트 안에 리스트가 내장된 구조를 생각하면 편하다. 가장 외곽의 리스트는 $n$개의 병렬적인 대화를 수용한다고 생각하면 되고, 우리는 실제로 단일 dialog에 하나의 유저에 대한 대화를 지속적으로 이어나갈 것이기 때문에 위와 같이 내부의 리스트에만 계속 query를 쌓아가는 구조가 된다. 간단한 코드 구조는 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">Llama</span><span class="p">,</span> <span class="n">Dialog</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">fire</span>

<span class="k">def</span> <span class="nf">chat_example</span><span class="p">():</span>
    <span class="c1"># Initialize Llama for text generation
</span>    <span class="n">generator</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">.</span><span class="n">build</span><span class="p">(</span>
        <span class="n">ckpt_dir</span><span class="o">=</span><span class="s">"llama-2-7b-chat/"</span><span class="p">,</span>
        <span class="n">tokenizer_path</span><span class="o">=</span><span class="s">"tokenizer.model"</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">dialog_prompt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dialog</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span>
            <span class="p">{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"system"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji."</span><span class="p">},</span>
            <span class="p">{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"Hello?"</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">]</span>

    <span class="c1"># Inference on Llama2 Model
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">generator</span><span class="p">.</span><span class="n">chat_completion</span><span class="p">(</span><span class="n">dialog_prompt</span><span class="p">,</span>
                                        <span class="n">max_gen_len</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
                                        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                                        <span class="p">)</span>

    <span class="c1"># Extract only answer(Bot reply)
</span>    <span class="n">answer</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"generation"</span><span class="p">][</span><span class="s">"content"</span><span class="p">]</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">fire</span><span class="p">.</span><span class="n">Fire</span><span class="p">(</span><span class="n">chat_example</span><span class="p">)</span>
</code></pre></div></div><hr /><h1 id="대화-쌓아가기">대화 쌓아가기</h1><p>기존에 했던 방식과 마찬가지로(이전 글인 ‘<a href="https://junia3.github.io/blog/chatbot">CPU에서 돌아가는 나만의 디스코드 챗봇 만들기</a>’ 참고) 챗봇은 이전 대화 내용을 어느 정도 고려하여 맥락을 맞출 필요가 있다. 이를 위해 대화를 쌓는 방식을 다음과 같이 지정해주었다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Initialize global variables
</span><span class="n">dialog_prompt</span><span class="p">:</span> <span class="n">Dialog</span> <span class="o">=</span> <span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"system"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji."</span><span class="p">}]</span>
<span class="n">dialogs_logs</span><span class="p">:</span> <span class="n">Dialog</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Define a function to make user dialog
</span><span class="k">def</span> <span class="nf">make_usr_dialog</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">dialog</span> <span class="o">=</span> <span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">query</span><span class="p">}]</span>
    <span class="k">return</span> <span class="n">dialog</span>

<span class="c1"># Define a function to make AI dialog
</span><span class="k">def</span> <span class="nf">make_ai_dialog</span><span class="p">(</span><span class="n">answer</span><span class="p">):</span>
    <span class="n">dialog</span> <span class="o">=</span> <span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"assistant"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">answer</span><span class="p">}]</span>
    <span class="k">return</span> <span class="n">dialog</span>

<span class="k">def</span> <span class="nf">answer_for_chat</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">dialogs_logs</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dialog</span> <span class="o">=</span> <span class="n">make_usr_dialog</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">dialogs_logs</span> <span class="o">+=</span> <span class="n">dialog</span>
        <span class="n">dialog_temp</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dialog</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">dialog_prompt</span> <span class="o">+</span> <span class="n">dialogs_logs</span><span class="p">]</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">generator</span><span class="p">.</span><span class="n">chat_completion</span><span class="p">(</span>
            <span class="n">dialog_temp</span><span class="p">,</span>
            <span class="n">max_gen_len</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">answer</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"generation"</span><span class="p">][</span><span class="s">"content"</span><span class="p">]</span>
        <span class="n">dialog</span> <span class="o">=</span> <span class="n">make_ai_dialog</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
        <span class="n">dialogs_logs</span> <span class="o">+=</span> <span class="n">dialog</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dialogs_logs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_chat_logs</span><span class="p">:</span>
            <span class="n">dialogs_logs</span> <span class="o">=</span> <span class="n">dialogs_logs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">dialogs_logs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">answer</span> <span class="o">=</span> <span class="s">"I could not generate message 🥲 ..."</span>

    <span class="k">return</span> <span class="n">answer</span>
</code></pre></div></div><p>쌓는 방식을 요약하면 다음과 같다. 챗봇이 대답해야하는 형태 (본인의 챗봇의 컨셉은 고양이이므로, 이를 system이라는 역할로 알려줌)를 prompt로 고정해둔다. 그리고 유저가 질문하는 내용을 llama에서 요구하는 프롬프트 형태로 바꿔주는 함수와, assistant의 대답을 llama에서 요구하는 프롬프트 형태로 바꿔주는 함수 <code class="language-plaintext highlighter-rouge">make_usr_dialog</code>와 <code class="language-plaintext highlighter-rouge">make_ai_dialog</code>를 각각 설정해준다. 쿼리가 들어오게 되면 이를 유저의 질문 형태로 바꿔 기존 로그에 추가하고, 모델에 들어가는 input에는 챗봇의 컨셉 + 질의 응답 로그룰 <code class="language-plaintext highlighter-rouge">List</code>로 wrapping하는 절차를 거친다.</p><p>또한 <code class="language-plaintext highlighter-rouge">max_chat_logs</code>라는 integer value를 통해 대화 로그의 메모리 관리를 하게 되는데, 대화 내용이 너무 길어지게 되면 모델 inference 시간이 증가하므로 이를 방지하기 위함이다. CPU에서의 방식과 다른 점이 있다면, input으로 들어가는 질의 응답의 경우 list의 요소가 유저/AI가 반복되어 들어가기 때문에 짝수 갯수만큼을 지워줘야하고, 이를 단순히 리스트의 indexing(windowing)으로 구현하였다. 대화가 반복될수록 이전 대화는 지워지고, 새로운 대화 내용이 로그에 남아 input에 사용된다.</p><hr /><h1 id="distributed-parallel-for-chatbot">Distributed parallel for ChatBot</h1><p>GPU 연산의 경우 랩 사용량이 꽤 되므로 이를 보조하기 위한 GPU 병렬 처리 시스템이 기본이다. Llama-2에서는 이를 자동으로 구현하였으며, 다음과 같이 본인의 컴퓨터 스펙에 따라 GPU 사용을 결정해주면 된다.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> torchrun <span class="nt">--nproc_per_node</span> 1 chatbot.py <span class="c"># GPU 1개 사용</span>
<span class="o">&gt;&gt;&gt;</span> torchrun <span class="nt">--nproc_per_node</span> 2 chatbot.py <span class="c"># GPU 2개 사용</span>
<span class="o">&gt;&gt;&gt;</span> torchrun <span class="nt">--nproc_per_node</span> 3 chatbot.py <span class="c"># GPU 3개 사용</span>
<span class="o">&gt;&gt;&gt;</span> torchrun <span class="nt">--nproc_per_node</span> 4 chatbot.py <span class="c"># GPU 4개 사용</span>
</code></pre></div></div><p>참고로 기본 셋팅의 경우 GPU는 아이디 오름차순으로 쓰이게 된다.</p><hr /><h1 id="디스코드-챗봇-시스템-코드와-연결하기">디스코드 챗봇 시스템 코드와 연결하기</h1><p>기존 디스코드 챗봇 시스템은 병렬 GPU에 대한 고려가 없었기 때문에 이를 무시할 수 있었지만, Llama 코드가 들어간 이상 병렬 처리가 가능하게끔 코드를 일부 손봐야한다. 구현 과정은 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Discord bot command to chat with Llama
</span><span class="o">@</span><span class="n">bot</span><span class="p">.</span><span class="n">command</span><span class="p">()</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">chat</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">fire</span><span class="p">.</span><span class="n">Fire</span><span class="p">(</span><span class="n">answer_for_chat</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>
    <span class="k">await</span> <span class="n">ctx</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</code></pre></div></div><p>만약 커맨드 상에서 chatting을 요청하고 이에 대한 query를 전송하면, fire 함수를 통해 <code class="language-plaintext highlighter-rouge">answer_for_chat</code> 함수를 불러오게 된다. 해당 함수는 위에서 소개한 함수 코드와 완전히 동일하며, query에 대한 Llama 모델의 인퍼런스를 담당한다. 모델을 불러오는 것은 코드 최초 실행 시 단 ‘한번만’ 수행한다. 최초 실행 시에 광역변수 추가 및 초기화 과정에 대해 다음 코드를 추가하였다.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize Llama for text generation
</span><span class="n">generator</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">ckpt_dir</span><span class="o">=</span><span class="s">"llama-2-7b-chat/"</span><span class="p">,</span>
    <span class="n">tokenizer_path</span><span class="o">=</span><span class="s">"tokenizer.model"</span><span class="p">,</span>
    <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Initialize global variables
</span><span class="n">dialog_prompt</span><span class="p">:</span> <span class="n">Dialog</span> <span class="o">=</span> <span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"system"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"You are a kitty deep learning researcher named 'DEV' and 10-year old. Reply with English with Emoji."</span><span class="p">}]</span>
<span class="n">dialogs_logs</span><span class="p">:</span> <span class="n">Dialog</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">max_chat_logs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
</code></pre></div></div><hr /><h1 id="결과-확인해보기">결과 확인해보기</h1><p>본인은 A6000 서버를 사용하여 테스트하였고, 각 GPU의 RAM 용량은 49기가바이트이다. 단일 GPU를 사용하면 다음과 같이 돌아가게 된다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/d7da1cb6-ceab-4d50-981b-b0f28f025361" width="800" /></p><p>대략 15~16기가 정도의 용량만 있다면 7B 모델을 넉넉하게 수용할 수 있는 것을 확인하였다. CPU와 비교했을때 긴 대답을 요구하는 질의에 대해서도 확연히 올라간 채팅 성능을 보여준다.</p><p align="center"> <img src="https://github.com/junia3/junia3.github.io/assets/79881119/91578953-0e09-4e85-b239-7d128f0845d0" width="800" /></p><p>13B 모델 또한 수용이 가능하여 확인해보았는데, 13B 모델의 경우에는 무슨 일인지 단일 GPU로는 돌아가지 않고 무조건 2개 이상의 GPU로 돌려야했다. 참고하면 좋을 것 같다.</p></p></div></div></div></section><div class="contain_cats" align="center"><div class="contain_category" align="center"><div class="anothercat" align="center"><body><div class="waviy"> <span style="--i:1">A</span> <span style="--i:1">n</span> <span style="--i:1">o</span> <span style="--i:1">t</span> <span style="--i:1">h</span> <span style="--i:1">e</span> <span style="--i:1">r</span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:2"> </span> <span style="--i:3">p</span> <span style="--i:3">o</span> <span style="--i:3">s</span> <span style="--i:3">t</span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:4"> </span> <span style="--i:5">i</span> <span style="--i:5">n</span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:6"> </span> <span style="--i:7">c</span> <span style="--i:7">a</span> <span style="--i:7">t</span> <span style="--i:7">e</span> <span style="--i:7">g</span> <span style="--i:7">o</span> <span style="--i:7">r</span> <span style="--i:7">y</span></div></body></div><div class="adjacent"><div class="prev_btn"> <a id="prev" class="button" href="/blog/chatbot"><p id="prev_title"> ❮❮ (LLama2-cpp) CPU에서 돌아가는 나만의 디스코드 챗봇 만들기</p></a></div><div class="next_btn"> <a id="next" class="button" href="/blog/githubreadme"><p id="next_title"> ❯❯ Github 프로필 리드미 작성하는 법 및 꾸미기 (feat. github readme snake)</p></a></div></div></div></div><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script> <script> const title1 = $("#prev_title").text(); const title2 = $('#next_title').text(); var speed = 100; var dots = '⋯⋯'; var titlelength = function () { setInterval(function () { var ww = $(window).width(); if(ww < 400){ offset = 10; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else if(ww < 600){ offset = 20; if(title1.length > offset){ part = title1.substr(0, offset); $("#prev_title").text(part + dots); } if(title2.length > offset){ part = title2.substr(0, offset); $("#next_title").text(part + dots) } } else{ $("#prev_title").text(title1); $("#next_title").text(title2); } }, speed); }; $(document).ready(function () { titlelength(); }); </script><div class="utterance-light" id="comment_light"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div><div class="utterance-dark" id="comment_dark"> <script src="https://utteranc.es/client.js" repo="junia3/comments" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div><footer id="footer"> <!--Footer Button--><div class="container has-text-centered has-background-grey-darker" id="backtotop"> <a class="has-text-white" onclick="window.scroll(0,0)">BACK TO TOP</a></div><!--Footer Main Section--><div class="has-background-grey-darker"><div class="container columns"> <!--Name Section--><div class="column has-text-left-desktop has-text-centered-mobile"> <a href="http://localhost:4000/#about"><div class="columns"><div class="column is-one-fifth-desktop is-one-fifth-fullhd is-one-quarter-tablet"><figure class="image is-64x64"> <img class="is-rounded" src="https://avatars.githubusercontent.com/u/79881119?v=4"></figure></div><div class="column is-marginless"><h5 class="has-text-grey-lighter">JY</h5><div class="content has-text-grey"><p>I am an AI researcher with a strong interest in machine learning and dee...</p></div></div></div></a></div><!--Link Section--><div class="column has-text-white"><h3>More Links</h3><li> <a href="http://localhost:4000/category/development">DEVELOPMENT</a></li><li> <a href="http://localhost:4000/category/github%20blog">GITHUB BLOG</a></li><li> <a href="http://localhost:4000/category/paper%20review">PAPER REVIEW</a></li></div><!--Blog-post Section--><div class="column has-text-white"><h3>Recent Posts</h3><li> <a href="http://localhost:4000/blog/deepseekr1">딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL).</a></li><li> <a href="http://localhost:4000/blog/mamba">Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여</a></li><li> <a href="http://localhost:4000/blog/s4">Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여</a></li></div></div></div><div class="has-background-black has-text-centered has-text-white" id="credits"></div></footer></body></html><script> $(window).scroll(function() { var scrollY = ($(window).scrollTop() / ($(document).height() - $(window).height()) * 100).toFixed(3); $(".bar").css({"width" : scrollY + "%"}); }); </script>
